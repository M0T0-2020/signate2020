{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "import nltk, string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "import optuna\n",
    "\n",
    "random.seed(2020)\n",
    "np.random.seed(2020)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "        self.stop_words = get_stop_words('en')\n",
    "        self.stop_words.append(' ')\n",
    "        self.stop_words.append('')\n",
    "    \n",
    "    def pipeline(self, df):\n",
    "        for lang in ['description']:\n",
    "            #, 'translate_es', 'translate_fr', 'translate_de', 'translate_ja']:\n",
    "            df[lang] = df[lang].apply(lambda x: self.change_text(x))\n",
    "        return df\n",
    "\n",
    "    def change_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace('ml', 'machine learning')\n",
    "        text = text.replace('machine learning', 'machinelearning')\n",
    "        text = \"\".join([char if char not in string.punctuation else ' ' for char in text])\n",
    "        text = \" \".join([self.porter.stem(char) for char in text.split(' ') if char not in self.stop_words])\n",
    "        return text\n",
    "    \n",
    "    def vectorize_tfidf(self, df):\n",
    "        vec_tfidf = TfidfVectorizer()\n",
    "        X = vec_tfidf.fit_transform(df.description.values)\n",
    "        X = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n",
    "        return X\n",
    "    \n",
    "    def vectorize_cnt(self, df):\n",
    "        vec_cnt = CountVectorizer()\n",
    "        X = vec_cnt.fit_transform(df.description.values)\n",
    "        X = pd.DataFrame(X.toarray(), columns=vec_cnt.get_feature_names())\n",
    "        return X\n",
    "\n",
    "\n",
    "class Optimize_by_Optuna:\n",
    "    def __init__(self, data, features, target_colname, target_name_2=None, _objective=None):\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.target = target_colname\n",
    "        if not target_colname:\n",
    "            self.target_2 = target_colname\n",
    "        else:\n",
    "            self.target_2 = target_name_2\n",
    "        self._objective = _objective\n",
    "        \n",
    "    \n",
    "    def make_score(self, y, preds):\n",
    "        s_1=1 - metrics.accuracy_score(y, preds)\n",
    "        s_2=list(self.model.best_score['valid_1'].values())[0]\n",
    "\n",
    "        return (s_1+s_2)/2\n",
    "\n",
    "    def objective(self, trial):\n",
    "                        \n",
    "        PARAMS = {#'boosting_type': 'gbdt', 'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            \n",
    "            #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "            \n",
    "            'objective': 'tweedie','metric': 'tweedie',\n",
    "            \n",
    "            'n_estimators': 1400,\n",
    "            'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "        \n",
    "\n",
    "            'tweedie_variance_power': trial.suggest_uniform('tweedie_variance_power', 1.01, 1.8),\n",
    "\n",
    "\n",
    "            'max_bin': trial.suggest_int('max_bin', 50, 300),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.4, 0.9),\n",
    "            'subsample_freq': trial.suggest_uniform('subsample_freq', 0.4, 0.9),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.03, 0.5),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 4, 2*5),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.0001, 10.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.0001, 10.0),\n",
    "        }\n",
    "        \n",
    "        score = 0\n",
    "        k = StratifiedKFold(n_splits=5)\n",
    "        for trn, val in k.split(self.data, self.data[self.target_2]):\n",
    "            train_df = self.data.iloc[trn,:]\n",
    "            val_df = self.data.iloc[val,:]\n",
    "            train_set= lgb.Dataset(train_df[self.features],  train_df[self.target])\n",
    "            val_set = lgb.Dataset(val_df[self.features],  val_df[self.target])   \n",
    "            \n",
    "            self.model = lgb.train(\n",
    "                train_set=train_set, valid_sets=[train_set, val_set], params=PARAMS, num_boost_round=3000, \n",
    "                early_stopping_rounds=200, verbose_eval=500\n",
    "                )\n",
    "                \n",
    "            preds = self.model.predict(val_df[self.features])\n",
    "            preds = np.round(preds)\n",
    "            y = val_df[self.target]\n",
    "            s = self.make_score(y, preds)\n",
    "            score+=s/5\n",
    "            \n",
    "        return score\n",
    "\n",
    "\n",
    "class Null_Importance:\n",
    "    def __init__(self, train_X, train_y, PARAMS, y_2=None):\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.y_2= y_2\n",
    "        self.PARAMS = PARAMS\n",
    "\n",
    "    def make_null_importance_df(self):\n",
    "        null_importance=pd.DataFrame()\n",
    "        null_importance['col'] = self.train_X.columns.tolist()\n",
    "        try:\n",
    "            for i in range(50):\n",
    "                tmp_null_importance=[]\n",
    "                \n",
    "                _train_y = self.train_y.apply(lambda x: random.choice([0,1]))\n",
    "                _train_y_2 = self.y_2.sample(frac=1).values\n",
    "                \n",
    "                print(f\"\"\"\n",
    "                \n",
    "                Train Null Importance   {i+1}\n",
    "                \n",
    "                \"\"\" )\n",
    "                k = StratifiedKFold(n_splits=5)\n",
    "                for trn, val in k.split(self.train_X, _train_y_2):\n",
    "                    trn_X, val_X = self.train_X.iloc[trn,:], self.train_X.iloc[val,:]\n",
    "                    trn_y, val_y = _train_y.iloc[trn].astype(int), _train_y.iloc[val].astype(int)\n",
    "                    train_set = lgb.Dataset(trn_X, trn_y)\n",
    "                    val_set = lgb.Dataset(val_X, val_y)\n",
    "\n",
    "                    model = lgb.train(params=self.PARAMS,\n",
    "                                      train_set=train_set, \n",
    "                                      valid_sets=[train_set, val_set],\n",
    "                                    num_boost_round=3000, early_stopping_rounds=200, verbose_eval=500)\n",
    "                    \n",
    "                    preds = model.predict(val_X)\n",
    "                    tmp_null_importance.append(model.feature_importance('gain'))\n",
    "                null_importance[f'null_importance_{i+1}'] = np.mean(tmp_null_importance, axis=0)\n",
    "            return null_importance\n",
    "        except:\n",
    "            return null_importance\n",
    "\n",
    "    def calu_importance(self, importance_df, null_importance_df):\n",
    "        importance_df = pd.merge(\n",
    "            importance_df, null_importance_df, on='col'\n",
    "            )\n",
    "        null_importance_col = [col for col in importance_df.columns if 'null' in col]\n",
    "        null_importance=pd.DataFrame()\n",
    "        for idx, row in importance_df.iterrows():\n",
    "            acc_v = 1e-10+row['true_importance']\n",
    "            null_v = 1+np.percentile(row[null_importance_col], 75)\n",
    "            null_importance[row['col']] = [np.log(acc_v/null_v)]\n",
    "        null_importance = null_importance.T\n",
    "        return null_importance\n",
    "\n",
    "    def all_flow(self):\n",
    "        k = StratifiedKFold(n_splits=5)\n",
    "        score=[]\n",
    "        importance=[]\n",
    "\n",
    "        importance_df=pd.DataFrame()\n",
    "        importance_df['col'] = self.train_X.columns\n",
    "        print(\"\"\"\n",
    "        \n",
    "        Train True Importance\n",
    "        \n",
    "        \"\"\" )\n",
    "        for trn, val in k.split(self.train_X, self.y_2):\n",
    "            trn_X, val_X = self.train_X.iloc[trn,:], self.train_X.iloc[val,:]\n",
    "            trn_y, val_y = self.train_y.iloc[trn].astype(int), self.train_y.iloc[val].astype(int)\n",
    "            train_set = lgb.Dataset(trn_X, trn_y)\n",
    "            val_set = lgb.Dataset(val_X, val_y)\n",
    "            \n",
    "            PARAMS['random_state']+=1\n",
    "            model = lgb.train(params=self.PARAMS, train_set=train_set, valid_sets=[train_set, val_set],\n",
    "                            num_boost_round=3000, early_stopping_rounds=200, verbose_eval=500)\n",
    "            preds = model.predict(val_X)\n",
    "            importance.append(model.feature_importance('gain'))\n",
    "        importance_df['true_importance'] = np.mean(importance, axis=0)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \n",
    "        Train Null Importance\n",
    "        \n",
    "        \"\"\" )\n",
    "        try:\n",
    "            null_importance_df = self.make_null_importance_df()\n",
    "        except:\n",
    "            pass\n",
    "        print(\"\"\"\n",
    "        \n",
    "        Calulate null_null_importance\n",
    "        \n",
    "        \"\"\" )\n",
    "        null_importance = self.calu_importance(importance_df, null_importance_df)\n",
    "        null_importance = null_importance.reset_index()\n",
    "        null_importance.columns = ['col', 'score']\n",
    "        null_importance = null_importance.sort_values('score', ascending=False)\n",
    "        return null_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_offdf(train_df, test_df, feature, params_list, _type):\n",
    "    k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    \n",
    "    y_1 = train_df.jobflag.apply(lambda x: 1 if x==1 else 0)\n",
    "    y_2 = train_df.jobflag.apply(lambda x: 1 if x==2 else 0)\n",
    "    y_3 = train_df.jobflag.apply(lambda x: 1 if x==3 else 0)\n",
    "    y_4 = train_df.jobflag.apply(lambda x: 1 if x==4 else 0)\n",
    "    \n",
    "    off_df = []\n",
    "    for i in range(4):\n",
    "        test_df[f'lgb_preds_{i+1}_{_type}']=0\n",
    "    \n",
    "    for trn, val in k.split(train_df, train_df.jobflag):\n",
    "        train_X, val_X = train_df.iloc[trn,:][feature], train_df.iloc[val,:][feature]\n",
    "        tmp_off_df = train_df.iloc[val,:]\n",
    "        c=1\n",
    "        for y, param in zip([y_1, y_2, y_3, y_4], params_list):\n",
    "            tmp_off_df[f'lgb_preds_{c}_{_type}']=0\n",
    "            for _ in range(5):\n",
    "                train_y, val_y = y.iloc[trn], y.iloc[val]\n",
    "                train_set= lgb.Dataset(train_X,  train_y)\n",
    "                val_set = lgb.Dataset(val_X,  val_y)   \n",
    "\n",
    "                model = lgb.train(\n",
    "                    train_set=train_set, valid_sets=[train_set, val_set], params=param, num_boost_round=3000, \n",
    "                    early_stopping_rounds=200, verbose_eval=500\n",
    "                )\n",
    "                tmp_off_df[f'lgb_preds_{c}_{_type}'] += model.predict(val_X)/5\n",
    "                param['random_state']+=1\n",
    "                \n",
    "                test_df[f'lgb_preds_{c}_{_type}'] += model.predict(test_df[feature])/5\n",
    "                \n",
    "            c+=1\n",
    "        \n",
    "        off_df.append(tmp_off_df)\n",
    "    \n",
    "    for i in range(4):\n",
    "        test_df[f'lgb_preds_{i+1}_{_type}']/=5\n",
    "    \n",
    "    off_df = pd.concat(off_df, axis=0)\n",
    "    return off_df.reset_index(drop=True), test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['abil', 'abl', 'accept', 'access', 'accord', 'account', 'accur', 'accuraci', 'achiev', 'acquisit', 'across', 'act', 'action', \n",
    "           'activ', 'ad', 'addit', 'address', 'adher', 'administr', 'advanc', 'advis', 'advisor', 'agil', 'agre', 'ai', 'algorithm', \n",
    "           'align', 'analys', 'analysi', 'analyst', 'analyt', 'analyz', 'api', 'appli', 'applic', 'approach', 'appropri', 'approv', \n",
    "           'architect', 'architectur', 'area', 'assembl', 'assess', 'assign', 'assist', 'audienc', 'autom', 'avail', 'aw', 'back',\n",
    "           'backend', 'base', 'basic', 'behavior', 'benefit', 'best', 'board', 'bug', 'build', 'busi', 'call', 'can', 'candid', 'capabl',\n",
    "           'capac', 'case', 'caus', 'challeng', 'chang', 'clearli', 'client', 'clinic', 'close', 'cloud', 'cluster', 'coach', 'code', \n",
    "           'collabor', 'collect', 'commerci', 'commiss', 'commun', 'compani', 'complet', 'complex', 'complianc', 'compon', 'comput', \n",
    "           'concept', 'conduct', 'confer', 'configur', 'connect', 'consist', 'construct', 'consult', 'content', 'continu', 'contract',\n",
    "           'contribut', 'control', 'coordin', 'core', 'corpor', 'correct', 'cost', 'creat', 'creation', 'creativ', 'critic', 'cross', \n",
    "           'cultur', 'current', 'custom', 'cycl', 'daili', 'dashboard', 'data', 'databas', 'dataset', 'date', 'deadlin', 'debug', 'decis',\n",
    "           'deep', 'defect', 'defin', 'definit', 'deliv', 'deliver', 'deliveri', 'demand', 'demonstr', 'depart', 'depend', 'deploy', 'depth',\n",
    "           'deriv', 'design', 'desir', 'detail', 'detect', 'determin', 'develop', 'devic', 'devop', 'differ', 'digit', 'direct', 'disciplin',\n",
    "           'discoveri', 'discuss', 'distribut', 'divers', 'document', 'domain', 'draw', 'drive', 'duti', 'dynam', 'edg', 'educ', 'effect',\n",
    "           'effici', 'effort', 'electron', 'email', 'embed', 'employe', 'enabl', 'end', 'engag', 'engin', 'enhanc', 'ensur', 'enterpris',\n",
    "           'environ', 'equip', 'erp', 'escal', 'establish', 'estim', 'etc', 'evalu', 'event', 'excel', 'execut', 'exist', 'expand', 'experi',\n",
    "           'expert', 'expertis', 'explain', 'explor', 'exploratori', 'extern', 'extract', 'face', 'facilit', 'failur', 'featur', 'feder', \n",
    "           'field', 'find', 'fix', 'flow', 'focu', 'follow', 'form', 'formul', 'framework', 'front', 'full', 'function', 'futur', 'gain',\n",
    "           'gap', 'gather', 'gener', 'global', 'go', 'goal', 'good', 'govern', 'group', 'grow', 'growth', 'guid', 'guidanc', 'hand', \n",
    "           'hardwar', 'healthcar', 'help', 'high', 'highli', 'hoc', 'idea', 'identifi', 'impact', 'implement', 'improv', 'incid', 'includ',\n",
    "           'increas', 'independ', 'individu', 'industri', 'influenc', 'inform', 'infrastructur', 'initi', 'innov', 'input', 'insight',\n",
    "           'inspect', 'instal', 'integr', 'intellig', 'interact', 'interfac', 'intern', 'interpret', 'investig', 'issu', 'iter', 'java',\n",
    "           'job', 'junior', 'keep', 'key', 'knowledg', 'languag', 'larg', 'latest', 'lead', 'leader', 'leadership', 'learn', 'level', \n",
    "           'leverag', 'librari', 'life', 'like', 'limit', 'linux', 'log', 'logic', 'machin', 'machinelearn', 'maintain', 'mainten', \n",
    "           'make', 'manag', 'manner', 'manufactur', 'map', 'market', 'materi', 'matter', 'may', 'measur', 'mechan', 'medic', 'meet',\n",
    "           'member', 'mentor', 'met', 'method', 'methodolog', 'metric', 'microsoft', 'migrat', 'mission', 'mobil', 'model', 'moder',\n",
    "           'modifi', 'modul', 'monitor', 'multi', 'multipl', 'must', 'necessari', 'need', 'net', 'network', 'new', 'next', 'non', 'novel',\n",
    "           'object', 'obtain', 'ongo', 'open', 'oper', 'opportun', 'optim', 'order', 'organ', 'organiz', 'orient', 'outcom', 'outsid',\n",
    "           'overal', 'overse', 'part', 'parti', 'particip', 'partner', 'partnership', 'pattern', 'payrol', 'peer', 'perform', 'person',\n",
    "           'personnel', 'pipelin', 'plan', 'platform', 'point', 'polici', 'posit', 'post', 'potenti', 'practic', 'pre', 'predict', 'prepar', \n",
    "           'present', 'price', 'principl', 'prior', 'priorit', 'proactiv', 'problem', 'procedur', 'process', 'produc', 'product', \n",
    "           'profession', 'program', 'progress', 'project', 'promot', 'proof', 'propos', 'prospect', 'protocol', 'prototyp','provid', \n",
    "           'purpos', 'python', 'qa', 'qualifi', 'qualiti', 'queri', 'question', 'quickli', 'real', 'recommend', 'referr', 'refin', 'regard',\n",
    "           'region', 'regul', 'regular', 'regulatori', 'relat', 'relationship', 'releas', 'relev', 'reliabl', 'report','repres', 'request',\n",
    "           'requir', 'research', 'resid', 'resolut', 'resolv', 'resourc', 'respons', 'result', 'retail', 'review', 'rigor', 'risk', 'roadmap',\n",
    "           'role', 'root', 'rule', 'run', 'safeti', 'sale', 'scalabl', 'scale', 'schedul', 'scienc', 'scientist', 'scope', 'script', 'scrum',\n",
    "           'secur', 'segment', 'select', 'self', 'sell', 'senior', 'serv', 'server', 'servic', 'set', 'share', 'show', 'simul', 'site',\n",
    "           'skill', 'small', 'softwar', 'solut', 'solv', 'sourc', 'specif', 'sql', 'stack', 'staff', 'stakehold', 'standard', 'state',\n",
    "           'statist', 'statu', 'stay', 'store', 'stori', 'strateg', 'strategi', 'stream', 'strong', 'structur', 'studi', 'subject', \n",
    "           'success', 'suggest', 'supplier', 'support', 'system', 'take', 'target', 'task', 'team', 'technic', 'techniqu', 'technolog', \n",
    "           'term','test', 'think', 'thought', 'throughout', 'time', 'timelin', 'tool', 'top', 'track', 'train', 'transform', 'translat',\n",
    "           'travel', 'trend', 'troubleshoot', 'tune', 'understand', 'unit', 'updat', 'upgrad', 'use', 'user', 'util', 'valid',\n",
    "           'valu', 'variou', 'vehicl', 'vendor', 'verif', 'verifi', 'version', 'via', 'vision', 'visual', 'way',\n",
    "           'web', 'well', 'wide', 'will', 'window', 'within', 'work', 'workflow','write']\n",
    "\n",
    "PARAMS_1={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    'objective': 'tweedie','metric': 'tweedie',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "   'tweedie_variance_power': 1.349969119190657, 'max_bin': 212, 'subsample': 0.5774043241504451, 'subsample_freq': 0.7045972939301558, \n",
    "    'learning_rate': 0.16528226095247364, 'num_leaves': 4, 'feature_fraction': 0.9964784224971625,\n",
    "    'bagging_freq': 6, 'min_child_samples': 23, 'lambda_l1': 0.016924825494747078, 'lambda_l2': 0.0008031532180312293\n",
    "}\n",
    "\n",
    "\n",
    "PARAMS_2={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    'objective': 'tweedie','metric': 'tweedie',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "    'tweedie_variance_power': 1.3014991003823067, 'max_bin': 134, 'subsample': 0.8990859498726816, 'subsample_freq': 0.5274951186330312,\n",
    "    'learning_rate': 0.3937162652059595, 'num_leaves': 5, 'feature_fraction': 0.8861294810479933, 'bagging_freq': 5,\n",
    "    'min_child_samples': 28, 'lambda_l1': 6.037171725930821, 'lambda_l2': 0.0025254105473444784\n",
    "}\n",
    "\n",
    "PARAMS_3={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    #'objective': 'tweedie','metric': 'tweedie',\n",
    "     \n",
    "    'objective': 'xentropy','metric': 'xentropy',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "    'max_bin': 50, 'subsample': 0.8509082362331666, 'subsample_freq': 0.6958806976511948, 'learning_rate': 0.09406169926162017,\n",
    "    'num_leaves': 7, 'feature_fraction': 0.7562554580497556, 'bagging_freq': 4, 'min_child_samples': 5, 'lambda_l1': 0.00021420978217365439,\n",
    "    'lambda_l2': 0.011867471326820044\n",
    "}\n",
    "\n",
    "PARAMS_4={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    'objective': 'tweedie','metric': 'tweedie',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "    'tweedie_variance_power': 1.3572492826220748, 'max_bin': 169, 'subsample': 0.6874225607452877, 'subsample_freq': 0.5369168449326642,\n",
    "    'learning_rate': 0.0353671206084155, 'num_leaves': 8, 'feature_fraction': 0.9508830019260512, \n",
    "    'bagging_freq': 2, 'min_child_samples': 63, 'lambda_l1': 8.281467382972142, 'lambda_l2': 0.1428656656583413\n",
    "}\n",
    "\n",
    "param_list = [PARAMS_1, PARAMS_2, PARAMS_3, PARAMS_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../train.csv')\n",
    "test_df = pd.read_csv('../test.csv')\n",
    "for i in range(4):    \n",
    "    train_df = pd.merge(train_df, pd.read_csv(f'../train_df_off{i+1}.csv'), on='id')\n",
    "    test_df = pd.merge(test_df, pd.read_csv(f'../test_df_off{i+1}.csv').drop(columns=['description', 'jobflag']), on='id')\n",
    "df = pd.concat([train_df, test_df],axis=0,ignore_index=True)\n",
    "df['text_id'] = df['id']\n",
    "del df['id']\n",
    "preprocessing = Preprocessing()\n",
    "df.description = df.description.apply(lambda x: preprocessing.change_text(x))\n",
    "cols = ['jobflag','text_id','bert_pred_1_1', 'bert_pred_2_1', 'bert_pred_3_1',\n",
    "       'bert_pred_4_1', 'bert_pred_5_1', 'bert_pred_1_2', 'bert_pred_2_2',\n",
    "       'bert_pred_3_2', 'bert_pred_4_2', 'bert_pred_5_2', 'bert_pred_1_3',\n",
    "       'bert_pred_2_3', 'bert_pred_3_3', 'bert_pred_4_3', 'bert_pred_5_3',\n",
    "       'bert_pred_1_4', 'bert_pred_2_4', 'bert_pred_3_4', 'bert_pred_4_4',\n",
    "       'bert_pred_5_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's tweedie: 1.41842\tvalid_1's tweedie: 1.45615\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttraining's tweedie: 1.4115\tvalid_1's tweedie: 1.46333\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's tweedie: 1.43239\tvalid_1's tweedie: 1.46237\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's tweedie: 1.43956\tvalid_1's tweedie: 1.47761\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttraining's tweedie: 1.39046\tvalid_1's tweedie: 1.45937\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's tweedie: 0.952388\tvalid_1's tweedie: 1.00149\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's tweedie: 0.959811\tvalid_1's tweedie: 1.00898\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's tweedie: 0.958413\tvalid_1's tweedie: 1.00516\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's tweedie: 0.961879\tvalid_1's tweedie: 1.00374\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's tweedie: 0.948668\tvalid_1's tweedie: 1.0072\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's cross_entropy: 0.392688\tvalid_1's cross_entropy: 0.521086\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttraining's cross_entropy: 0.360151\tvalid_1's cross_entropy: 0.51321\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's cross_entropy: 0.393708\tvalid_1's cross_entropy: 0.520145\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[182]\ttraining's cross_entropy: 0.350039\tvalid_1's cross_entropy: 0.520056\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[144]\ttraining's cross_entropy: 0.37791\tvalid_1's cross_entropy: 0.515256\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.37145\tvalid_1's tweedie: 1.40596\n",
      "Early stopping, best iteration is:\n",
      "[694]\ttraining's tweedie: 1.36292\tvalid_1's tweedie: 1.40295\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.37182\tvalid_1's tweedie: 1.40902\n",
      "Early stopping, best iteration is:\n",
      "[782]\ttraining's tweedie: 1.35937\tvalid_1's tweedie: 1.40771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.37129\tvalid_1's tweedie: 1.40805\n",
      "Early stopping, best iteration is:\n",
      "[444]\ttraining's tweedie: 1.3745\tvalid_1's tweedie: 1.40733\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.37051\tvalid_1's tweedie: 1.40896\n",
      "Early stopping, best iteration is:\n",
      "[781]\ttraining's tweedie: 1.35795\tvalid_1's tweedie: 1.4052\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.37157\tvalid_1's tweedie: 1.40789\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttraining's tweedie: 1.36314\tvalid_1's tweedie: 1.40488\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's tweedie: 1.43752\tvalid_1's tweedie: 1.48633\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's tweedie: 1.43113\tvalid_1's tweedie: 1.49161\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's tweedie: 1.45339\tvalid_1's tweedie: 1.48963\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's tweedie: 1.46034\tvalid_1's tweedie: 1.49567\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's tweedie: 1.418\tvalid_1's tweedie: 1.48064\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's tweedie: 0.945915\tvalid_1's tweedie: 0.962421\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's tweedie: 0.908481\tvalid_1's tweedie: 0.96676\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's tweedie: 0.925962\tvalid_1's tweedie: 0.966212\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttraining's tweedie: 0.914724\tvalid_1's tweedie: 0.964127\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's tweedie: 0.944497\tvalid_1's tweedie: 0.958152\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's cross_entropy: 0.388044\tvalid_1's cross_entropy: 0.514771\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttraining's cross_entropy: 0.417263\tvalid_1's cross_entropy: 0.512027\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[140]\ttraining's cross_entropy: 0.381415\tvalid_1's cross_entropy: 0.514561\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[121]\ttraining's cross_entropy: 0.397295\tvalid_1's cross_entropy: 0.516428\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[142]\ttraining's cross_entropy: 0.37989\tvalid_1's cross_entropy: 0.513074\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[184]\ttraining's tweedie: 1.39193\tvalid_1's tweedie: 1.44704\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[206]\ttraining's tweedie: 1.38585\tvalid_1's tweedie: 1.44873\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[200]\ttraining's tweedie: 1.3861\tvalid_1's tweedie: 1.44915\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[272]\ttraining's tweedie: 1.37649\tvalid_1's tweedie: 1.44847\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[170]\ttraining's tweedie: 1.39422\tvalid_1's tweedie: 1.44752\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\ttraining's tweedie: 1.40264\tvalid_1's tweedie: 1.44552\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's tweedie: 1.41791\tvalid_1's tweedie: 1.42756\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's tweedie: 1.4649\tvalid_1's tweedie: 1.45343\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's tweedie: 1.41658\tvalid_1's tweedie: 1.43596\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's tweedie: 1.43311\tvalid_1's tweedie: 1.44196\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's tweedie: 0.937848\tvalid_1's tweedie: 1.01204\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's tweedie: 0.974938\tvalid_1's tweedie: 1.01296\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's tweedie: 0.931347\tvalid_1's tweedie: 1.01448\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's tweedie: 0.966156\tvalid_1's tweedie: 1.01767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's tweedie: 0.943051\tvalid_1's tweedie: 1.01052\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[219]\ttraining's cross_entropy: 0.330149\tvalid_1's cross_entropy: 0.526225\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[164]\ttraining's cross_entropy: 0.364804\tvalid_1's cross_entropy: 0.520382\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[159]\ttraining's cross_entropy: 0.367371\tvalid_1's cross_entropy: 0.528992\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[252]\ttraining's cross_entropy: 0.312734\tvalid_1's cross_entropy: 0.528517\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttraining's cross_entropy: 0.355705\tvalid_1's cross_entropy: 0.515373\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36297\tvalid_1's tweedie: 1.43254\n",
      "Early stopping, best iteration is:\n",
      "[550]\ttraining's tweedie: 1.36033\tvalid_1's tweedie: 1.43184\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36181\tvalid_1's tweedie: 1.43287\n",
      "Early stopping, best iteration is:\n",
      "[586]\ttraining's tweedie: 1.35722\tvalid_1's tweedie: 1.43179\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36323\tvalid_1's tweedie: 1.43467\n",
      "[1000]\ttraining's tweedie: 1.34161\tvalid_1's tweedie: 1.43054\n",
      "Early stopping, best iteration is:\n",
      "[934]\ttraining's tweedie: 1.34408\tvalid_1's tweedie: 1.42869\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36228\tvalid_1's tweedie: 1.43242\n",
      "[1000]\ttraining's tweedie: 1.34145\tvalid_1's tweedie: 1.43226\n",
      "Early stopping, best iteration is:\n",
      "[914]\ttraining's tweedie: 1.34456\tvalid_1's tweedie: 1.43005\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36183\tvalid_1's tweedie: 1.43409\n",
      "Early stopping, best iteration is:\n",
      "[699]\ttraining's tweedie: 1.35189\tvalid_1's tweedie: 1.42957\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's tweedie: 1.42371\tvalid_1's tweedie: 1.45279\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[108]\ttraining's tweedie: 1.37024\tvalid_1's tweedie: 1.46155\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's tweedie: 1.42667\tvalid_1's tweedie: 1.47447\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's tweedie: 1.41802\tvalid_1's tweedie: 1.46068\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's tweedie: 1.40753\tvalid_1's tweedie: 1.4539\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[117]\ttraining's tweedie: 0.885331\tvalid_1's tweedie: 0.959723\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's tweedie: 0.887959\tvalid_1's tweedie: 0.956838\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's tweedie: 0.91559\tvalid_1's tweedie: 0.959504\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's tweedie: 0.91696\tvalid_1's tweedie: 0.963978\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's tweedie: 0.879285\tvalid_1's tweedie: 0.961055\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[183]\ttraining's cross_entropy: 0.359922\tvalid_1's cross_entropy: 0.479326\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[214]\ttraining's cross_entropy: 0.342633\tvalid_1's cross_entropy: 0.47043\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[245]\ttraining's cross_entropy: 0.325274\tvalid_1's cross_entropy: 0.471733\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[298]\ttraining's cross_entropy: 0.301839\tvalid_1's cross_entropy: 0.471324\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[203]\ttraining's cross_entropy: 0.349135\tvalid_1's cross_entropy: 0.475439\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.37296\tvalid_1's tweedie: 1.38298\n",
      "Early stopping, best iteration is:\n",
      "[665]\ttraining's tweedie: 1.36427\tvalid_1's tweedie: 1.37815\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.37337\tvalid_1's tweedie: 1.38486\n",
      "[1000]\ttraining's tweedie: 1.35081\tvalid_1's tweedie: 1.37676\n",
      "Early stopping, best iteration is:\n",
      "[1178]\ttraining's tweedie: 1.34487\tvalid_1's tweedie: 1.37607\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.37279\tvalid_1's tweedie: 1.38127\n",
      "[1000]\ttraining's tweedie: 1.35104\tvalid_1's tweedie: 1.37518\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's tweedie: 1.33831\tvalid_1's tweedie: 1.37425\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.37216\tvalid_1's tweedie: 1.38376\n",
      "[1000]\ttraining's tweedie: 1.35142\tvalid_1's tweedie: 1.37702\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's tweedie: 1.33977\tvalid_1's tweedie: 1.37481\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.3721\tvalid_1's tweedie: 1.385\n",
      "[1000]\ttraining's tweedie: 1.3514\tvalid_1's tweedie: 1.37841\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's tweedie: 1.3392\tvalid_1's tweedie: 1.37503\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's tweedie: 1.40688\tvalid_1's tweedie: 1.4792\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's tweedie: 1.40987\tvalid_1's tweedie: 1.46728\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's tweedie: 1.39372\tvalid_1's tweedie: 1.46646\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's tweedie: 1.38887\tvalid_1's tweedie: 1.46258\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's tweedie: 1.41851\tvalid_1's tweedie: 1.46831\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's tweedie: 0.904955\tvalid_1's tweedie: 0.977084\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttraining's tweedie: 0.918641\tvalid_1's tweedie: 0.969829\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's tweedie: 0.944224\tvalid_1's tweedie: 0.976406\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's tweedie: 0.906878\tvalid_1's tweedie: 0.985856\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's tweedie: 0.918974\tvalid_1's tweedie: 0.97535\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's cross_entropy: 0.389666\tvalid_1's cross_entropy: 0.491613\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's cross_entropy: 0.382222\tvalid_1's cross_entropy: 0.484449\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[165]\ttraining's cross_entropy: 0.37048\tvalid_1's cross_entropy: 0.494887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's cross_entropy: 0.359881\tvalid_1's cross_entropy: 0.483468\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[130]\ttraining's cross_entropy: 0.394095\tvalid_1's cross_entropy: 0.489683\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.35931\tvalid_1's tweedie: 1.42477\n",
      "[1000]\ttraining's tweedie: 1.33861\tvalid_1's tweedie: 1.42372\n",
      "Early stopping, best iteration is:\n",
      "[924]\ttraining's tweedie: 1.34048\tvalid_1's tweedie: 1.42308\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36107\tvalid_1's tweedie: 1.42574\n",
      "[1000]\ttraining's tweedie: 1.33745\tvalid_1's tweedie: 1.42366\n",
      "Early stopping, best iteration is:\n",
      "[1079]\ttraining's tweedie: 1.33483\tvalid_1's tweedie: 1.42218\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.35868\tvalid_1's tweedie: 1.4226\n",
      "Early stopping, best iteration is:\n",
      "[740]\ttraining's tweedie: 1.34758\tvalid_1's tweedie: 1.41951\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.3595\tvalid_1's tweedie: 1.42728\n",
      "Early stopping, best iteration is:\n",
      "[508]\ttraining's tweedie: 1.35921\tvalid_1's tweedie: 1.42668\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36058\tvalid_1's tweedie: 1.42351\n",
      "[1000]\ttraining's tweedie: 1.33927\tvalid_1's tweedie: 1.42187\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's tweedie: 1.32713\tvalid_1's tweedie: 1.42208\n"
     ]
    }
   ],
   "source": [
    "X = preprocessing.vectorize_tfidf(df)\n",
    "X = pd.concat([df[cols], X], axis=1)\n",
    "train_df = X[X.jobflag.notnull()].reset_index(drop=True)\n",
    "test_df = X[X.jobflag.isnull()].drop(columns=['jobflag']).reset_index(drop=True)\n",
    "\n",
    "off_df_tfidf, test_df2_tfidf = make_offdf(train_df, test_df, feature, param_list, _type='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's tweedie: 1.42809\tvalid_1's tweedie: 1.45092\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's tweedie: 1.4232\tvalid_1's tweedie: 1.4524\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's tweedie: 1.40336\tvalid_1's tweedie: 1.45466\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's tweedie: 1.42955\tvalid_1's tweedie: 1.46706\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[74]\ttraining's tweedie: 1.4049\tvalid_1's tweedie: 1.44774\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's tweedie: 0.977071\tvalid_1's tweedie: 1.0205\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's tweedie: 0.935551\tvalid_1's tweedie: 1.0107\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's tweedie: 0.965989\tvalid_1's tweedie: 1.00923\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's tweedie: 0.95132\tvalid_1's tweedie: 1.0086\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's tweedie: 0.970861\tvalid_1's tweedie: 1.01129\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[202]\ttraining's cross_entropy: 0.370958\tvalid_1's cross_entropy: 0.506902\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[200]\ttraining's cross_entropy: 0.372878\tvalid_1's cross_entropy: 0.505181\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttraining's cross_entropy: 0.358474\tvalid_1's cross_entropy: 0.505948\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttraining's cross_entropy: 0.344344\tvalid_1's cross_entropy: 0.509211\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[197]\ttraining's cross_entropy: 0.37287\tvalid_1's cross_entropy: 0.508691\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39403\tvalid_1's tweedie: 1.41132\n",
      "Early stopping, best iteration is:\n",
      "[490]\ttraining's tweedie: 1.39446\tvalid_1's tweedie: 1.4107\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39416\tvalid_1's tweedie: 1.41108\n",
      "Early stopping, best iteration is:\n",
      "[510]\ttraining's tweedie: 1.39391\tvalid_1's tweedie: 1.41039\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39473\tvalid_1's tweedie: 1.41054\n",
      "Early stopping, best iteration is:\n",
      "[472]\ttraining's tweedie: 1.39581\tvalid_1's tweedie: 1.40886\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39522\tvalid_1's tweedie: 1.41091\n",
      "Early stopping, best iteration is:\n",
      "[474]\ttraining's tweedie: 1.39522\tvalid_1's tweedie: 1.41091\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39696\tvalid_1's tweedie: 1.4108\n",
      "Early stopping, best iteration is:\n",
      "[406]\ttraining's tweedie: 1.39721\tvalid_1's tweedie: 1.41072\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's tweedie: 1.40163\tvalid_1's tweedie: 1.46976\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttraining's tweedie: 1.42848\tvalid_1's tweedie: 1.47151\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's tweedie: 1.42916\tvalid_1's tweedie: 1.47784\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[66]\ttraining's tweedie: 1.41003\tvalid_1's tweedie: 1.46755\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\ttraining's tweedie: 1.40675\tvalid_1's tweedie: 1.46817\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's tweedie: 0.949976\tvalid_1's tweedie: 0.968675\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's tweedie: 0.943437\tvalid_1's tweedie: 0.970428\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[71]\ttraining's tweedie: 0.930141\tvalid_1's tweedie: 0.965583\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttraining's tweedie: 0.940906\tvalid_1's tweedie: 0.97359\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's tweedie: 0.952663\tvalid_1's tweedie: 0.966186\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's cross_entropy: 0.437756\tvalid_1's cross_entropy: 0.503073\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[120]\ttraining's cross_entropy: 0.419679\tvalid_1's cross_entropy: 0.503535\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\ttraining's cross_entropy: 0.454115\tvalid_1's cross_entropy: 0.511048\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttraining's cross_entropy: 0.45894\tvalid_1's cross_entropy: 0.504979\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\ttraining's cross_entropy: 0.357422\tvalid_1's cross_entropy: 0.506924\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[276]\ttraining's tweedie: 1.39622\tvalid_1's tweedie: 1.43965\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[256]\ttraining's tweedie: 1.39879\tvalid_1's tweedie: 1.43999\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.38468\tvalid_1's tweedie: 1.44368\n",
      "Early stopping, best iteration is:\n",
      "[306]\ttraining's tweedie: 1.39366\tvalid_1's tweedie: 1.44099\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[228]\ttraining's tweedie: 1.39899\tvalid_1's tweedie: 1.43897\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.38369\tvalid_1's tweedie: 1.44251\n",
      "Early stopping, best iteration is:\n",
      "[356]\ttraining's tweedie: 1.39014\tvalid_1's tweedie: 1.44024\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's tweedie: 1.41649\tvalid_1's tweedie: 1.43434\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's tweedie: 1.4382\tvalid_1's tweedie: 1.42406\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's tweedie: 1.43623\tvalid_1's tweedie: 1.40959\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttraining's tweedie: 1.43155\tvalid_1's tweedie: 1.41484\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's tweedie: 1.43124\tvalid_1's tweedie: 1.42065\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's tweedie: 0.965003\tvalid_1's tweedie: 1.0063\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\ttraining's tweedie: 0.925153\tvalid_1's tweedie: 1.008\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's tweedie: 0.962518\tvalid_1's tweedie: 1.004\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33]\ttraining's tweedie: 0.938556\tvalid_1's tweedie: 1.00316\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[195]\ttraining's tweedie: 0.903044\tvalid_1's tweedie: 1.00571\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[196]\ttraining's cross_entropy: 0.375505\tvalid_1's cross_entropy: 0.520493\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[207]\ttraining's cross_entropy: 0.37027\tvalid_1's cross_entropy: 0.523775\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[215]\ttraining's cross_entropy: 0.366058\tvalid_1's cross_entropy: 0.517799\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[221]\ttraining's cross_entropy: 0.363776\tvalid_1's cross_entropy: 0.517459\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[145]\ttraining's cross_entropy: 0.402598\tvalid_1's cross_entropy: 0.523193\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39033\tvalid_1's tweedie: 1.43037\n",
      "[1000]\ttraining's tweedie: 1.37998\tvalid_1's tweedie: 1.42333\n",
      "Early stopping, best iteration is:\n",
      "[1072]\ttraining's tweedie: 1.37876\tvalid_1's tweedie: 1.42193\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.3901\tvalid_1's tweedie: 1.42898\n",
      "Early stopping, best iteration is:\n",
      "[736]\ttraining's tweedie: 1.38404\tvalid_1's tweedie: 1.42423\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.38978\tvalid_1's tweedie: 1.4282\n",
      "Early stopping, best iteration is:\n",
      "[751]\ttraining's tweedie: 1.38389\tvalid_1's tweedie: 1.42539\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.3897\tvalid_1's tweedie: 1.42615\n",
      "[1000]\ttraining's tweedie: 1.37948\tvalid_1's tweedie: 1.4212\n",
      "Early stopping, best iteration is:\n",
      "[966]\ttraining's tweedie: 1.37993\tvalid_1's tweedie: 1.42108\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39017\tvalid_1's tweedie: 1.42796\n",
      "Early stopping, best iteration is:\n",
      "[678]\ttraining's tweedie: 1.38537\tvalid_1's tweedie: 1.42564\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's tweedie: 1.41155\tvalid_1's tweedie: 1.44351\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[143]\ttraining's tweedie: 1.38469\tvalid_1's tweedie: 1.44376\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\ttraining's tweedie: 1.39457\tvalid_1's tweedie: 1.44297\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[80]\ttraining's tweedie: 1.40627\tvalid_1's tweedie: 1.44228\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's tweedie: 1.42153\tvalid_1's tweedie: 1.44069\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's tweedie: 0.919281\tvalid_1's tweedie: 0.970404\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttraining's tweedie: 0.916038\tvalid_1's tweedie: 0.965214\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[165]\ttraining's tweedie: 0.915276\tvalid_1's tweedie: 0.966628\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[121]\ttraining's tweedie: 0.920288\tvalid_1's tweedie: 0.965015\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[67]\ttraining's tweedie: 0.930651\tvalid_1's tweedie: 0.966385\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[140]\ttraining's cross_entropy: 0.4145\tvalid_1's cross_entropy: 0.486334\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[199]\ttraining's cross_entropy: 0.378477\tvalid_1's cross_entropy: 0.484453\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[192]\ttraining's cross_entropy: 0.384189\tvalid_1's cross_entropy: 0.479873\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[164]\ttraining's cross_entropy: 0.399099\tvalid_1's cross_entropy: 0.488023\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[175]\ttraining's cross_entropy: 0.391314\tvalid_1's cross_entropy: 0.483065\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39645\tvalid_1's tweedie: 1.39232\n",
      "Early stopping, best iteration is:\n",
      "[778]\ttraining's tweedie: 1.38954\tvalid_1's tweedie: 1.38698\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39699\tvalid_1's tweedie: 1.39295\n",
      "Early stopping, best iteration is:\n",
      "[680]\ttraining's tweedie: 1.39327\tvalid_1's tweedie: 1.38931\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39721\tvalid_1's tweedie: 1.39543\n",
      "[1000]\ttraining's tweedie: 1.38919\tvalid_1's tweedie: 1.38924\n",
      "Early stopping, best iteration is:\n",
      "[824]\ttraining's tweedie: 1.38919\tvalid_1's tweedie: 1.38924\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39643\tvalid_1's tweedie: 1.39501\n",
      "Early stopping, best iteration is:\n",
      "[476]\ttraining's tweedie: 1.39643\tvalid_1's tweedie: 1.39501\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.39634\tvalid_1's tweedie: 1.39363\n",
      "Early stopping, best iteration is:\n",
      "[780]\ttraining's tweedie: 1.39019\tvalid_1's tweedie: 1.38935\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's tweedie: 1.40581\tvalid_1's tweedie: 1.46862\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's tweedie: 1.42216\tvalid_1's tweedie: 1.46446\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's tweedie: 1.42932\tvalid_1's tweedie: 1.47456\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's tweedie: 1.40839\tvalid_1's tweedie: 1.46299\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's tweedie: 1.42487\tvalid_1's tweedie: 1.46375\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[152]\ttraining's tweedie: 0.914755\tvalid_1's tweedie: 0.969111\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[168]\ttraining's tweedie: 0.91533\tvalid_1's tweedie: 0.972082\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's tweedie: 0.926336\tvalid_1's tweedie: 0.972647\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[86]\ttraining's tweedie: 0.926513\tvalid_1's tweedie: 0.98031\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[104]\ttraining's tweedie: 0.922714\tvalid_1's tweedie: 0.971778\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttraining's cross_entropy: 0.39341\tvalid_1's cross_entropy: 0.485813\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[258]\ttraining's cross_entropy: 0.358772\tvalid_1's cross_entropy: 0.480352\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[245]\ttraining's cross_entropy: 0.363464\tvalid_1's cross_entropy: 0.486399\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[191]\ttraining's cross_entropy: 0.386388\tvalid_1's cross_entropy: 0.478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's cross_entropy: 0.388436\tvalid_1's cross_entropy: 0.48333\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.3889\tvalid_1's tweedie: 1.42448\n",
      "Early stopping, best iteration is:\n",
      "[477]\ttraining's tweedie: 1.38946\tvalid_1's tweedie: 1.42401\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.38756\tvalid_1's tweedie: 1.42426\n",
      "Early stopping, best iteration is:\n",
      "[792]\ttraining's tweedie: 1.38007\tvalid_1's tweedie: 1.42129\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.38831\tvalid_1's tweedie: 1.42521\n",
      "Early stopping, best iteration is:\n",
      "[780]\ttraining's tweedie: 1.38119\tvalid_1's tweedie: 1.42264\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.38752\tvalid_1's tweedie: 1.4238\n",
      "[1000]\ttraining's tweedie: 1.37748\tvalid_1's tweedie: 1.4209\n",
      "Early stopping, best iteration is:\n",
      "[957]\ttraining's tweedie: 1.37842\tvalid_1's tweedie: 1.42003\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.38779\tvalid_1's tweedie: 1.42488\n",
      "Early stopping, best iteration is:\n",
      "[540]\ttraining's tweedie: 1.38675\tvalid_1's tweedie: 1.42402\n"
     ]
    }
   ],
   "source": [
    "X = preprocessing.vectorize_cnt(df)\n",
    "X = pd.concat([df[cols], X], axis=1)\n",
    "train_df = X[X.jobflag.notnull()].reset_index(drop=True)\n",
    "test_df = X[X.jobflag.isnull()].drop(columns=['jobflag']).reset_index(drop=True)\n",
    "\n",
    "off_df_cnt, test_df2_cnt = make_offdf(train_df, test_df, feature, param_list, _type='cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_df = pd.concat([\n",
    "    off_df_tfidf, \n",
    "    off_df_cnt[['lgb_preds_1_cnt', 'lgb_preds_2_cnt', 'lgb_preds_3_cnt', 'lgb_preds_4_cnt']]\n",
    " ], axis=1)\n",
    "\n",
    "test_df2 = pd.concat([\n",
    "    test_df2_tfidf, \n",
    "    test_df2_cnt[['lgb_preds_1_cnt', 'lgb_preds_2_cnt', 'lgb_preds_3_cnt', 'lgb_preds_4_cnt']]\n",
    " ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature+=[\n",
    "    'bert_pred_1_1', 'bert_pred_2_1', 'bert_pred_3_1', 'bert_pred_4_1', 'bert_pred_5_1',\n",
    "    'bert_pred_1_2', 'bert_pred_2_2', 'bert_pred_3_2', 'bert_pred_4_2', 'bert_pred_5_2',\n",
    "    'bert_pred_1_3', 'bert_pred_2_3', 'bert_pred_3_3', 'bert_pred_4_3', 'bert_pred_5_3',\n",
    "    'bert_pred_1_4', 'bert_pred_2_4', 'bert_pred_3_4', 'bert_pred_4_4', 'bert_pred_5_4',\n",
    "    'lgb_preds_1_cnt', 'lgb_preds_2_cnt', 'lgb_preds_3_cnt', 'lgb_preds_4_cnt',\n",
    "    'lgb_preds_1_tfidf', 'lgb_preds_2_tfidf', 'lgb_preds_3_tfidf', 'lgb_preds_4_tfidf'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's tweedie: 1.38711\tvalid_1's tweedie: 1.37656\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's tweedie: 1.38452\tvalid_1's tweedie: 1.37844\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's tweedie: 1.37542\tvalid_1's tweedie: 1.37499\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's tweedie: 1.3637\tvalid_1's tweedie: 1.39261\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's tweedie: 1.39096\tvalid_1's tweedie: 1.37957\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's tweedie: 0.914802\tvalid_1's tweedie: 0.959845\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's tweedie: 0.88413\tvalid_1's tweedie: 0.953883\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's tweedie: 0.837417\tvalid_1's tweedie: 0.965085\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's tweedie: 0.889376\tvalid_1's tweedie: 0.956709\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's tweedie: 0.904929\tvalid_1's tweedie: 0.957851\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's cross_entropy: 0.438843\tvalid_1's cross_entropy: 0.491383\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's cross_entropy: 0.436542\tvalid_1's cross_entropy: 0.500146\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's cross_entropy: 0.441443\tvalid_1's cross_entropy: 0.496874\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's cross_entropy: 0.443426\tvalid_1's cross_entropy: 0.49313\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's cross_entropy: 0.453223\tvalid_1's cross_entropy: 0.495293\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[190]\ttraining's tweedie: 1.28289\tvalid_1's tweedie: 1.37436\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[150]\ttraining's tweedie: 1.30311\tvalid_1's tweedie: 1.36953\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttraining's tweedie: 1.28941\tvalid_1's tweedie: 1.37429\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[142]\ttraining's tweedie: 1.30683\tvalid_1's tweedie: 1.37596\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[161]\ttraining's tweedie: 1.29688\tvalid_1's tweedie: 1.37411\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's tweedie: 1.38111\tvalid_1's tweedie: 1.41024\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's tweedie: 1.37528\tvalid_1's tweedie: 1.41255\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's tweedie: 1.38078\tvalid_1's tweedie: 1.41587\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's tweedie: 1.37537\tvalid_1's tweedie: 1.40733\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's tweedie: 1.38591\tvalid_1's tweedie: 1.41306\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's tweedie: 0.941212\tvalid_1's tweedie: 0.978004\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's tweedie: 0.932182\tvalid_1's tweedie: 0.971991\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's tweedie: 0.928712\tvalid_1's tweedie: 0.973016\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's tweedie: 0.940069\tvalid_1's tweedie: 0.973901\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's tweedie: 0.943255\tvalid_1's tweedie: 0.974436\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's cross_entropy: 0.432948\tvalid_1's cross_entropy: 0.467462\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's cross_entropy: 0.439288\tvalid_1's cross_entropy: 0.466709\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's cross_entropy: 0.440995\tvalid_1's cross_entropy: 0.465587\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's cross_entropy: 0.431946\tvalid_1's cross_entropy: 0.469288\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's cross_entropy: 0.444199\tvalid_1's cross_entropy: 0.469201\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[152]\ttraining's tweedie: 1.30518\tvalid_1's tweedie: 1.36037\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[198]\ttraining's tweedie: 1.28355\tvalid_1's tweedie: 1.34379\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[146]\ttraining's tweedie: 1.30996\tvalid_1's tweedie: 1.35223\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[169]\ttraining's tweedie: 1.29768\tvalid_1's tweedie: 1.35378\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[208]\ttraining's tweedie: 1.27958\tvalid_1's tweedie: 1.34907\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's tweedie: 1.39543\tvalid_1's tweedie: 1.43997\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's tweedie: 1.38007\tvalid_1's tweedie: 1.44309\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's tweedie: 1.39061\tvalid_1's tweedie: 1.45897\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\ttraining's tweedie: 1.38373\tvalid_1's tweedie: 1.44718\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's tweedie: 1.39036\tvalid_1's tweedie: 1.44574\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's tweedie: 0.920399\tvalid_1's tweedie: 0.979229\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's tweedie: 0.951367\tvalid_1's tweedie: 0.984117\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's tweedie: 0.952685\tvalid_1's tweedie: 0.978488\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's tweedie: 0.937863\tvalid_1's tweedie: 0.978563\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's tweedie: 0.950779\tvalid_1's tweedie: 0.98453\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's cross_entropy: 0.449044\tvalid_1's cross_entropy: 0.520222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's cross_entropy: 0.439768\tvalid_1's cross_entropy: 0.519288\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's cross_entropy: 0.439114\tvalid_1's cross_entropy: 0.517313\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's cross_entropy: 0.435042\tvalid_1's cross_entropy: 0.52093\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's cross_entropy: 0.451782\tvalid_1's cross_entropy: 0.523402\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[106]\ttraining's tweedie: 1.33022\tvalid_1's tweedie: 1.39125\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[114]\ttraining's tweedie: 1.32268\tvalid_1's tweedie: 1.38585\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's tweedie: 1.33538\tvalid_1's tweedie: 1.38065\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's tweedie: 1.34436\tvalid_1's tweedie: 1.38896\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's tweedie: 1.31713\tvalid_1's tweedie: 1.38466\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's tweedie: 1.36617\tvalid_1's tweedie: 1.4472\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's tweedie: 1.38068\tvalid_1's tweedie: 1.44373\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's tweedie: 1.37459\tvalid_1's tweedie: 1.44803\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's tweedie: 1.37567\tvalid_1's tweedie: 1.45446\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's tweedie: 1.39073\tvalid_1's tweedie: 1.45788\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's tweedie: 0.920247\tvalid_1's tweedie: 0.949001\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's tweedie: 0.893211\tvalid_1's tweedie: 0.938295\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's tweedie: 0.928276\tvalid_1's tweedie: 0.949478\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's tweedie: 0.924951\tvalid_1's tweedie: 0.942817\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's tweedie: 0.918173\tvalid_1's tweedie: 0.941441\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's cross_entropy: 0.444667\tvalid_1's cross_entropy: 0.484107\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's cross_entropy: 0.440945\tvalid_1's cross_entropy: 0.483237\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's cross_entropy: 0.439324\tvalid_1's cross_entropy: 0.47983\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's cross_entropy: 0.436845\tvalid_1's cross_entropy: 0.48269\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttraining's cross_entropy: 0.431154\tvalid_1's cross_entropy: 0.483062\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[99]\ttraining's tweedie: 1.33449\tvalid_1's tweedie: 1.39779\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[94]\ttraining's tweedie: 1.33821\tvalid_1's tweedie: 1.40677\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttraining's tweedie: 1.3315\tvalid_1's tweedie: 1.4014\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[94]\ttraining's tweedie: 1.3384\tvalid_1's tweedie: 1.40107\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[108]\ttraining's tweedie: 1.32747\tvalid_1's tweedie: 1.4055\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's tweedie: 1.3865\tvalid_1's tweedie: 1.38847\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's tweedie: 1.37582\tvalid_1's tweedie: 1.40004\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's tweedie: 1.38621\tvalid_1's tweedie: 1.38848\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's tweedie: 1.37462\tvalid_1's tweedie: 1.38729\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's tweedie: 1.39106\tvalid_1's tweedie: 1.39192\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's tweedie: 0.921037\tvalid_1's tweedie: 0.989229\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's tweedie: 0.922342\tvalid_1's tweedie: 0.988801\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\ttraining's tweedie: 0.943145\tvalid_1's tweedie: 0.995408\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's tweedie: 0.922779\tvalid_1's tweedie: 0.982064\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's tweedie: 0.921291\tvalid_1's tweedie: 0.981358\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's cross_entropy: 0.438741\tvalid_1's cross_entropy: 0.505174\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's cross_entropy: 0.440023\tvalid_1's cross_entropy: 0.509876\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's cross_entropy: 0.457814\tvalid_1's cross_entropy: 0.504359\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's cross_entropy: 0.438709\tvalid_1's cross_entropy: 0.500285\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttraining's cross_entropy: 0.426351\tvalid_1's cross_entropy: 0.506216\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttraining's tweedie: 1.33486\tvalid_1's tweedie: 1.39471\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[118]\ttraining's tweedie: 1.32121\tvalid_1's tweedie: 1.39026\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's tweedie: 1.32838\tvalid_1's tweedie: 1.39008\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttraining's tweedie: 1.32651\tvalid_1's tweedie: 1.39368\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[128]\ttraining's tweedie: 1.31326\tvalid_1's tweedie: 1.39328\n"
     ]
    }
   ],
   "source": [
    "off_df2, test_df2 = make_offdf(off_df, test_df2, feature, param_list, _type='tfidf_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobflag</th>\n",
       "      <th>text_id</th>\n",
       "      <th>bert_pred_1_1</th>\n",
       "      <th>bert_pred_2_1</th>\n",
       "      <th>bert_pred_3_1</th>\n",
       "      <th>bert_pred_4_1</th>\n",
       "      <th>bert_pred_5_1</th>\n",
       "      <th>bert_pred_1_2</th>\n",
       "      <th>bert_pred_2_2</th>\n",
       "      <th>bert_pred_3_2</th>\n",
       "      <th>...</th>\n",
       "      <th>lgb_preds_3_tfidf</th>\n",
       "      <th>lgb_preds_4_tfidf</th>\n",
       "      <th>lgb_preds_1_cnt</th>\n",
       "      <th>lgb_preds_2_cnt</th>\n",
       "      <th>lgb_preds_3_cnt</th>\n",
       "      <th>lgb_preds_4_cnt</th>\n",
       "      <th>lgb_preds_1_tfidf_2</th>\n",
       "      <th>lgb_preds_2_tfidf_2</th>\n",
       "      <th>lgb_preds_3_tfidf_2</th>\n",
       "      <th>lgb_preds_4_tfidf_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>82</td>\n",
       "      <td>0.454008</td>\n",
       "      <td>0.175759</td>\n",
       "      <td>0.135985</td>\n",
       "      <td>0.146271</td>\n",
       "      <td>0.129238</td>\n",
       "      <td>0.366413</td>\n",
       "      <td>0.663031</td>\n",
       "      <td>0.249829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680642</td>\n",
       "      <td>0.311793</td>\n",
       "      <td>0.115256</td>\n",
       "      <td>0.072991</td>\n",
       "      <td>0.600513</td>\n",
       "      <td>0.339703</td>\n",
       "      <td>0.070656</td>\n",
       "      <td>0.029855</td>\n",
       "      <td>0.712662</td>\n",
       "      <td>0.112588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>85</td>\n",
       "      <td>0.469055</td>\n",
       "      <td>0.302547</td>\n",
       "      <td>0.162259</td>\n",
       "      <td>0.169569</td>\n",
       "      <td>0.099552</td>\n",
       "      <td>0.549090</td>\n",
       "      <td>0.674342</td>\n",
       "      <td>0.708079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561328</td>\n",
       "      <td>0.255754</td>\n",
       "      <td>0.087106</td>\n",
       "      <td>0.109901</td>\n",
       "      <td>0.428973</td>\n",
       "      <td>0.238634</td>\n",
       "      <td>0.096199</td>\n",
       "      <td>0.214362</td>\n",
       "      <td>0.596812</td>\n",
       "      <td>0.129877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>88</td>\n",
       "      <td>0.391455</td>\n",
       "      <td>0.236896</td>\n",
       "      <td>0.169082</td>\n",
       "      <td>0.157534</td>\n",
       "      <td>0.116313</td>\n",
       "      <td>0.454449</td>\n",
       "      <td>0.327076</td>\n",
       "      <td>0.211715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807319</td>\n",
       "      <td>0.049357</td>\n",
       "      <td>0.051540</td>\n",
       "      <td>0.078550</td>\n",
       "      <td>0.809162</td>\n",
       "      <td>0.056888</td>\n",
       "      <td>0.009910</td>\n",
       "      <td>0.032098</td>\n",
       "      <td>0.848295</td>\n",
       "      <td>0.022426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>114</td>\n",
       "      <td>0.423601</td>\n",
       "      <td>0.176318</td>\n",
       "      <td>0.118656</td>\n",
       "      <td>0.108090</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.385015</td>\n",
       "      <td>0.263328</td>\n",
       "      <td>0.254876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781202</td>\n",
       "      <td>0.082821</td>\n",
       "      <td>0.139298</td>\n",
       "      <td>0.080233</td>\n",
       "      <td>0.694847</td>\n",
       "      <td>0.114137</td>\n",
       "      <td>0.042194</td>\n",
       "      <td>0.032519</td>\n",
       "      <td>0.804370</td>\n",
       "      <td>0.071299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>140</td>\n",
       "      <td>0.509585</td>\n",
       "      <td>0.488802</td>\n",
       "      <td>0.476889</td>\n",
       "      <td>0.317098</td>\n",
       "      <td>0.273337</td>\n",
       "      <td>0.377859</td>\n",
       "      <td>0.616022</td>\n",
       "      <td>0.767837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618995</td>\n",
       "      <td>0.248506</td>\n",
       "      <td>0.063244</td>\n",
       "      <td>0.098146</td>\n",
       "      <td>0.663062</td>\n",
       "      <td>0.234907</td>\n",
       "      <td>0.070465</td>\n",
       "      <td>0.098263</td>\n",
       "      <td>0.688054</td>\n",
       "      <td>0.078104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2890</td>\n",
       "      <td>0.428777</td>\n",
       "      <td>0.282805</td>\n",
       "      <td>0.183124</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>0.111246</td>\n",
       "      <td>0.487478</td>\n",
       "      <td>0.277243</td>\n",
       "      <td>0.222191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414978</td>\n",
       "      <td>0.709427</td>\n",
       "      <td>0.102473</td>\n",
       "      <td>0.031436</td>\n",
       "      <td>0.275221</td>\n",
       "      <td>0.651165</td>\n",
       "      <td>0.051504</td>\n",
       "      <td>0.031439</td>\n",
       "      <td>0.287321</td>\n",
       "      <td>0.731688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2907</td>\n",
       "      <td>0.614129</td>\n",
       "      <td>0.815955</td>\n",
       "      <td>0.781114</td>\n",
       "      <td>0.887786</td>\n",
       "      <td>0.907913</td>\n",
       "      <td>0.512127</td>\n",
       "      <td>0.759280</td>\n",
       "      <td>0.836819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138497</td>\n",
       "      <td>0.056981</td>\n",
       "      <td>0.499546</td>\n",
       "      <td>0.181583</td>\n",
       "      <td>0.132044</td>\n",
       "      <td>0.047214</td>\n",
       "      <td>0.624778</td>\n",
       "      <td>0.361019</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.046876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2916</td>\n",
       "      <td>0.429295</td>\n",
       "      <td>0.225428</td>\n",
       "      <td>0.389456</td>\n",
       "      <td>0.492671</td>\n",
       "      <td>0.436080</td>\n",
       "      <td>0.313882</td>\n",
       "      <td>0.460332</td>\n",
       "      <td>0.479630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483048</td>\n",
       "      <td>0.237682</td>\n",
       "      <td>0.114135</td>\n",
       "      <td>0.083715</td>\n",
       "      <td>0.508330</td>\n",
       "      <td>0.228604</td>\n",
       "      <td>0.165755</td>\n",
       "      <td>0.156996</td>\n",
       "      <td>0.542018</td>\n",
       "      <td>0.173151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2924</td>\n",
       "      <td>0.564244</td>\n",
       "      <td>0.726400</td>\n",
       "      <td>0.636942</td>\n",
       "      <td>0.844422</td>\n",
       "      <td>0.897441</td>\n",
       "      <td>0.526862</td>\n",
       "      <td>0.706846</td>\n",
       "      <td>0.795487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242756</td>\n",
       "      <td>0.142191</td>\n",
       "      <td>0.237105</td>\n",
       "      <td>0.145984</td>\n",
       "      <td>0.243798</td>\n",
       "      <td>0.174133</td>\n",
       "      <td>0.584363</td>\n",
       "      <td>0.343823</td>\n",
       "      <td>0.208213</td>\n",
       "      <td>0.074696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2930</td>\n",
       "      <td>0.603844</td>\n",
       "      <td>0.826770</td>\n",
       "      <td>0.820659</td>\n",
       "      <td>0.859095</td>\n",
       "      <td>0.882771</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.350853</td>\n",
       "      <td>0.195399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138597</td>\n",
       "      <td>0.066571</td>\n",
       "      <td>0.555052</td>\n",
       "      <td>0.134372</td>\n",
       "      <td>0.194957</td>\n",
       "      <td>0.079166</td>\n",
       "      <td>0.609760</td>\n",
       "      <td>0.112357</td>\n",
       "      <td>0.160332</td>\n",
       "      <td>0.075957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2886 rows × 3417 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      jobflag  text_id  bert_pred_1_1  bert_pred_2_1  bert_pred_3_1  \\\n",
       "0         3.0       82       0.454008       0.175759       0.135985   \n",
       "1         3.0       85       0.469055       0.302547       0.162259   \n",
       "2         3.0       88       0.391455       0.236896       0.169082   \n",
       "3         3.0      114       0.423601       0.176318       0.118656   \n",
       "4         3.0      140       0.509585       0.488802       0.476889   \n",
       "...       ...      ...            ...            ...            ...   \n",
       "2881      4.0     2890       0.428777       0.282805       0.183124   \n",
       "2882      1.0     2907       0.614129       0.815955       0.781114   \n",
       "2883      3.0     2916       0.429295       0.225428       0.389456   \n",
       "2884      3.0     2924       0.564244       0.726400       0.636942   \n",
       "2885      2.0     2930       0.603844       0.826770       0.820659   \n",
       "\n",
       "      bert_pred_4_1  bert_pred_5_1  bert_pred_1_2  bert_pred_2_2  \\\n",
       "0          0.146271       0.129238       0.366413       0.663031   \n",
       "1          0.169569       0.099552       0.549090       0.674342   \n",
       "2          0.157534       0.116313       0.454449       0.327076   \n",
       "3          0.108090       0.041600       0.385015       0.263328   \n",
       "4          0.317098       0.273337       0.377859       0.616022   \n",
       "...             ...            ...            ...            ...   \n",
       "2881       0.139200       0.111246       0.487478       0.277243   \n",
       "2882       0.887786       0.907913       0.512127       0.759280   \n",
       "2883       0.492671       0.436080       0.313882       0.460332   \n",
       "2884       0.844422       0.897441       0.526862       0.706846   \n",
       "2885       0.859095       0.882771       0.328358       0.350853   \n",
       "\n",
       "      bert_pred_3_2  ...  lgb_preds_3_tfidf  lgb_preds_4_tfidf  \\\n",
       "0          0.249829  ...           0.680642           0.311793   \n",
       "1          0.708079  ...           0.561328           0.255754   \n",
       "2          0.211715  ...           0.807319           0.049357   \n",
       "3          0.254876  ...           0.781202           0.082821   \n",
       "4          0.767837  ...           0.618995           0.248506   \n",
       "...             ...  ...                ...                ...   \n",
       "2881       0.222191  ...           0.414978           0.709427   \n",
       "2882       0.836819  ...           0.138497           0.056981   \n",
       "2883       0.479630  ...           0.483048           0.237682   \n",
       "2884       0.795487  ...           0.242756           0.142191   \n",
       "2885       0.195399  ...           0.138597           0.066571   \n",
       "\n",
       "      lgb_preds_1_cnt  lgb_preds_2_cnt  lgb_preds_3_cnt  lgb_preds_4_cnt  \\\n",
       "0            0.115256         0.072991         0.600513         0.339703   \n",
       "1            0.087106         0.109901         0.428973         0.238634   \n",
       "2            0.051540         0.078550         0.809162         0.056888   \n",
       "3            0.139298         0.080233         0.694847         0.114137   \n",
       "4            0.063244         0.098146         0.663062         0.234907   \n",
       "...               ...              ...              ...              ...   \n",
       "2881         0.102473         0.031436         0.275221         0.651165   \n",
       "2882         0.499546         0.181583         0.132044         0.047214   \n",
       "2883         0.114135         0.083715         0.508330         0.228604   \n",
       "2884         0.237105         0.145984         0.243798         0.174133   \n",
       "2885         0.555052         0.134372         0.194957         0.079166   \n",
       "\n",
       "      lgb_preds_1_tfidf_2  lgb_preds_2_tfidf_2  lgb_preds_3_tfidf_2  \\\n",
       "0                0.070656             0.029855             0.712662   \n",
       "1                0.096199             0.214362             0.596812   \n",
       "2                0.009910             0.032098             0.848295   \n",
       "3                0.042194             0.032519             0.804370   \n",
       "4                0.070465             0.098263             0.688054   \n",
       "...                   ...                  ...                  ...   \n",
       "2881             0.051504             0.031439             0.287321   \n",
       "2882             0.624778             0.361019             0.138889   \n",
       "2883             0.165755             0.156996             0.542018   \n",
       "2884             0.584363             0.343823             0.208213   \n",
       "2885             0.609760             0.112357             0.160332   \n",
       "\n",
       "      lgb_preds_4_tfidf_2  \n",
       "0                0.112588  \n",
       "1                0.129877  \n",
       "2                0.022426  \n",
       "3                0.071299  \n",
       "4                0.078104  \n",
       "...                   ...  \n",
       "2881             0.731688  \n",
       "2882             0.046876  \n",
       "2883             0.173151  \n",
       "2884             0.074696  \n",
       "2885             0.075957  \n",
       "\n",
       "[2886 rows x 3417 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(df,n,random_state):\n",
    "    df_0 = df[df['k_group']==0]\n",
    "    df_1 = df[df['k_group']==1]\n",
    "    df_2 = df[df['k_group']==2]\n",
    "    \n",
    "    l_0 = len(df_0)\n",
    "    l_1 = len(df_1)\n",
    "    l_2 = len(df_2)\n",
    "    \n",
    "    n_0 = int(n/3) if int(n/3)<l_0 else l_0\n",
    "    n_1 = int(n/3) if int(n/3)<l_1 else l_1\n",
    "    n_2 = int(n/3) if int(n/3)<l_2 else l_2\n",
    "    \n",
    "    sample_df = pd.concat([\n",
    "        df_0.sample(n_0, random_state=random_state),\n",
    "        df_1.sample(n_1, random_state=random_state),\n",
    "        df_2.sample(n_2, random_state=random_state),\n",
    "    ], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['xmachinelearn', 'xp', 'xrd', 'yarn', 'year', 'yearli', 'yellow', 'yet',\n",
       "       'yield', 'younger', 'zeiss', 'zookeep', 'lgb_preds_1_tfidf',\n",
       "       'lgb_preds_2_tfidf', 'lgb_preds_3_tfidf', 'lgb_preds_4_tfidf',\n",
       "       'lgb_preds_1_cnt', 'lgb_preds_2_cnt', 'lgb_preds_3_cnt',\n",
       "       'lgb_preds_4_cnt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_df.columns[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans_clustering(off_df, random_state):\n",
    "    off_df['k_group'] = np.nan\n",
    "    for jobflag in [1,3,4]:\n",
    "        k = KMeans(n_clusters=3, random_state=random_state)\n",
    "        k.fit(off_df[off_df.jobflag==jobflag][feature])\n",
    "\n",
    "        off_df.loc[off_df.jobflag==jobflag, 'k_group'] = k.predict(off_df[off_df.jobflag==jobflag][feature])\n",
    "    return off_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0979c6fd4304531a91a244518406fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0800e47a404f64b74411526c57490f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e456bc5cd8544cb9b1e8bc24f47a16ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f279eeb497344c283556addf7da933e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a3d01264f949239724b29cf66d281b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "off_df_2=[]\n",
    "test_preds = np.zeros(shape=(len(test_df2),4))\n",
    "pred_cols =['bert_pred_1_1', 'bert_pred_2_1', 'bert_pred_3_1', 'bert_pred_4_1', 'bert_pred_5_1',\n",
    " 'bert_pred_1_2', 'bert_pred_2_2', 'bert_pred_3_2', 'bert_pred_4_2', 'bert_pred_5_2',\n",
    " 'bert_pred_1_3', 'bert_pred_2_3', 'bert_pred_3_3', 'bert_pred_4_3', 'bert_pred_5_3',\n",
    " 'bert_pred_1_4', 'bert_pred_2_4', 'bert_pred_3_4', 'bert_pred_4_4', 'bert_pred_5_4',\n",
    " 'lgb_preds_1_tfidf', 'lgb_preds_2_tfidf', 'lgb_preds_3_tfidf', 'lgb_preds_4_tfidf',\n",
    " 'lgb_preds_1_cnt', 'lgb_preds_2_cnt', 'lgb_preds_3_cnt', 'lgb_preds_4_cnt',\n",
    " 'lgb_preds_1_tfidf_2', 'lgb_preds_2_tfidf_2', 'lgb_preds_3_tfidf_2', 'lgb_preds_4_tfidf_2']\n",
    "\n",
    "for trn, val in k.split(train_df, train_df.jobflag):\n",
    "    trn_df = off_df2.iloc[trn,:]\n",
    "    val_df  =  off_df2.iloc[val,:]\n",
    "    \n",
    "    min_value = trn_df.jobflag.value_counts().min()\n",
    "    \n",
    "    preds = np.zeros(shape=(len(val_df),4))\n",
    "    \n",
    "    for i in tqdm(range(80)):\n",
    "        tmp_trn_df = pd.concat(\n",
    "        [trn_df[trn_df.jobflag==1].sample(n=min_value, random_state=i),\n",
    "         trn_df[trn_df.jobflag==2].sample(n=min_value, random_state=i),\n",
    "         trn_df[trn_df.jobflag==3].sample(n=min_value, random_state=i),\n",
    "         trn_df[trn_df.jobflag==4].sample(n=min_value, random_state=i)], axis=0).reset_index(drop=True)\n",
    "        tmp_trn_X = tmp_trn_df[pred_cols]\n",
    "        tmp_trn_y = tmp_trn_df['jobflag']\n",
    "        \n",
    "        \n",
    "        for penalty  in [ 'l2']:\n",
    "            for m in range(5):\n",
    "                logit = LogisticRegression(penalty=penalty, random_state=m)\n",
    "                logit.fit(tmp_trn_X, tmp_trn_y)\n",
    "\n",
    "                    #ridge_cls = RidgeClassifier()\n",
    "                    #ridge_cls.fit(tmp_trn_X, tmp_trn_y)\n",
    "\n",
    "                    #kncls = KNeighborsClassifier(n_neighbors=4)\n",
    "                    #kncls.fit(tmp_trn_X, tmp_trn_y)\n",
    "                preds += logit.predict_proba(val_df[pred_cols])\n",
    "                test_preds += logit.predict_proba(test_df2[pred_cols])\n",
    "                \n",
    "    val_df[f'preds'] = np.argmax(preds, axis=1)+1\n",
    "    off_df_2.append(val_df)\n",
    "\n",
    "test_df2[f'preds'] = np.argmax(test_preds, axis=1)+1\n",
    "off_df_2 = pd.concat(off_df_2, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5577924735950446\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>342</td>\n",
       "      <td>129</td>\n",
       "      <td>66</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>147</td>\n",
       "      <td>74</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85</td>\n",
       "      <td>106</td>\n",
       "      <td>891</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>25</td>\n",
       "      <td>136</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3\n",
       "0  342  129   66   83\n",
       "1   84  147   74   32\n",
       "2   85  106  891  269\n",
       "3   52   25  136  365"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(metrics.f1_score(off_df_2.jobflag, off_df_2.preds, average='macro'))\n",
    "plt.figure(figsize=(10,10))\n",
    "cnfn_matrix = pd.DataFrame(metrics.confusion_matrix(off_df_2.jobflag, off_df_2.preds))\n",
    "#cnfn_matrix.index = \n",
    "sns.heatmap(cnfn_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "cnfn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_p2 = off_df_2[off_df_2.jobflag==1][off_df_2.preds==2]['text_id'].values.tolist()\n",
    "t2_p1 = off_df_2[off_df_2.jobflag==2][off_df_2.preds==1]['text_id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Work in a startup-type environment to design and build innovative applications using Automation, Machine Learning/Cognitive Service, business / functional proofs-of-concepts to scalable, production ready solutions across a strategic business group',\n",
       "       'Strategize in collaboration with functional teams to identify data-driven solutions to ongoing needs across the organization.',\n",
       "       'Inform bidding strategy and data engineering architecture.',\n",
       "       'Defining data augmentation pipelines',\n",
       "       'Develop AI/ML solutions based on NLP techniques such as key phrases extraction, Topics, themes, summarization, entity extraction and Sentiment analysis',\n",
       "       'Plan and build data-driven banking ecosystem, including ecosystem support models for customer experience and marketing.',\n",
       "       'Selecting features, building and optimizing classifiers using machine learning techniques',\n",
       "       'Build API interface for AI/Client models',\n",
       "       'Builds a deep understanding of the Company s products, services, data and customers to facilitate development of personalized and fulfilling experience.',\n",
       "       'Collaborate with functional leaders to translate business needs into differentiated products & solutions, by driving business case & proforma development.',\n",
       "       'Work with other development teams to integrate models into user applications',\n",
       "       'Communicate analytic solutions to stakeholders and implement improvements as needed to operational systems',\n",
       "       'Evaluate accuracy and quality of the designed models as well as data sources.',\n",
       "       'Mastery of anomaly detection, forecasting, and clustering algorithms',\n",
       "       \"An ideal candidate for the Lead Machine Learning Scientist role should have practical experience, a love of experimentation, and a passion for the problem we're trying to solve.\",\n",
       "       'Build data processing pipeline for deep learning tasks',\n",
       "       'Build production grade machine learning pipelines and data products that solve critical, complex problems and deliver business value',\n",
       "       'Defining/designing a rough data flow of whole systems',\n",
       "       'Advise and collaborate in internal partnerships to deliver accurate, available, and accessible data.',\n",
       "       'Be the interface between the data warehouse and the data science team, using transformation frameworks to automate dataset generation',\n",
       "       'Collaborating with a globallyl distributed team',\n",
       "       'prototype new algorithms and productionize solutions at scale',\n",
       "       'Analyze data to effectively coordinate the installation of new systems or modifications to existing systems.',\n",
       "       'Execute analytics strategy, requirements, and adapt processes as needed.',\n",
       "       'Train, calibrate and validate computer vision algorithms to extract objects, text etc.',\n",
       "       'Design, implement and optimize algorithms for unsupervised and supervised learning based on structured and unstructured data.',\n",
       "       'Build robust and interpretable models that predict how digital search engagement leads to real-world health outcomes.',\n",
       "       'Mentor, coach, train and lead fellow data scientists and analysts.',\n",
       "       'Evaluate predictive ability of datasets',\n",
       "       'Establish scalable, efficient, automated processes for data analyses, model development, validation and implementation',\n",
       "       \"Identify strong AI/ML use cases in cloud security, data security and adjacent domains, leveraging Netskope's rich set of data sources\",\n",
       "       'Leverage the latest machine and deep learning techniques to challenge our current practices in supply chain, procurement, sales, marketing, operations, claims, etc.',\n",
       "       'Effectively explain technical concepts at all levels in the organization, including senior managers/stakeholders',\n",
       "       'Lead the application of AI and machine-learning tools to the media planning and buying process and develop new proprietary tools, prioritizing IP ownership.',\n",
       "       'Guide creative solutions to challenging scientific questions from early proof-of-concept stages to production deployments',\n",
       "       'Build custom predictive models and machine-learning algorithms to optimize business outcomes',\n",
       "       'Optimize the Python code to reduce the runtime.',\n",
       "       'Develop replicable analyses and a reusable code base for understanding and enhancing search performance.',\n",
       "       'Work on rich, high-quality training data set creation, augmenting and annotation',\n",
       "       'Coordinate with cross-functional teams to implement models, monitor outcomes, and make enhancements.',\n",
       "       'Enhance data collection procedures to include information that is relevant for building analytic systems',\n",
       "       'Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, building and testing solutions and data models, and implementing the algorithms and results of the models',\n",
       "       'Develop efficient feature capabilities to support proper data extraction & curation for analytical purposes',\n",
       "       'Flexibility of working across all functions/levels as part of a team.',\n",
       "       'Designing systems to monitor the results of models in productions, discover and address anomalies, and ensure the robustness and reliability of these models',\n",
       "       'Testing of ML models, such as cross-validation, A/B testing, bias and fairness',\n",
       "       'Verifying data quality, and/or ensuring it via data cleaning',\n",
       "       'Support team in building approach (e.g., systems, personnel, processes) for end-to-end data services.',\n",
       "       'Keep up to date with latest technology trends in machine and deep learning and quickly learn about new frameworks/techniques to be used in projects delivery',\n",
       "       'Continually learn new analytical skills, techniques and tools to maximise competitive advantage; participate in internal & external technology communities',\n",
       "       'Working alongside machine learning experts, technologists, product managers and other informatics analysts to solve complex business problems.',\n",
       "       'Optimize joint development efforts through appropriate database use and project design.',\n",
       "       'Communicating with clients, internal and external, to understand the business problems that we can solve using machine learning and AI',\n",
       "       'Analyze the quality and calibration of predictive models.',\n",
       "       'Be the SQL/Python/Spark/EMR/Snowflake expert on the team and help with data wrangling',\n",
       "       'Research and develop analyses and forecasting and optimization methods across ads quality, search quality, end-user behavioral modeling, and live experiments.',\n",
       "       'Develop, implement, and use a broad set of machine-learning models and quantitative techniques for prediction and classification for mission critical applications',\n",
       "       'Assess the effectiveness and accuracy of new data sources and data gathering techniques.',\n",
       "       'Develop and deliver communication & education plans on analytic data engineering capabilities, standards, and processes',\n",
       "       'Collaborates with Product Management, Development, and Research teams in the creation and implementation of machine learning algorithms and statistical models.',\n",
       "       'Select ML models for defined use cases. Implement KPIs to ensure optimal algorithms and results',\n",
       "       'Candidates should be able to work in large data sets in a big data (Hadoop, Spark) framework is required.',\n",
       "       'Maintain awareness of state-of-the-art machine learning and techniques, methods and platforms, including commercial and open source.',\n",
       "       'Developing and deploying solutions with Microsoft Partners for solving business problems using machine (deep) learning and Azure data platforms',\n",
       "       'Constantly evaluate new technologies, ideas, concepts, and designs',\n",
       "       'Applying machine learning and predictive modeling techniques',\n",
       "       'Analysis of current technologies and assess their fit for purpose',\n",
       "       'Develop and prototype deep learning models for subsurface data covering from large scale of 3D seismic data, well log, unstructured data, etc.',\n",
       "       'Seek out which areas need what kind of data and what their analytical requirements are and work on getting that data in the correct data model',\n",
       "       'Possesses in depth understanding of building streaming, ingestion, processing data pipelines.',\n",
       "       'Work closely with our data infrastructure team and other domain SMEs by giving requirements for new data acquisitions',\n",
       "       'Demonstrate self-directed and proactive approach to tackling problems and leveraging resources.',\n",
       "       'Utilize statistical methods to process, clean and validate data for uniformity and accuracy',\n",
       "       'Develop and establish model management technology and processes for helping enable MLOps such as: measure on-going accuracy, tooling for data / model drift, etc.',\n",
       "       'Collaborate closely within an interdisciplinary team composed of scientists, developers, and drug hunters to deliver new research that meet business and product goals',\n",
       "       'Learn, develop, and apply new techniques in the intersection of math, probability, and optimization',\n",
       "       'Develop the skills of data team through mentorship and training',\n",
       "       'Use knowledge of Python for implementing Python based workflow solutions such as Airflow or MLflow',\n",
       "       \"Implementing the Scoring Platform's data intensive infrastructure and APIs\",\n",
       "       'Stay abreast of emerging machine learning and data science technologies.',\n",
       "       'Analyzing, extracting, and optimizing relevant information from     large amounts of data to help drive business decisions on product     features and operational efficiency',\n",
       "       'Developing, improving, and leading modeling for Product and Solution teams in order to complete proof-of-concept case studies for existing and prospective clients.',\n",
       "       'Strong analytical and problem-solving skills',\n",
       "       'Feature Engineering to prepare data sets as available from our acquisition pipeline, ontology and graph to build robust models',\n",
       "       'Work closely with data scientists to ensure the source data is aggregated and cleansed.',\n",
       "       'Research, design, implement, and oversee high-end analytical/technology process and solutions with a focus on leveraging advanced machine learning, artificial intelligence and cognitive methods.',\n",
       "       'Planning, developing, and applying cutting-edge machine learning systems and statistical modeling to extract insight from vast amounts of data at scale',\n",
       "       'Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions',\n",
       "       'Compare results from various methodologies and recommend best techniques to stake holders',\n",
       "       'Leveraging the latest machine and deep learning techniques to challenge the current practices across the business units',\n",
       "       'Undertake concept-and-feasibility projects related to semiconductor process control using statistical, machine learning, and first-principle modeling methods.',\n",
       "       'Developing data driven culture in team',\n",
       "       'Design data pipelines to gather and store large quantities of cyber security data',\n",
       "       'Enhance optimization models based on measurement opportunities',\n",
       "       'Exploring and visualizing data to gain an understanding of it, then identifying differences in data distribution that could affect',\n",
       "       'Develop and embed automated processes for predictive model validation, deployment, and implementation.',\n",
       "       'Generalizing solutions and innovating to create the next generation of product features',\n",
       "       'Develop machine learning, data mining, statistical, and graph-based algorithms designed to analyze massive data sets; partner with Cloud technologists to ensure proper implementation and usage of said algorithms.',\n",
       "       'Investigate for trend/challenge in data mining and machine learning area',\n",
       "       \"Manage and support the organization's cloud-based data and computing platforms and infrastructure for AI applications\",\n",
       "       'Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.',\n",
       "       'Possess a solid machine-learning foundation',\n",
       "       'Work with appropriate stakeholders to identify and scope promising applications of machine learning',\n",
       "       'Apply machine learning, collaborative filtering, NLP, and deep learning methods to massive data sets.',\n",
       "       'Apply simple and complex statistical theories and methods to explore, infer and predict;',\n",
       "       'Recommendation algorithms/IR: Understand IR or ML really well can explain MF, latent, KNN, Community cluster',\n",
       "       'Develop and evaluate the performance of predictive statistical models and machine learning algorithms',\n",
       "       'Create data products for analytics and data scientist team members to improve their productivity',\n",
       "       'Perform automated data labeling to train AI models',\n",
       "       'Analyze data from disparate sources including images, video, radar, signals and more',\n",
       "       'Understand available data, including both in structured and unstructured formats, and recommend effective ways for storage and analytical processing in the cloud.',\n",
       "       'Translate strategic analytics requirements into Analytics tools and infrastructure',\n",
       "       'Compare results from various methodologies and recommend best techniques to stake holders.',\n",
       "       'Lead development efforts to build efficient feature capabilities to support proper data extraction & curation for analytical purposes',\n",
       "       'Must be highly responsive and able to produce clinical content very quickly',\n",
       "       'Build complex advanced-level machine learning and advanced     analytics models.',\n",
       "       'Study and transform data science prototypes',\n",
       "       'Partner in development of scalable solutions using large datasets with other data scientists on the team',\n",
       "       'Leverage the optimum machine learning techniques for predictions in various Business Units such as Procurement, Sales, Marketing, Operations, etc. to enhance revenue and reduce costs.',\n",
       "       'Collaborate with the digital and DSAI teams',\n",
       "       'Analyze and extract relevant information from large amounts of historical data - provide hands-on data wrangling expertise',\n",
       "       'Deriving technical parameters and specifications from measured data',\n",
       "       'Identify the most appropriate algorithm for a given data set and tune input and model parameters',\n",
       "       'Establish a universal standard, process, and repository of predictive and machine learning models to support team documentation and organizational sharing.',\n",
       "       'Creating and Finding available datasets online that could be used for training',\n",
       "       'Partner with internal data scientists, developers, and tech teams to develop new methodologies and utilities',\n",
       "       'Perform DevOps and orchestration for all data hub and analytical processes.',\n",
       "       'Design, build, deploy, and support scalable statistical and machine learning models',\n",
       "       'Build data into everything - and analyze it to find efficiencies, make automatons, and predict outcomes.',\n",
       "       'Characterize aspects of our ML system and recommend ways it can be improved',\n",
       "       'Support the incorporation of RWE methods and perspectives into global development strategies',\n",
       "       'Sourcing, cleansing, structuring and ingesting new data sources',\n",
       "       'Deal with enormous amounts of video data, and developing solutions in a distributed setting',\n",
       "       'Research machine learning (including deep learning) approaches and apply them to tackle real-world problems in construction project life cycle, including but not limited to knowledge extraction from unstructured data, construction site object detection and tracking etc.',\n",
       "       'Work with internal clients and the Data Science team to solve problems',\n",
       "       'Collaborate with external clients and internal departments to understand company needs and devise possible solutions leveraging the power of Machine Learning.',\n",
       "       'Designing and applying (machine learning) algorithms for extraction of information from data of any kind',\n",
       "       'Stay up to date with tech, prototype with and learn new technologies, proactive in technology communities',\n",
       "       'Identify opportunities to apply Machine Learning and Artificial Intelligence to build, test, and validate predictive models.',\n",
       "       'Apply expertise in Machine Learning and data mining techniques: feature engineering, regression, classification, anomaly detection, time series analysis etc',\n",
       "       'Collaborate with software development to implement and deploy newly developed technologies and algorithms',\n",
       "       'Build an ad targeting system to find the most relevant users for ads through interest, keyword and audience targeting',\n",
       "       'Write production code to produce data infrastructure and automation of tasks',\n",
       "       'Communicate complex machine learning ideas, designs, and processes to both technical and non-technical stakeholders.',\n",
       "       'Learn the business inside and out, interfacing with domain knowledge experts',\n",
       "       'Apply Machine Learning and Artificial Intelligence expertise to build prototypes and translate them into product features.',\n",
       "       'Manage the full life cycle of model builds.',\n",
       "       'Working with Software Engineers and SaaS Operations to package and deploy the models',\n",
       "       'Build predictive models and machine-learning algorithms',\n",
       "       'Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals.',\n",
       "       'Understanding business objectives and developing models that help to achieve them, along with metrics to track their progress',\n",
       "       'Provide technical leadership to a strong team of data scientists, engineers, and analysts.',\n",
       "       'Take a hands-on role and coach data science teams to deliver on highly visible multiple projects.',\n",
       "       'Build APIs and other ETL processes to support the AI/ML models',\n",
       "       'Select appropriate datasets and data representation methods',\n",
       "       'Exercises judgment in selecting optimal AI/ML methods, techniques and evaluation criteria for obtaining results',\n",
       "       'Research new techniques and best practices within the industry.',\n",
       "       'Represent ECD AI in meetings with external and internal stakeholders',\n",
       "       'Build end-to-end production level Machine Learning/AI solutions',\n",
       "       'Optimize algorithms for efficiency, speed, and usability.',\n",
       "       'Research new technologies and methods across data science, data engineering, and data visualization to improve the technical capabilities of the team.',\n",
       "       'Evaluate various algorithmic approaches in information extraction',\n",
       "       'Hunting for quality datasets, including development from scratch (setting up the right process), for-fee acquisition of existing data, and bootstrapping corpora from messy and/or limited data sources',\n",
       "       'Deep dive into data through systematic and ad hoc analyses and build machine learning models to score the identity, behavior, threat and other risk characteristics of the various entities.',\n",
       "       'Identify what kind of data in which format should be collected in order to solve the problem.',\n",
       "       'Effectively explain technical concepts at all levels in the organization, including senior managers/stakeholders.',\n",
       "       'Providing technical subject matter expertise in OSS and Microsoft big data technologies, to support architecture design sessions, and unblock critical scenarios for our partners.',\n",
       "       'Deliver on time with a high bar on quality of research, innovation and engineering',\n",
       "       'Collaborate with Marketing in the design, implementation, and analysis of marketplace research',\n",
       "       'Collaborate with data team for in-house data management and labelling',\n",
       "       'Develop new ideas into AI driven products using our wealth of sports data',\n",
       "       'Help customers translate their data and problems in ways that our ML system can best solve them',\n",
       "       'Leverage DevOps and Data Architecture teams to deploy your code at scale.',\n",
       "       'Collaborate with a cross functional agile team spanning user research, design, data science, product management, and engineering to build new product features that advance our mission to connect artists and fans in personalized and relevant ways',\n",
       "       'Develop and embed automated processes for predictive model validation, deployment, and implementation',\n",
       "       'Combine data features to determine search models.',\n",
       "       'Build testing frameworks to constantly improve model iterations and compare the performance of models and communicate results with stakeholders',\n",
       "       'Author formal and informal reports that effectively communicate the models and the results to both technical and non-technical audiences',\n",
       "       'Developing supervised, unsupervised, and reinforcement models in the laboratory',\n",
       "       'Implement batch and real-time model scoring to drive actions.',\n",
       "       'Writing simulation code using Scalding to run MapReduce jobs on our Hadoop cluster to help us understand what would happen across different segments if we changed how we action our models.',\n",
       "       'This role conducts experimentation in various data science techniques, developing, executing, and maintaining scripts and prototypes to analyze, interpret, visualize, and gain knowledge from numerous data sets individually or in combination to meet mission needs.',\n",
       "       'Create algorithms to extract information from large, multiparametric data sets',\n",
       "       'Coordinate with AI and Olfactometry teams at the Parent Company to align and prioritize with global strategy on data science and machine learning',\n",
       "       'Closely work with the product and engineering teams to ensure high impact products',\n",
       "       'Develop integrated software algorithms to structure, analyze and use data in product and systems applications in both structured and unstructured environments',\n",
       "       'Develop models of current state in order to determine improvements needed.',\n",
       "       'Work closely with product teams to in order to drive maximum product enhancements through the use of research',\n",
       "       'Responsible for providing line of sight to data quality and gaps where issues need to be addressed.',\n",
       "       'Exploring and visualizing data to gain an understanding of it, then identifying differences in data distribution that could affect performance when deploying the model in the real world',\n",
       "       'Communicate key project data to team members and build team cohesion and effectiveness.',\n",
       "       'Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions.',\n",
       "       'Research, actively experiment to stay abreast of the emerging ML and Cognitive Automation trends',\n",
       "       'Conduct analysis by mining internal and external data sources',\n",
       "       'Design and develop relational databases for collecting data',\n",
       "       'Communicate data collection and statistical methodologies to both technical and nontechnical audiences',\n",
       "       'Support Data Science team with any data movements, data transformation, and analysis, etc.',\n",
       "       'Help build econometric models to drive operational excellence',\n",
       "       'Develop highly accurate machine learning models to help improve in-product experiences and drive business growth',\n",
       "       'Explore and utilize prepackaged ML services where it makes sense',\n",
       "       'Work closely with our Data team to extend and improve our Food Genome by applying NLP based methods to this complex domain',\n",
       "       'Develop an understanding of Conversant personalization platform and proprietary datasets.',\n",
       "       'Transform and process the data sets to maximize accuracy, performance, and ML transparency',\n",
       "       'Collaborate with Software Engineers to streamline and optimize model building and deployment.',\n",
       "       'Make impactful contributions to internal discussions on emerging machine learning methodologies.',\n",
       "       'Build and operate infrastructure, toolset, and deployment pipelines.',\n",
       "       'Build machine learning pipeline solutions for Data Science models developed by consumer bank, including core banking, cards and wealth management businesses.',\n",
       "       'Train algorithms to apply models to new data sets.',\n",
       "       'Contribute to visually-appealing, web-enabled prototype applications that illustrate relevant machine learning capabilities.',\n",
       "       'Translates product management, engineering and business contraints and queries into tractable data science questions.',\n",
       "       'Identify innovative uses of data and/or data technology',\n",
       "       'Build and fine tune algorithms that scale up from small-scale proof-of-concept stage to full production systems',\n",
       "       'Undertake to preprocess of structured and unstructured data'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.id.isin(t1_p2)].description.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Inform bidding strategy and data engineering architecture.',\n",
       "       'Defining data augmentation pipelines',\n",
       "       'Plan and build data-driven banking ecosystem, including ecosystem support models for customer experience and marketing.',\n",
       "       'Build API interface for AI/Client models',\n",
       "       'Builds a deep understanding of the Company s products, services, data and customers to facilitate development of personalized and fulfilling experience.',\n",
       "       'Collaborate with functional leaders to translate business needs into differentiated products & solutions, by driving business case & proforma development.',\n",
       "       'Communicate analytic solutions to stakeholders and implement improvements as needed to operational systems',\n",
       "       'Evaluate accuracy and quality of the designed models as well as data sources.',\n",
       "       'Advise and collaborate in internal partnerships to deliver accurate, available, and accessible data.',\n",
       "       'Be the interface between the data warehouse and the data science team, using transformation frameworks to automate dataset generation',\n",
       "       'prototype new algorithms and productionize solutions at scale',\n",
       "       'Analyze data to effectively coordinate the installation of new systems or modifications to existing systems.',\n",
       "       'Execute analytics strategy, requirements, and adapt processes as needed.',\n",
       "       'Establish scalable, efficient, automated processes for data analyses, model development, validation and implementation',\n",
       "       'Effectively explain technical concepts at all levels in the organization, including senior managers/stakeholders',\n",
       "       'Work on rich, high-quality training data set creation, augmenting and annotation',\n",
       "       'Collaborate with business stakeholders, business operations, and product engineering teams in analyzing business problems, building and testing solutions and data models, and implementing the algorithms and results of the models',\n",
       "       'Develop efficient feature capabilities to support proper data extraction & curation for analytical purposes',\n",
       "       'Verifying data quality, and/or ensuring it via data cleaning',\n",
       "       'Analyze the quality and calibration of predictive models.',\n",
       "       'Be the SQL/Python/Spark/EMR/Snowflake expert on the team and help with data wrangling',\n",
       "       'Assess the effectiveness and accuracy of new data sources and data gathering techniques.',\n",
       "       'Develop and deliver communication & education plans on analytic data engineering capabilities, standards, and processes',\n",
       "       'Analysis of current technologies and assess their fit for purpose',\n",
       "       'Seek out which areas need what kind of data and what their analytical requirements are and work on getting that data in the correct data model',\n",
       "       'Possesses in depth understanding of building streaming, ingestion, processing data pipelines.',\n",
       "       'Demonstrate self-directed and proactive approach to tackling problems and leveraging resources.',\n",
       "       'Utilize statistical methods to process, clean and validate data for uniformity and accuracy',\n",
       "       'Develop and establish model management technology and processes for helping enable MLOps such as: measure on-going accuracy, tooling for data / model drift, etc.',\n",
       "       'Collaborate closely within an interdisciplinary team composed of scientists, developers, and drug hunters to deliver new research that meet business and product goals',\n",
       "       'Develop the skills of data team through mentorship and training',\n",
       "       \"Implementing the Scoring Platform's data intensive infrastructure and APIs\",\n",
       "       'Analyzing, extracting, and optimizing relevant information from     large amounts of data to help drive business decisions on product     features and operational efficiency',\n",
       "       'Strong analytical and problem-solving skills',\n",
       "       'Work closely with data scientists to ensure the source data is aggregated and cleansed.',\n",
       "       'Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions',\n",
       "       'Compare results from various methodologies and recommend best techniques to stake holders',\n",
       "       'Design data pipelines to gather and store large quantities of cyber security data',\n",
       "       'Exploring and visualizing data to gain an understanding of it, then identifying differences in data distribution that could affect',\n",
       "       \"Manage and support the organization's cloud-based data and computing platforms and infrastructure for AI applications\",\n",
       "       'Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.',\n",
       "       'Create data products for analytics and data scientist team members to improve their productivity',\n",
       "       'Analyze data from disparate sources including images, video, radar, signals and more',\n",
       "       'Understand available data, including both in structured and unstructured formats, and recommend effective ways for storage and analytical processing in the cloud.',\n",
       "       'Translate strategic analytics requirements into Analytics tools and infrastructure',\n",
       "       'Lead development efforts to build efficient feature capabilities to support proper data extraction & curation for analytical purposes',\n",
       "       'Study and transform data science prototypes',\n",
       "       'Analyze and extract relevant information from large amounts of historical data - provide hands-on data wrangling expertise',\n",
       "       'Creating and Finding available datasets online that could be used for training',\n",
       "       'Perform DevOps and orchestration for all data hub and analytical processes.',\n",
       "       'Deal with enormous amounts of video data, and developing solutions in a distributed setting',\n",
       "       'Research machine learning (including deep learning) approaches and apply them to tackle real-world problems in construction project life cycle, including but not limited to knowledge extraction from unstructured data, construction site object detection and tracking etc.',\n",
       "       'Work with internal clients and the Data Science team to solve problems',\n",
       "       'Apply expertise in Machine Learning and data mining techniques: feature engineering, regression, classification, anomaly detection, time series analysis etc',\n",
       "       'Build an ad targeting system to find the most relevant users for ads through interest, keyword and audience targeting',\n",
       "       'Learn the business inside and out, interfacing with domain knowledge experts',\n",
       "       'Build predictive models and machine-learning algorithms',\n",
       "       'Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals.',\n",
       "       'Understanding business objectives and developing models that help to achieve them, along with metrics to track their progress',\n",
       "       'Provide technical leadership to a strong team of data scientists, engineers, and analysts.',\n",
       "       'Select appropriate datasets and data representation methods',\n",
       "       'Represent ECD AI in meetings with external and internal stakeholders',\n",
       "       'Optimize algorithms for efficiency, speed, and usability.',\n",
       "       'Hunting for quality datasets, including development from scratch (setting up the right process), for-fee acquisition of existing data, and bootstrapping corpora from messy and/or limited data sources',\n",
       "       'Identify what kind of data in which format should be collected in order to solve the problem.',\n",
       "       'Collaborate with data team for in-house data management and labelling',\n",
       "       'Develop new ideas into AI driven products using our wealth of sports data',\n",
       "       'Collaborate with a cross functional agile team spanning user research, design, data science, product management, and engineering to build new product features that advance our mission to connect artists and fans in personalized and relevant ways',\n",
       "       'Develop and embed automated processes for predictive model validation, deployment, and implementation',\n",
       "       'Author formal and informal reports that effectively communicate the models and the results to both technical and non-technical audiences',\n",
       "       'Writing simulation code using Scalding to run MapReduce jobs on our Hadoop cluster to help us understand what would happen across different segments if we changed how we action our models.',\n",
       "       'This role conducts experimentation in various data science techniques, developing, executing, and maintaining scripts and prototypes to analyze, interpret, visualize, and gain knowledge from numerous data sets individually or in combination to meet mission needs.',\n",
       "       'Create algorithms to extract information from large, multiparametric data sets',\n",
       "       'Develop integrated software algorithms to structure, analyze and use data in product and systems applications in both structured and unstructured environments',\n",
       "       'Work closely with product teams to in order to drive maximum product enhancements through the use of research',\n",
       "       'Exploring and visualizing data to gain an understanding of it, then identifying differences in data distribution that could affect performance when deploying the model in the real world',\n",
       "       'Communicate key project data to team members and build team cohesion and effectiveness.',\n",
       "       'Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions.',\n",
       "       'Conduct analysis by mining internal and external data sources',\n",
       "       'Support Data Science team with any data movements, data transformation, and analysis, etc.',\n",
       "       'Work closely with our Data team to extend and improve our Food Genome by applying NLP based methods to this complex domain',\n",
       "       'Build machine learning pipeline solutions for Data Science models developed by consumer bank, including core banking, cards and wealth management businesses.',\n",
       "       'Translates product management, engineering and business contraints and queries into tractable data science questions.',\n",
       "       'Undertake to preprocess of structured and unstructured data'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.id.isin(t2_p1)].description.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>jobflag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Executes and writes portions of testing plans,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Maintain Network Performance by assisting with...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Supports the regional compliance manager with ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Keep up to date with local and national busine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Assist with Service Organization Control (SOC)...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>2926</td>\n",
       "      <td>Preparation of reports for operational and man...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>2927</td>\n",
       "      <td>Line and/or indirect management of up to 20 st...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>2928</td>\n",
       "      <td>Partner with external agencies as needed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>2929</td>\n",
       "      <td>Design, Implement and test software for embedd...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>2930</td>\n",
       "      <td>Undertake to preprocess of structured and unst...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2931 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        description  jobflag\n",
       "0        0  Executes and writes portions of testing plans,...        2\n",
       "1        1  Maintain Network Performance by assisting with...        3\n",
       "2        2  Supports the regional compliance manager with ...        4\n",
       "3        3  Keep up to date with local and national busine...        1\n",
       "4        4  Assist with Service Organization Control (SOC)...        4\n",
       "...    ...                                                ...      ...\n",
       "2926  2926  Preparation of reports for operational and man...        3\n",
       "2927  2927  Line and/or indirect management of up to 20 st...        3\n",
       "2928  2928           Partner with external agencies as needed        1\n",
       "2929  2929  Design, Implement and test software for embedd...        3\n",
       "2930  2930  Undertake to preprocess of structured and unst...        2\n",
       "\n",
       "[2931 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    659\n",
       "4    540\n",
       "1    383\n",
       "2    161\n",
       "Name: preds, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2.preds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../submit_sample.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[1] = test_df2.preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2931</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2932</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2933</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2935</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>4669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>4670</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>4671</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>4672</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>4673</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1743 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1\n",
       "0     2931  3\n",
       "1     2932  3\n",
       "2     2933  3\n",
       "3     2934  1\n",
       "4     2935  4\n",
       "...    ... ..\n",
       "1738  4669  1\n",
       "1739  4670  4\n",
       "1740  4671  1\n",
       "1741  4672  4\n",
       "1742  4673  3\n",
       "\n",
       "[1743 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub.to_csv('/Users/abcdm/Downloads/sub_14.csv',  index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12838508, 0.31230478, 0.36480007, 0.19451007],\n",
       "       [0.08310936, 0.15570187, 0.47398954, 0.28719922],\n",
       "       [0.10580147, 0.14900548, 0.46465474, 0.28053831],\n",
       "       ...,\n",
       "       [0.29589718, 0.23231882, 0.29489432, 0.17688969],\n",
       "       [0.08283854, 0.07187707, 0.36812042, 0.47716396],\n",
       "       [0.06988015, 0.1236374 , 0.63688098, 0.16960147]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds/(80*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
