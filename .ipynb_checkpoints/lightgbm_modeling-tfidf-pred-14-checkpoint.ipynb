{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "import nltk, string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "import optuna\n",
    "\n",
    "random.seed(2020)\n",
    "np.random.seed(2020)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "        self.stop_words = get_stop_words('en')\n",
    "        self.stop_words.append(' ')\n",
    "        self.stop_words.append('')\n",
    "    \n",
    "    def pipeline(self, df):\n",
    "        for lang in ['description']:\n",
    "            #, 'translate_es', 'translate_fr', 'translate_de', 'translate_ja']:\n",
    "            df[lang] = df[lang].apply(lambda x: self.change_text(x))\n",
    "        return df\n",
    "\n",
    "    def change_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace('ml', 'machine learning')\n",
    "        text = text.replace('machine learning', 'machinelearning')\n",
    "        text = \"\".join([char if char not in string.punctuation else ' ' for char in text])\n",
    "        text = \" \".join([self.porter.stem(char) for char in text.split(' ') if char not in self.stop_words])\n",
    "        return text\n",
    "    \n",
    "    def vectorize_tfidf(self, df):\n",
    "        vec_tfidf = TfidfVectorizer()\n",
    "        X = vec_tfidf.fit_transform(df.description.values)\n",
    "        X = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n",
    "        return X\n",
    "    \n",
    "    def vectorize_cnt(self, df):\n",
    "        vec_cnt = CountVectorizer()\n",
    "        X = vec_cnt.fit_transform(df.description.values)\n",
    "        X = pd.DataFrame(X.toarray(), columns=vec_cnt.get_feature_names())\n",
    "        return X\n",
    "\n",
    "\n",
    "class Optimize_by_Optuna:\n",
    "    def __init__(self, data, features, target_colname, target_name_2=None, _objective=None):\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.target = target_colname\n",
    "        if not target_colname:\n",
    "            self.target_2 = target_colname\n",
    "        else:\n",
    "            self.target_2 = target_name_2\n",
    "        self._objective = _objective\n",
    "        \n",
    "    \n",
    "    def make_score(self, y, preds):\n",
    "        s_1=1 - metrics.accuracy_score(y, preds)\n",
    "        s_2=list(self.model.best_score['valid_1'].values())[0]\n",
    "\n",
    "        return (s_1+s_2)/2\n",
    "\n",
    "    def objective(self, trial):\n",
    "                        \n",
    "        PARAMS = {#'boosting_type': 'gbdt', 'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            \n",
    "            #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "            \n",
    "            'objective': 'tweedie','metric': 'tweedie',\n",
    "            \n",
    "            'n_estimators': 1400,\n",
    "            'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "        \n",
    "\n",
    "            'tweedie_variance_power': trial.suggest_uniform('tweedie_variance_power', 1.01, 1.8),\n",
    "\n",
    "\n",
    "            'max_bin': trial.suggest_int('max_bin', 50, 300),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.4, 0.9),\n",
    "            'subsample_freq': trial.suggest_uniform('subsample_freq', 0.4, 0.9),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.03, 0.5),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 4, 2*5),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.0001, 10.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.0001, 10.0),\n",
    "        }\n",
    "        \n",
    "        score = 0\n",
    "        k = StratifiedKFold(n_splits=5)\n",
    "        for trn, val in k.split(self.data, self.data[self.target_2]):\n",
    "            train_df = self.data.iloc[trn,:]\n",
    "            val_df = self.data.iloc[val,:]\n",
    "            train_set= lgb.Dataset(train_df[self.features],  train_df[self.target])\n",
    "            val_set = lgb.Dataset(val_df[self.features],  val_df[self.target])   \n",
    "            \n",
    "            self.model = lgb.train(\n",
    "                train_set=train_set, valid_sets=[train_set, val_set], params=PARAMS, num_boost_round=3000, \n",
    "                early_stopping_rounds=200, verbose_eval=500\n",
    "                )\n",
    "                \n",
    "            preds = self.model.predict(val_df[self.features])\n",
    "            preds = np.round(preds)\n",
    "            y = val_df[self.target]\n",
    "            s = self.make_score(y, preds)\n",
    "            score+=s/5\n",
    "            \n",
    "        return score\n",
    "\n",
    "\n",
    "class Null_Importance:\n",
    "    def __init__(self, train_X, train_y, PARAMS, y_2=None):\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.y_2= y_2\n",
    "        self.PARAMS = PARAMS\n",
    "\n",
    "    def make_null_importance_df(self):\n",
    "        null_importance=pd.DataFrame()\n",
    "        null_importance['col'] = self.train_X.columns.tolist()\n",
    "        try:\n",
    "            for i in range(50):\n",
    "                tmp_null_importance=[]\n",
    "                \n",
    "                _train_y = self.train_y.apply(lambda x: random.choice([0,1]))\n",
    "                _train_y_2 = self.y_2.sample(frac=1).values\n",
    "                \n",
    "                print(f\"\"\"\n",
    "                \n",
    "                Train Null Importance   {i+1}\n",
    "                \n",
    "                \"\"\" )\n",
    "                k = StratifiedKFold(n_splits=5)\n",
    "                for trn, val in k.split(self.train_X, _train_y_2):\n",
    "                    trn_X, val_X = self.train_X.iloc[trn,:], self.train_X.iloc[val,:]\n",
    "                    trn_y, val_y = _train_y.iloc[trn].astype(int), _train_y.iloc[val].astype(int)\n",
    "                    train_set = lgb.Dataset(trn_X, trn_y)\n",
    "                    val_set = lgb.Dataset(val_X, val_y)\n",
    "\n",
    "                    model = lgb.train(params=self.PARAMS,\n",
    "                                      train_set=train_set, \n",
    "                                      valid_sets=[train_set, val_set],\n",
    "                                    num_boost_round=3000, early_stopping_rounds=200, verbose_eval=500)\n",
    "                    \n",
    "                    preds = model.predict(val_X)\n",
    "                    tmp_null_importance.append(model.feature_importance('gain'))\n",
    "                null_importance[f'null_importance_{i+1}'] = np.mean(tmp_null_importance, axis=0)\n",
    "            return null_importance\n",
    "        except:\n",
    "            return null_importance\n",
    "\n",
    "    def calu_importance(self, importance_df, null_importance_df):\n",
    "        importance_df = pd.merge(\n",
    "            importance_df, null_importance_df, on='col'\n",
    "            )\n",
    "        null_importance_col = [col for col in importance_df.columns if 'null' in col]\n",
    "        null_importance=pd.DataFrame()\n",
    "        for idx, row in importance_df.iterrows():\n",
    "            acc_v = 1e-10+row['true_importance']\n",
    "            null_v = 1+np.percentile(row[null_importance_col], 75)\n",
    "            null_importance[row['col']] = [np.log(acc_v/null_v)]\n",
    "        null_importance = null_importance.T\n",
    "        return null_importance\n",
    "\n",
    "    def all_flow(self):\n",
    "        k = StratifiedKFold(n_splits=5)\n",
    "        score=[]\n",
    "        importance=[]\n",
    "\n",
    "        importance_df=pd.DataFrame()\n",
    "        importance_df['col'] = self.train_X.columns\n",
    "        print(\"\"\"\n",
    "        \n",
    "        Train True Importance\n",
    "        \n",
    "        \"\"\" )\n",
    "        for trn, val in k.split(self.train_X, self.y_2):\n",
    "            trn_X, val_X = self.train_X.iloc[trn,:], self.train_X.iloc[val,:]\n",
    "            trn_y, val_y = self.train_y.iloc[trn].astype(int), self.train_y.iloc[val].astype(int)\n",
    "            train_set = lgb.Dataset(trn_X, trn_y)\n",
    "            val_set = lgb.Dataset(val_X, val_y)\n",
    "            \n",
    "            PARAMS['random_state']+=1\n",
    "            model = lgb.train(params=self.PARAMS, train_set=train_set, valid_sets=[train_set, val_set],\n",
    "                            num_boost_round=3000, early_stopping_rounds=200, verbose_eval=500)\n",
    "            preds = model.predict(val_X)\n",
    "            importance.append(model.feature_importance('gain'))\n",
    "        importance_df['true_importance'] = np.mean(importance, axis=0)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \n",
    "        Train Null Importance\n",
    "        \n",
    "        \"\"\" )\n",
    "        try:\n",
    "            null_importance_df = self.make_null_importance_df()\n",
    "        except:\n",
    "            pass\n",
    "        print(\"\"\"\n",
    "        \n",
    "        Calulate null_null_importance\n",
    "        \n",
    "        \"\"\" )\n",
    "        null_importance = self.calu_importance(importance_df, null_importance_df)\n",
    "        null_importance = null_importance.reset_index()\n",
    "        null_importance.columns = ['col', 'score']\n",
    "        null_importance = null_importance.sort_values('score', ascending=False)\n",
    "        return null_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_offdf(train_df, test_df, feature, params_list):\n",
    "    k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    \n",
    "    y_1 = train_df.jobflag.apply(lambda x: 1 if x==1 else 0)\n",
    "    y_2 = train_df.jobflag.apply(lambda x: 1 if x==2 else 0)\n",
    "    y_3 = train_df.jobflag.apply(lambda x: 1 if x==3 else 0)\n",
    "    y_4 = train_df.jobflag.apply(lambda x: 1 if x==4 else 0)\n",
    "    \n",
    "    off_df = []\n",
    "    for i in range(4):\n",
    "        test_df[f'lgb_preds_{i+1}']=0\n",
    "    \n",
    "    for trn, val in k.split(train_df, train_df.jobflag):\n",
    "        train_X, val_X = train_df.iloc[trn,:][feature], train_df.iloc[val,:][feature]\n",
    "        tmp_off_df = train_df.iloc[val,:]\n",
    "        c=1\n",
    "        for y, param in zip([y_1, y_2, y_3, y_4], params_list):\n",
    "            tmp_off_df[f'lgb_preds_{c}']=0\n",
    "            for _ in range(5):\n",
    "                train_y, val_y = y.iloc[trn], y.iloc[val]\n",
    "                train_set= lgb.Dataset(train_X,  train_y)\n",
    "                val_set = lgb.Dataset(val_X,  val_y)   \n",
    "\n",
    "                model = lgb.train(\n",
    "                    train_set=train_set, valid_sets=[train_set, val_set], params=param, num_boost_round=3000, \n",
    "                    early_stopping_rounds=200, verbose_eval=500\n",
    "                )\n",
    "                tmp_off_df[f'lgb_preds_{c}'] += model.predict(val_X)/5\n",
    "                param['random_state']+=1\n",
    "                \n",
    "                test_df[f'lgb_preds_{c}'] += model.predict(test_df[feature])/5\n",
    "                \n",
    "            c+=1\n",
    "        \n",
    "        off_df.append(tmp_off_df)\n",
    "    \n",
    "    for i in range(4):\n",
    "        test_df[f'lgb_preds_{i+1}']/=5\n",
    "    \n",
    "    off_df = pd.concat(off_df, axis=0)\n",
    "    return off_df.reset_index(drop=True), test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['abil', 'abl', 'accept', 'access', 'accord', 'account', 'accur', 'accuraci', 'achiev', 'acquisit', 'across', 'act', 'action', \n",
    "           'activ', 'ad', 'addit', 'address', 'adher', 'administr', 'advanc', 'advis', 'advisor', 'agil', 'agre', 'ai', 'algorithm', \n",
    "           'align', 'analys', 'analysi', 'analyst', 'analyt', 'analyz', 'api', 'appli', 'applic', 'approach', 'appropri', 'approv', \n",
    "           'architect', 'architectur', 'area', 'assembl', 'assess', 'assign', 'assist', 'audienc', 'autom', 'avail', 'aw', 'back',\n",
    "           'backend', 'base', 'basic', 'behavior', 'benefit', 'best', 'board', 'bug', 'build', 'busi', 'call', 'can', 'candid', 'capabl',\n",
    "           'capac', 'case', 'caus', 'challeng', 'chang', 'clearli', 'client', 'clinic', 'close', 'cloud', 'cluster', 'coach', 'code', \n",
    "           'collabor', 'collect', 'commerci', 'commiss', 'commun', 'compani', 'complet', 'complex', 'complianc', 'compon', 'comput', \n",
    "           'concept', 'conduct', 'confer', 'configur', 'connect', 'consist', 'construct', 'consult', 'content', 'continu', 'contract',\n",
    "           'contribut', 'control', 'coordin', 'core', 'corpor', 'correct', 'cost', 'creat', 'creation', 'creativ', 'critic', 'cross', \n",
    "           'cultur', 'current', 'custom', 'cycl', 'daili', 'dashboard', 'data', 'databas', 'dataset', 'date', 'deadlin', 'debug', 'decis',\n",
    "           'deep', 'defect', 'defin', 'definit', 'deliv', 'deliver', 'deliveri', 'demand', 'demonstr', 'depart', 'depend', 'deploy', 'depth',\n",
    "           'deriv', 'design', 'desir', 'detail', 'detect', 'determin', 'develop', 'devic', 'devop', 'differ', 'digit', 'direct', 'disciplin',\n",
    "           'discoveri', 'discuss', 'distribut', 'divers', 'document', 'domain', 'draw', 'drive', 'duti', 'dynam', 'edg', 'educ', 'effect',\n",
    "           'effici', 'effort', 'electron', 'email', 'embed', 'employe', 'enabl', 'end', 'engag', 'engin', 'enhanc', 'ensur', 'enterpris',\n",
    "           'environ', 'equip', 'erp', 'escal', 'establish', 'estim', 'etc', 'evalu', 'event', 'excel', 'execut', 'exist', 'expand', 'experi',\n",
    "           'expert', 'expertis', 'explain', 'explor', 'exploratori', 'extern', 'extract', 'face', 'facilit', 'failur', 'featur', 'feder', \n",
    "           'field', 'find', 'fix', 'flow', 'focu', 'follow', 'form', 'formul', 'framework', 'front', 'full', 'function', 'futur', 'gain',\n",
    "           'gap', 'gather', 'gener', 'global', 'go', 'goal', 'good', 'govern', 'group', 'grow', 'growth', 'guid', 'guidanc', 'hand', \n",
    "           'hardwar', 'healthcar', 'help', 'high', 'highli', 'hoc', 'idea', 'identifi', 'impact', 'implement', 'improv', 'incid', 'includ',\n",
    "           'increas', 'independ', 'individu', 'industri', 'influenc', 'inform', 'infrastructur', 'initi', 'innov', 'input', 'insight',\n",
    "           'inspect', 'instal', 'integr', 'intellig', 'interact', 'interfac', 'intern', 'interpret', 'investig', 'issu', 'iter', 'java',\n",
    "           'job', 'junior', 'keep', 'key', 'knowledg', 'languag', 'larg', 'latest', 'lead', 'leader', 'leadership', 'learn', 'level', \n",
    "           'leverag', 'librari', 'life', 'like', 'limit', 'linux', 'log', 'logic', 'machin', 'machinelearn', 'maintain', 'mainten', \n",
    "           'make', 'manag', 'manner', 'manufactur', 'map', 'market', 'materi', 'matter', 'may', 'measur', 'mechan', 'medic', 'meet',\n",
    "           'member', 'mentor', 'met', 'method', 'methodolog', 'metric', 'microsoft', 'migrat', 'mission', 'mobil', 'model', 'moder',\n",
    "           'modifi', 'modul', 'monitor', 'multi', 'multipl', 'must', 'necessari', 'need', 'net', 'network', 'new', 'next', 'non', 'novel',\n",
    "           'object', 'obtain', 'ongo', 'open', 'oper', 'opportun', 'optim', 'order', 'organ', 'organiz', 'orient', 'outcom', 'outsid',\n",
    "           'overal', 'overse', 'part', 'parti', 'particip', 'partner', 'partnership', 'pattern', 'payrol', 'peer', 'perform', 'person',\n",
    "           'personnel', 'pipelin', 'plan', 'platform', 'point', 'polici', 'posit', 'post', 'potenti', 'practic', 'pre', 'predict', 'prepar', \n",
    "           'present', 'price', 'principl', 'prior', 'priorit', 'proactiv', 'problem', 'procedur', 'process', 'produc', 'product', \n",
    "           'profession', 'program', 'progress', 'project', 'promot', 'proof', 'propos', 'prospect', 'protocol', 'prototyp','provid', \n",
    "           'purpos', 'python', 'qa', 'qualifi', 'qualiti', 'queri', 'question', 'quickli', 'real', 'recommend', 'referr', 'refin', 'regard',\n",
    "           'region', 'regul', 'regular', 'regulatori', 'relat', 'relationship', 'releas', 'relev', 'reliabl', 'report','repres', 'request',\n",
    "           'requir', 'research', 'resid', 'resolut', 'resolv', 'resourc', 'respons', 'result', 'retail', 'review', 'rigor', 'risk', 'roadmap',\n",
    "           'role', 'root', 'rule', 'run', 'safeti', 'sale', 'scalabl', 'scale', 'schedul', 'scienc', 'scientist', 'scope', 'script', 'scrum',\n",
    "           'secur', 'segment', 'select', 'self', 'sell', 'senior', 'serv', 'server', 'servic', 'set', 'share', 'show', 'simul', 'site',\n",
    "           'skill', 'small', 'softwar', 'solut', 'solv', 'sourc', 'specif', 'sql', 'stack', 'staff', 'stakehold', 'standard', 'state',\n",
    "           'statist', 'statu', 'stay', 'store', 'stori', 'strateg', 'strategi', 'stream', 'strong', 'structur', 'studi', 'subject', \n",
    "           'success', 'suggest', 'supplier', 'support', 'system', 'take', 'target', 'task', 'team', 'technic', 'techniqu', 'technolog', \n",
    "           'term','test', 'think', 'thought', 'throughout', 'time', 'timelin', 'tool', 'top', 'track', 'train', 'transform', 'translat',\n",
    "           'travel', 'trend', 'troubleshoot', 'tune', 'understand', 'unit', 'updat', 'upgrad', 'use', 'user', 'util', 'valid',\n",
    "           'valu', 'variou', 'vehicl', 'vendor', 'verif', 'verifi', 'version', 'via', 'vision', 'visual', 'way',\n",
    "           'web', 'well', 'wide', 'will', 'window', 'within', 'work', 'workflow','write']\n",
    "\n",
    "PARAMS_1={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    'objective': 'tweedie','metric': 'tweedie',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "   'tweedie_variance_power': 1.349969119190657, 'max_bin': 212, 'subsample': 0.5774043241504451, 'subsample_freq': 0.7045972939301558, \n",
    "    'learning_rate': 0.16528226095247364, 'num_leaves': 4, 'feature_fraction': 0.9964784224971625,\n",
    "    'bagging_freq': 6, 'min_child_samples': 23, 'lambda_l1': 0.016924825494747078, 'lambda_l2': 0.0008031532180312293\n",
    "}\n",
    "\n",
    "\n",
    "PARAMS_2={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    'objective': 'tweedie','metric': 'tweedie',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "    'tweedie_variance_power': 1.3014991003823067, 'max_bin': 134, 'subsample': 0.8990859498726816, 'subsample_freq': 0.5274951186330312,\n",
    "    'learning_rate': 0.3937162652059595, 'num_leaves': 5, 'feature_fraction': 0.8861294810479933, 'bagging_freq': 5,\n",
    "    'min_child_samples': 28, 'lambda_l1': 6.037171725930821, 'lambda_l2': 0.0025254105473444784\n",
    "}\n",
    "\n",
    "PARAMS_3={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    #'objective': 'tweedie','metric': 'tweedie',\n",
    "     \n",
    "    'objective': 'xentropy','metric': 'xentropy',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "    'max_bin': 50, 'subsample': 0.8509082362331666, 'subsample_freq': 0.6958806976511948, 'learning_rate': 0.09406169926162017,\n",
    "    'num_leaves': 7, 'feature_fraction': 0.7562554580497556, 'bagging_freq': 4, 'min_child_samples': 5, 'lambda_l1': 0.00021420978217365439,\n",
    "    'lambda_l2': 0.011867471326820044\n",
    "}\n",
    "\n",
    "PARAMS_4={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    'objective': 'tweedie','metric': 'tweedie',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "    'tweedie_variance_power': 1.3572492826220748, 'max_bin': 169, 'subsample': 0.6874225607452877, 'subsample_freq': 0.5369168449326642,\n",
    "    'learning_rate': 0.0353671206084155, 'num_leaves': 8, 'feature_fraction': 0.9508830019260512, \n",
    "    'bagging_freq': 2, 'min_child_samples': 63, 'lambda_l1': 8.281467382972142, 'lambda_l2': 0.1428656656583413\n",
    "}\n",
    "\n",
    "param_list = [PARAMS_1, PARAMS_2, PARAMS_3, PARAMS_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../train.csv')\n",
    "test_df = pd.read_csv('../test.csv')\n",
    "for i in range(4):    \n",
    "    train_df = pd.merge(train_df, pd.read_csv(f'../train_df_off{i+1}.csv'), on='id')\n",
    "    test_df = pd.merge(test_df, pd.read_csv(f'../test_df_off{i+1}.csv').drop(columns=['description', 'jobflag']), on='id')\n",
    "df = pd.concat([train_df, test_df],axis=0,ignore_index=True)\n",
    "preprocessing = Preprocessing()\n",
    "df.description = df.description.apply(lambda x: preprocessing.change_text(x))\n",
    "cols = ['jobflag','id','bert_pred_1_1', 'bert_pred_2_1', 'bert_pred_3_1',\n",
    "       'bert_pred_4_1', 'bert_pred_5_1', 'bert_pred_1_2', 'bert_pred_2_2',\n",
    "       'bert_pred_3_2', 'bert_pred_4_2', 'bert_pred_5_2', 'bert_pred_1_3',\n",
    "       'bert_pred_2_3', 'bert_pred_3_3', 'bert_pred_4_3', 'bert_pred_5_3',\n",
    "       'bert_pred_1_4', 'bert_pred_2_4', 'bert_pred_3_4', 'bert_pred_4_4',\n",
    "       'bert_pred_5_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's tweedie: 1.40681\tvalid_1's tweedie: 1.43566\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttraining's tweedie: 1.41926\tvalid_1's tweedie: 1.45214\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[81]\ttraining's tweedie: 1.38798\tvalid_1's tweedie: 1.4528\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[65]\ttraining's tweedie: 1.40081\tvalid_1's tweedie: 1.43151\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[89]\ttraining's tweedie: 1.38559\tvalid_1's tweedie: 1.45117\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's tweedie: 0.927475\tvalid_1's tweedie: 0.985683\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's tweedie: 0.944292\tvalid_1's tweedie: 0.997409\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's tweedie: 0.937709\tvalid_1's tweedie: 0.990317\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[25]\ttraining's tweedie: 0.932137\tvalid_1's tweedie: 0.992638\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's tweedie: 0.920814\tvalid_1's tweedie: 0.990401\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[179]\ttraining's cross_entropy: 0.36084\tvalid_1's cross_entropy: 0.492026\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[172]\ttraining's cross_entropy: 0.364931\tvalid_1's cross_entropy: 0.495781\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[194]\ttraining's cross_entropy: 0.350057\tvalid_1's cross_entropy: 0.48512\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[188]\ttraining's cross_entropy: 0.355324\tvalid_1's cross_entropy: 0.484158\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[179]\ttraining's cross_entropy: 0.358706\tvalid_1's cross_entropy: 0.49315\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36555\tvalid_1's tweedie: 1.40762\n",
      "Early stopping, best iteration is:\n",
      "[522]\ttraining's tweedie: 1.36427\tvalid_1's tweedie: 1.40721\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36595\tvalid_1's tweedie: 1.40983\n",
      "Early stopping, best iteration is:\n",
      "[562]\ttraining's tweedie: 1.36242\tvalid_1's tweedie: 1.40787\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36672\tvalid_1's tweedie: 1.4068\n",
      "Early stopping, best iteration is:\n",
      "[481]\ttraining's tweedie: 1.36762\tvalid_1's tweedie: 1.40661\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36488\tvalid_1's tweedie: 1.40456\n",
      "Early stopping, best iteration is:\n",
      "[584]\ttraining's tweedie: 1.36066\tvalid_1's tweedie: 1.40422\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36682\tvalid_1's tweedie: 1.40835\n",
      "Early stopping, best iteration is:\n",
      "[314]\ttraining's tweedie: 1.38163\tvalid_1's tweedie: 1.40545\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's tweedie: 1.44792\tvalid_1's tweedie: 1.50791\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's tweedie: 1.4263\tvalid_1's tweedie: 1.506\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's tweedie: 1.44124\tvalid_1's tweedie: 1.50363\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's tweedie: 1.4524\tvalid_1's tweedie: 1.50437\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's tweedie: 1.44718\tvalid_1's tweedie: 1.51529\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's tweedie: 0.913231\tvalid_1's tweedie: 0.96785\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's tweedie: 0.926805\tvalid_1's tweedie: 0.973237\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's tweedie: 0.924278\tvalid_1's tweedie: 0.966453\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's tweedie: 0.910396\tvalid_1's tweedie: 0.968749\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's tweedie: 0.916225\tvalid_1's tweedie: 0.967964\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[233]\ttraining's cross_entropy: 0.322713\tvalid_1's cross_entropy: 0.508576\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\ttraining's cross_entropy: 0.361601\tvalid_1's cross_entropy: 0.518638\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[155]\ttraining's cross_entropy: 0.369369\tvalid_1's cross_entropy: 0.510106\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[127]\ttraining's cross_entropy: 0.388833\tvalid_1's cross_entropy: 0.523868\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[167]\ttraining's cross_entropy: 0.360631\tvalid_1's cross_entropy: 0.517053\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36753\tvalid_1's tweedie: 1.42783\n",
      "[1000]\ttraining's tweedie: 1.34792\tvalid_1's tweedie: 1.4258\n",
      "Early stopping, best iteration is:\n",
      "[846]\ttraining's tweedie: 1.35297\tvalid_1's tweedie: 1.42487\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36681\tvalid_1's tweedie: 1.43065\n",
      "Early stopping, best iteration is:\n",
      "[354]\ttraining's tweedie: 1.3766\tvalid_1's tweedie: 1.43001\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36609\tvalid_1's tweedie: 1.42752\n",
      "Early stopping, best iteration is:\n",
      "[604]\ttraining's tweedie: 1.36075\tvalid_1's tweedie: 1.42657\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36633\tvalid_1's tweedie: 1.42748\n",
      "Early stopping, best iteration is:\n",
      "[554]\ttraining's tweedie: 1.36367\tvalid_1's tweedie: 1.42617\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36813\tvalid_1's tweedie: 1.42882\n",
      "[1000]\ttraining's tweedie: 1.34827\tvalid_1's tweedie: 1.42826\n",
      "Early stopping, best iteration is:\n",
      "[810]\ttraining's tweedie: 1.3551\tvalid_1's tweedie: 1.42661\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's tweedie: 1.4231\tvalid_1's tweedie: 1.455\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's tweedie: 1.40841\tvalid_1's tweedie: 1.45271\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttraining's tweedie: 1.43223\tvalid_1's tweedie: 1.47618\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's tweedie: 1.40948\tvalid_1's tweedie: 1.46113\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttraining's tweedie: 1.39901\tvalid_1's tweedie: 1.47201\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[92]\ttraining's tweedie: 0.891439\tvalid_1's tweedie: 0.961586\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49]\ttraining's tweedie: 0.910225\tvalid_1's tweedie: 0.963441\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttraining's tweedie: 0.894676\tvalid_1's tweedie: 0.96469\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttraining's tweedie: 0.907872\tvalid_1's tweedie: 0.957699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's tweedie: 0.903796\tvalid_1's tweedie: 0.95649\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's cross_entropy: 0.363527\tvalid_1's cross_entropy: 0.487057\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[205]\ttraining's cross_entropy: 0.341988\tvalid_1's cross_entropy: 0.485309\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[198]\ttraining's cross_entropy: 0.346072\tvalid_1's cross_entropy: 0.489744\n"
     ]
    }
   ],
   "source": [
    "X = preprocessing.vectorize_tfidf(df)\n",
    "X = pd.concat([df[cols], X], axis=1)\n",
    "train_df = X[X.jobflag.notnull()].reset_index(drop=True)\n",
    "test_df = X[X.jobflag.isnull()].drop(columns=['jobflag']).reset_index(drop=True)\n",
    "\n",
    "off_df_tfidf, test_df2_tfidf = make_offdf(train_df, test_df, feature, param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb_preds_1</th>\n",
       "      <th>lgb_preds_2</th>\n",
       "      <th>lgb_preds_3</th>\n",
       "      <th>lgb_preds_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.525360</td>\n",
       "      <td>0.176473</td>\n",
       "      <td>-0.443558</td>\n",
       "      <td>-0.160037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.125056</td>\n",
       "      <td>0.366627</td>\n",
       "      <td>-0.207003</td>\n",
       "      <td>-0.169210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.404214</td>\n",
       "      <td>-0.241977</td>\n",
       "      <td>0.586404</td>\n",
       "      <td>-0.096219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.135513</td>\n",
       "      <td>-0.173602</td>\n",
       "      <td>-0.109832</td>\n",
       "      <td>0.419984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lgb_preds_1  lgb_preds_2  lgb_preds_3  lgb_preds_4\n",
       "1     0.525360     0.176473    -0.443558    -0.160037\n",
       "2     0.125056     0.366627    -0.207003    -0.169210\n",
       "3    -0.404214    -0.241977     0.586404    -0.096219\n",
       "4    -0.135513    -0.173602    -0.109832     0.419984"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "    pd.get_dummies(off_df.jobflag)[[1,2,3,4]],\n",
    "    off_df[[ 'lgb_preds_1', 'lgb_preds_2', 'lgb_preds_3', 'lgb_preds_4']]\n",
    "], axis=1).corr().loc[[1,2,3,4], [ 'lgb_preds_1', 'lgb_preds_2', 'lgb_preds_3', 'lgb_preds_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobflag</th>\n",
       "      <th>id</th>\n",
       "      <th>bert_pred_1_1</th>\n",
       "      <th>bert_pred_2_1</th>\n",
       "      <th>bert_pred_3_1</th>\n",
       "      <th>bert_pred_4_1</th>\n",
       "      <th>bert_pred_5_1</th>\n",
       "      <th>bert_pred_1_2</th>\n",
       "      <th>bert_pred_2_2</th>\n",
       "      <th>bert_pred_3_2</th>\n",
       "      <th>...</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yet</th>\n",
       "      <th>yield</th>\n",
       "      <th>younger</th>\n",
       "      <th>zeiss</th>\n",
       "      <th>zookeep</th>\n",
       "      <th>lgb_preds_1</th>\n",
       "      <th>lgb_preds_2</th>\n",
       "      <th>lgb_preds_3</th>\n",
       "      <th>lgb_preds_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524498</td>\n",
       "      <td>0.257973</td>\n",
       "      <td>0.144118</td>\n",
       "      <td>0.133779</td>\n",
       "      <td>0.076740</td>\n",
       "      <td>0.484906</td>\n",
       "      <td>0.294941</td>\n",
       "      <td>0.220875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124375</td>\n",
       "      <td>0.046654</td>\n",
       "      <td>0.776604</td>\n",
       "      <td>0.049806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.578211</td>\n",
       "      <td>0.409709</td>\n",
       "      <td>0.237590</td>\n",
       "      <td>0.349191</td>\n",
       "      <td>0.117446</td>\n",
       "      <td>0.445859</td>\n",
       "      <td>0.238172</td>\n",
       "      <td>0.217901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119715</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>0.906843</td>\n",
       "      <td>0.025833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.394083</td>\n",
       "      <td>0.252006</td>\n",
       "      <td>0.214029</td>\n",
       "      <td>0.169860</td>\n",
       "      <td>0.175528</td>\n",
       "      <td>0.664439</td>\n",
       "      <td>0.778905</td>\n",
       "      <td>0.830853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118658</td>\n",
       "      <td>0.069503</td>\n",
       "      <td>0.623205</td>\n",
       "      <td>0.242803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.450969</td>\n",
       "      <td>0.387301</td>\n",
       "      <td>0.247143</td>\n",
       "      <td>0.448752</td>\n",
       "      <td>0.165567</td>\n",
       "      <td>0.498336</td>\n",
       "      <td>0.543906</td>\n",
       "      <td>0.383343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145846</td>\n",
       "      <td>0.090250</td>\n",
       "      <td>0.390933</td>\n",
       "      <td>0.200563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.426481</td>\n",
       "      <td>0.272545</td>\n",
       "      <td>0.192479</td>\n",
       "      <td>0.184106</td>\n",
       "      <td>0.098343</td>\n",
       "      <td>0.473353</td>\n",
       "      <td>0.459011</td>\n",
       "      <td>0.220175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098323</td>\n",
       "      <td>0.045169</td>\n",
       "      <td>0.565293</td>\n",
       "      <td>0.142971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3409 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   jobflag  id  bert_pred_1_1  bert_pred_2_1  bert_pred_3_1  bert_pred_4_1  \\\n",
       "0      2.0   0       0.524498       0.257973       0.144118       0.133779   \n",
       "1      3.0   9       0.578211       0.409709       0.237590       0.349191   \n",
       "2      3.0  17       0.394083       0.252006       0.214029       0.169860   \n",
       "3      3.0  18       0.450969       0.387301       0.247143       0.448752   \n",
       "4      3.0  20       0.426481       0.272545       0.192479       0.184106   \n",
       "\n",
       "   bert_pred_5_1  bert_pred_1_2  bert_pred_2_2  bert_pred_3_2     ...       \\\n",
       "0       0.076740       0.484906       0.294941       0.220875     ...        \n",
       "1       0.117446       0.445859       0.238172       0.217901     ...        \n",
       "2       0.175528       0.664439       0.778905       0.830853     ...        \n",
       "3       0.165567       0.498336       0.543906       0.383343     ...        \n",
       "4       0.098343       0.473353       0.459011       0.220175     ...        \n",
       "\n",
       "   yellow  yet  yield  younger  zeiss  zookeep  lgb_preds_1  lgb_preds_2  \\\n",
       "0     0.0  0.0    0.0      0.0    0.0      0.0     0.124375     0.046654   \n",
       "1     0.0  0.0    0.0      0.0    0.0      0.0     0.119715     0.038722   \n",
       "2     0.0  0.0    0.0      0.0    0.0      0.0     0.118658     0.069503   \n",
       "3     0.0  0.0    0.0      0.0    0.0      0.0     0.145846     0.090250   \n",
       "4     0.0  0.0    0.0      0.0    0.0      0.0     0.098323     0.045169   \n",
       "\n",
       "   lgb_preds_3  lgb_preds_4  \n",
       "0     0.776604     0.049806  \n",
       "1     0.906843     0.025833  \n",
       "2     0.623205     0.242803  \n",
       "3     0.390933     0.200563  \n",
       "4     0.565293     0.142971  \n",
       "\n",
       "[5 rows x 3409 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bert_pred_1_1</th>\n",
       "      <th>bert_pred_2_1</th>\n",
       "      <th>bert_pred_3_1</th>\n",
       "      <th>bert_pred_4_1</th>\n",
       "      <th>bert_pred_5_1</th>\n",
       "      <th>bert_pred_1_2</th>\n",
       "      <th>bert_pred_2_2</th>\n",
       "      <th>bert_pred_3_2</th>\n",
       "      <th>bert_pred_4_2</th>\n",
       "      <th>...</th>\n",
       "      <th>zeiss</th>\n",
       "      <th>zookeep</th>\n",
       "      <th>preds_1</th>\n",
       "      <th>preds_2</th>\n",
       "      <th>preds_3</th>\n",
       "      <th>preds_4</th>\n",
       "      <th>lgb_preds_1</th>\n",
       "      <th>lgb_preds_2</th>\n",
       "      <th>lgb_preds_3</th>\n",
       "      <th>lgb_preds_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2931</td>\n",
       "      <td>0.304999</td>\n",
       "      <td>0.261890</td>\n",
       "      <td>0.256754</td>\n",
       "      <td>0.299635</td>\n",
       "      <td>0.292701</td>\n",
       "      <td>0.487088</td>\n",
       "      <td>0.482487</td>\n",
       "      <td>0.499640</td>\n",
       "      <td>0.490313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286953</td>\n",
       "      <td>0.300346</td>\n",
       "      <td>1.255970</td>\n",
       "      <td>0.246627</td>\n",
       "      <td>0.146160</td>\n",
       "      <td>0.173406</td>\n",
       "      <td>0.528802</td>\n",
       "      <td>0.161975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2932</td>\n",
       "      <td>0.157685</td>\n",
       "      <td>0.186549</td>\n",
       "      <td>0.179390</td>\n",
       "      <td>0.162669</td>\n",
       "      <td>0.161642</td>\n",
       "      <td>0.204091</td>\n",
       "      <td>0.194912</td>\n",
       "      <td>0.183070</td>\n",
       "      <td>0.193481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160164</td>\n",
       "      <td>0.229296</td>\n",
       "      <td>1.148036</td>\n",
       "      <td>0.251646</td>\n",
       "      <td>0.063599</td>\n",
       "      <td>0.112505</td>\n",
       "      <td>0.616123</td>\n",
       "      <td>0.151675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2933</td>\n",
       "      <td>0.109045</td>\n",
       "      <td>0.131893</td>\n",
       "      <td>0.112129</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>0.127725</td>\n",
       "      <td>0.169313</td>\n",
       "      <td>0.180290</td>\n",
       "      <td>0.184457</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301584</td>\n",
       "      <td>0.134988</td>\n",
       "      <td>0.992523</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.152356</td>\n",
       "      <td>0.077784</td>\n",
       "      <td>0.506150</td>\n",
       "      <td>0.182886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2934</td>\n",
       "      <td>0.914908</td>\n",
       "      <td>0.937084</td>\n",
       "      <td>0.923681</td>\n",
       "      <td>0.940199</td>\n",
       "      <td>0.922476</td>\n",
       "      <td>0.194414</td>\n",
       "      <td>0.175685</td>\n",
       "      <td>0.217253</td>\n",
       "      <td>0.189669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.596967</td>\n",
       "      <td>0.167668</td>\n",
       "      <td>0.189846</td>\n",
       "      <td>0.133128</td>\n",
       "      <td>0.766366</td>\n",
       "      <td>0.098118</td>\n",
       "      <td>0.095914</td>\n",
       "      <td>0.079734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2935</td>\n",
       "      <td>0.129471</td>\n",
       "      <td>0.158606</td>\n",
       "      <td>0.149187</td>\n",
       "      <td>0.163271</td>\n",
       "      <td>0.166768</td>\n",
       "      <td>0.138063</td>\n",
       "      <td>0.127007</td>\n",
       "      <td>0.146903</td>\n",
       "      <td>0.132465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219979</td>\n",
       "      <td>0.060010</td>\n",
       "      <td>1.269596</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.094777</td>\n",
       "      <td>0.034124</td>\n",
       "      <td>0.636871</td>\n",
       "      <td>0.592658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3412 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  bert_pred_1_1  bert_pred_2_1  bert_pred_3_1  bert_pred_4_1  \\\n",
       "0  2931       0.304999       0.261890       0.256754       0.299635   \n",
       "1  2932       0.157685       0.186549       0.179390       0.162669   \n",
       "2  2933       0.109045       0.131893       0.112129       0.123854   \n",
       "3  2934       0.914908       0.937084       0.923681       0.940199   \n",
       "4  2935       0.129471       0.158606       0.149187       0.163271   \n",
       "\n",
       "   bert_pred_5_1  bert_pred_1_2  bert_pred_2_2  bert_pred_3_2  bert_pred_4_2  \\\n",
       "0       0.292701       0.487088       0.482487       0.499640       0.490313   \n",
       "1       0.161642       0.204091       0.194912       0.183070       0.193481   \n",
       "2       0.127725       0.169313       0.180290       0.184457       0.186450   \n",
       "3       0.922476       0.194414       0.175685       0.217253       0.189669   \n",
       "4       0.166768       0.138063       0.127007       0.146903       0.132465   \n",
       "\n",
       "      ...       zeiss  zookeep   preds_1   preds_2   preds_3   preds_4  \\\n",
       "0     ...         0.0      0.0  0.286953  0.300346  1.255970  0.246627   \n",
       "1     ...         0.0      0.0  0.160164  0.229296  1.148036  0.251646   \n",
       "2     ...         0.0      0.0  0.301584  0.134988  0.992523  0.277500   \n",
       "3     ...         0.0      0.0  1.596967  0.167668  0.189846  0.133128   \n",
       "4     ...         0.0      0.0  0.219979  0.060010  1.269596  0.909977   \n",
       "\n",
       "   lgb_preds_1  lgb_preds_2  lgb_preds_3  lgb_preds_4  \n",
       "0     0.146160     0.173406     0.528802     0.161975  \n",
       "1     0.063599     0.112505     0.616123     0.151675  \n",
       "2     0.152356     0.077784     0.506150     0.182886  \n",
       "3     0.766366     0.098118     0.095914     0.079734  \n",
       "4     0.094777     0.034124     0.636871     0.592658  \n",
       "\n",
       "[5 rows x 3412 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4d70c12b764ffd8a44ff30b8d792d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "off_df_2=[]\n",
    "test_preds = np.zeros(shape=(len(test_df2),4))\n",
    "bert_pred_cols =['bert_pred_1_1', 'bert_pred_2_1', 'bert_pred_3_1', 'bert_pred_4_1', 'bert_pred_5_1', 'bert_pred_1_2', 'bert_pred_2_2',\n",
    "       'bert_pred_3_2', 'bert_pred_4_2', 'bert_pred_5_2', 'bert_pred_1_3', 'bert_pred_2_3', 'bert_pred_3_3', 'bert_pred_4_3', \n",
    "       'bert_pred_5_3', 'bert_pred_1_4', 'bert_pred_2_4', 'bert_pred_3_4', 'bert_pred_4_4', 'bert_pred_5_4']\n",
    "for trn, val in k.split(train_df, train_df.jobflag):\n",
    "    trn_df = off_df.iloc[trn,:]\n",
    "    val_df  =  off_df.iloc[val,:]\n",
    "    \n",
    "    min_value = trn_df.jobflag.value_counts().min()\n",
    "    \n",
    "    preds = np.zeros(shape=(len(val_df),4))\n",
    "    \n",
    "    for i in tqdm(range(80)):\n",
    "        tmp_trn_df = pd.concat(\n",
    "        [trn_df[trn_df.jobflag==1].sample(n=min_value, random_state=i),\n",
    "         trn_df[trn_df.jobflag==2].sample(n=min_value, random_state=i),\n",
    "         trn_df[trn_df.jobflag==3].sample(n=min_value, random_state=i),\n",
    "         trn_df[trn_df.jobflag==4].sample(n=min_value, random_state=i)], axis=0).reset_index(drop=True)\n",
    "        tmp_trn_X = tmp_trn_df[bert_pred_cols+[ 'lgb_preds_1', 'lgb_preds_2', 'lgb_preds_3', 'lgb_preds_4']]\n",
    "        tmp_trn_y = tmp_trn_df['jobflag']\n",
    "        \n",
    "        \n",
    "        for penalty  in [ 'l2']:\n",
    "            for m in range(5):\n",
    "                logit = LogisticRegression(penalty=penalty, random_state=m)\n",
    "                logit.fit(tmp_trn_X, tmp_trn_y)\n",
    "\n",
    "                    #ridge_cls = RidgeClassifier()\n",
    "                    #ridge_cls.fit(tmp_trn_X, tmp_trn_y)\n",
    "\n",
    "                    #kncls = KNeighborsClassifier(n_neighbors=4)\n",
    "                    #kncls.fit(tmp_trn_X, tmp_trn_y)\n",
    "                preds += logit.predict_proba(val_df[bert_pred_cols+[ 'lgb_preds_1', 'lgb_preds_2', 'lgb_preds_3', 'lgb_preds_4']])\n",
    "                test_preds += logit.predict_proba(test_df2[bert_pred_cols+[ 'lgb_preds_1', 'lgb_preds_2', 'lgb_preds_3', 'lgb_preds_4']])\n",
    "                \n",
    "    val_df[f'preds'] = np.argmax(preds, axis=1)+1\n",
    "    off_df_2.append(val_df)\n",
    "\n",
    "test_df2[f'preds'] = np.argmax(test_preds, axis=1)+1\n",
    "off_df_2 = pd.concat(off_df_2, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.f1_score(off_df_2.jobflag, off_df_2.preds, average='macro'))\n",
    "plt.figure(figsize=(10,10))\n",
    "cnfn_matrix = pd.DataFrame(metrics.confusion_matrix(off_df_2.jobflag, off_df_2.preds))\n",
    "#cnfn_matrix.index = \n",
    "sns.heatmap(cnfn_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "cnfn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2.preds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../submit_sample.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[1] = test_df2.preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('/Users/kanoumotoharu/Downloads/sub_14.csv',  index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10127369, 0.40086803, 0.27877044, 0.21908784],\n",
       "       [0.08530234, 0.28073404, 0.39763328, 0.23633033],\n",
       "       [0.17745063, 0.25154499, 0.33936384, 0.23164054],\n",
       "       ...,\n",
       "       [0.25766106, 0.2301475 , 0.22386374, 0.2883277 ],\n",
       "       [0.06947772, 0.0708203 , 0.41543539, 0.44426659],\n",
       "       [0.07184275, 0.12031271, 0.59142699, 0.21641754]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds/(80*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
