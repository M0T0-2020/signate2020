{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, KFold, ShuffleSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import time, gc, random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "import nltk, string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "random.seed(2020)\n",
    "np.random.seed(2020)\n",
    "torch.cuda.manual_seed_all(2020)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "        self.stop_words = get_stop_words('en')\n",
    "        self.stop_words.append(' ')\n",
    "        self.stop_words.append('')\n",
    "    \n",
    "    def pipeline(self, df):\n",
    "        for lang in ['description']:\n",
    "            #, 'translate_es', 'translate_fr', 'translate_de', 'translate_ja']:\n",
    "            df[lang] = df[lang].apply(lambda x: self.change_text(x))\n",
    "        return df\n",
    "\n",
    "    def change_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace('ml', 'machine learning')\n",
    "        text = \"\".join([char if char not in string.punctuation else ' ' for char in text])\n",
    "        text = \" \".join([self.porter.stem(char) for char in text.split(' ') if char not in self.stop_words])\n",
    "        return text\n",
    "    \n",
    "    def vectorize_tfidf(self, df):\n",
    "        vec_tfidf = TfidfVectorizer()\n",
    "        X = vec_tfidf.fit_transform(df.values)\n",
    "        X = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n",
    "        return X\n",
    "    \n",
    "    def vectorize_cnt(self, df):\n",
    "        vec_cnt = CountVectorizer()\n",
    "        X = vec_cnt.fit_transform(df.values)\n",
    "        X = pd.DataFrame(X.toarray(), columns=vec_cnt.get_feature_names())\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        \n",
    "    def __len__(self):  # len(Dataset)で返す値を指定\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
    "        # argumentation\n",
    "        x = self.X[index]\n",
    "        x = torch.FloatTensor(x)\n",
    "        \n",
    "        \n",
    "        if self.y!=None:\n",
    "            return {\n",
    "                'input': x,\n",
    "                'label': torch.FloatTensor(self.y[index])\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input': x\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "        callback_get_label func: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = callback_get_label\n",
    "\n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / np.sqrt(label_to_count[self._get_label(dataset, idx)])\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        \"\"\"if isinstance(dataset, torchvision.datasets.MNIST):\n",
    "            return dataset.train_labels[idx].item()\n",
    "        elif isinstance(dataset, torchvision.datasets.ImageFolder):\n",
    "            return dataset.imgs[idx][1]\n",
    "        elif isinstance(dataset, torch.utils.data.Subset):\n",
    "            return dataset.dataset.imgs[idx][1]\n",
    "        elif self.callback_get_label:\n",
    "            return self.callback_get_label(dataset, idx)\n",
    "        else:\n",
    "            raise NotImplementedError\"\"\"\n",
    "        return dataset.y[idx][0]\n",
    "        \n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred_1 = pred[target<1.5][:,0]\n",
    "        loss_1 = torch.mean((pred_1-1)**2)\n",
    "        \n",
    "        pred_2 = pred[(target>1.5)&(target<2.5)][:,1]\n",
    "        loss_2 = torch.mean((pred_2-1)**2)\n",
    "        \n",
    "        pred_3 = pred[(target>2.5)&(target<3.5)][:,2]\n",
    "        loss_3 = torch.mean((pred_3-1)**2)\n",
    "\n",
    "        pred_4 = pred[target>3.5][:,3]\n",
    "        loss_4 = torch.mean((pred_4-1)**2)\n",
    "        \n",
    "        loss = loss_1 + loss_2 + loss_3 + loss_4\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClass(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_layers[0]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_layers[0], hidden_layers[1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_layers[1], output_size)\n",
    "        )\n",
    "        \n",
    "            \n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        #x= self.sigmoid(x)\n",
    "        x= self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Trainer:\n",
    "    def __init__(self, output_size, feature, hidden_layers, lr=6e-4, weight_decay=0.0005):\n",
    "        self.device =  'cuda' if cuda.is_available() else 'cpu'\n",
    "        self.model = MLPClass(len(feature), output_size, hidden_layers).to(self.device)\n",
    "        self.criterion = CustomLoss()\n",
    "        self.optimizer = optim.Adam(params=self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.scheduler = CosineAnnealingLR(optimizer=self.optimizer, T_max=10)\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "    \n",
    "    def load_params(self, state_dict):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    \n",
    "    def train(self, trn_dataloader):\n",
    "        self.model.train()\n",
    "        avg_loss=0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for data in trn_dataloader:\n",
    "            self.optimizer.zero_grad()\n",
    "            x = data['input'].to(self.device)\n",
    "            label = data['label'].squeeze(1)\n",
    "            if len(np.unique(label))!=4:\n",
    "                continue\n",
    "            label = label.to(self.device)\n",
    "            x = self.model(x)   \n",
    "            loss = self.criterion(x, label)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            avg_loss += loss.item()/len(trn_dataloader)\n",
    "            all_preds+=x.detach().cpu().tolist()\n",
    "            all_labels+=label.cpu().tolist()\n",
    "            \n",
    "        return avg_loss, np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "    def eval(self, val_dataloader):\n",
    "        self.model.eval()\n",
    "        avg_loss=0\n",
    "        for data in val_dataloader:\n",
    "            x = data['input'].to(self.device)\n",
    "            label = data['label'].to(self.device).squeeze(1)\n",
    "            x = self.model(x)\n",
    "            loss = self.criterion(x, label)\n",
    "            avg_loss += loss.item()/len(val_dataloader)\n",
    "        return avg_loss, x.detach().cpu().numpy()\n",
    "    \n",
    "    def predict(self, data_loader):\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        for data in data_loader:\n",
    "            x = data['input'].to(self.device)\n",
    "            preds+=self.model(x).detach().cpu().tolist()\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Predict:\n",
    "    \n",
    "    def __init__(self, train_df, test_df, feature, hidden_layers, lr=6e-4, weight_decay=0.0005, loop_num=3):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.feature = feature\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lr=lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loop_num=loop_num\n",
    "        self.torch_random_state = 2020\n",
    "        torch.cuda.manual_seed_all(self.torch_random_state)\n",
    "    \n",
    "    def init_model(self):\n",
    "        self.model_trainer = Model_Trainer(output_size=4, feature=self.feature, hidden_layers=self.hidden_layers,\n",
    "                                           lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "    def make_off_df(self, epoch_num, k):\n",
    "        if k==None:\n",
    "            k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "        \n",
    "        trn_cv_loss = []\n",
    "        val_cv_loss = []\n",
    "        trn_score = []\n",
    "        val_score = []\n",
    "        \n",
    "        off_df=[]\n",
    "\n",
    "        for trn, val in tqdm( k.split(self.train_df, self.train_df.jobflag), total=k.n_splits ):\n",
    "            trn_df = self.train_df.iloc[\n",
    "                trn,:\n",
    "            ]\n",
    "            val_df = self.train_df.iloc[\n",
    "                val,:\n",
    "            ]\n",
    "\n",
    "            trn_X, trn_y = trn_df[self.feature].values.tolist(),trn_df[['jobflag']].values.tolist() \n",
    "            val_X, val_y = val_df[self.feature].values.tolist(),val_df[['jobflag']].values.tolist() \n",
    "\n",
    "\n",
    "            trn_data_set = CreateDataset(trn_X, trn_y)\n",
    "            val_data_set = CreateDataset(val_X, val_y)\n",
    "            trn_dataloader = DataLoader(trn_data_set, shuffle=False, batch_size=512, sampler=ImbalancedDatasetSampler(trn_data_set))\n",
    "            val_dataloader = DataLoader(val_data_set, shuffle=False, batch_size=len(val_data_set))\n",
    "\n",
    "            all_trn_loss=[]\n",
    "            all_val_loss=[]\n",
    "            all_trn_score=[]\n",
    "            all_val_score=[]\n",
    "            \n",
    "            # make columns\n",
    "            for e in range(epoch_num):\n",
    "                for mm in range(4):\n",
    "                    val_df[f'p_{mm+1}_{e}'] = 0  \n",
    "                    \n",
    "            for loop in range(self.loop_num):\n",
    "                self.torch_random_state+=1\n",
    "                torch.cuda.manual_seed_all(self.torch_random_state)\n",
    "                \n",
    "                trn_loss_list=[]\n",
    "                val_loss_list=[]\n",
    "                trn_score_list=[]\n",
    "                self.init_model()\n",
    "                \n",
    "                for e in range(epoch_num):\n",
    "                    trn_loss_avg, trn_preds, trn_labels = self.model_trainer.train(trn_dataloader)\n",
    "                    val_loss_avg, p = self.model_trainer.eval(val_dataloader)\n",
    "                    \n",
    "                    trn_preds = np.argmax(trn_preds, axis=1) + 1\n",
    "                    trn_score_list.append(metrics.f1_score(trn_labels, trn_preds, average='macro'))\n",
    "                    \n",
    "                    trn_loss_list.append(trn_loss_avg)\n",
    "                    val_loss_list.append(val_loss_avg)\n",
    "                    \n",
    "                    for mm in range(4):\n",
    "                        val_df[f'p_{mm+1}_{e}'] += p[:,mm]/self.loop_num\n",
    "                    \n",
    "                all_trn_loss.append(trn_loss_list)\n",
    "                all_val_loss.append(val_loss_list)\n",
    "                all_trn_score.append(trn_score_list)\n",
    "                \n",
    "            for e in range(epoch_num):\n",
    "                p = val_df[[f'p_{mm+1}_{e}' for mm in range(4)]].values\n",
    "                val_df[f'p_{e}'] = np.argmax(p, axis=1)+1\n",
    "                all_val_score.append( metrics.f1_score(val_df['jobflag'], val_df[f'p_{e}'], average='macro'))\n",
    "            \n",
    "            trn_cv_loss.append(np.mean(all_trn_loss, axis=0))\n",
    "            val_cv_loss.append(np.mean(all_val_loss, axis=0))\n",
    "            trn_score.append(np.mean(all_trn_score, axis=0))\n",
    "            val_score.append(all_val_score)\n",
    "            off_df.append(val_df)\n",
    "\n",
    "        off_df = pd.concat(off_df, axis=0)\n",
    "        off_df.sort_values('text_id', inplace=True)\n",
    "        off_df.reset_index(drop=True, inplace=True)\n",
    "        return off_df, trn_cv_loss, val_cv_loss, trn_score, val_score\n",
    "    \n",
    "    def predict_test_df(self, epoch_num):\n",
    "\n",
    "        X, y = self.train_df[self.feature].values.tolist(), self.train_df[['jobflag']].values.tolist()\n",
    "        trn_data_set = CreateDataset(X, y)\n",
    "        trn_dataloader = DataLoader(trn_data_set, shuffle=False, batch_size=512, sampler=ImbalancedDatasetSampler(trn_data_set))\n",
    "\n",
    "        val_X, val_y = self.test_df[self.feature].values.tolist(), None\n",
    "        val_data_set = CreateDataset(val_X, val_y)\n",
    "        val_dataloader = DataLoader(val_data_set, shuffle=False, batch_size=len(val_data_set))\n",
    "        \n",
    "        all_trn_loss = []\n",
    "        test_df = self.test_df.copy()\n",
    "        #make columns\n",
    "        for e in range(epoch_num):\n",
    "            for mm in range(4):\n",
    "                test_df[f'p_{mm+1}_{e}'] = 0\n",
    "        \n",
    "        for loop in tqdm(range(self.loop_num)):\n",
    "            self.torch_random_state+=1\n",
    "            torch.cuda.manual_seed_all(self.torch_random_state)\n",
    "            \n",
    "            self.init_model()\n",
    "            trn_loss_list=[]\n",
    "            \n",
    "            for e in range(epoch_num):\n",
    "                trn_loss_avg = self.model_trainer.train(trn_dataloader)\n",
    "                p = self.model_trainer.predict(val_dataloader)\n",
    "                for mm in range(4):\n",
    "                    test_df[f'p_{mm+1}_{e}'] += p[:,mm]/self.loop_num\n",
    "                trn_loss_list.append(trn_loss_avg)\n",
    "                \n",
    "            all_trn_loss.append(trn_loss_list)\n",
    "            \n",
    "        trn_loss_list = np.mean(all_trn_loss, axis=0)\n",
    "\n",
    "        return test_df, trn_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKFlod:\n",
    "    def __init__(self, n_splits, random_state, val_distribution):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.val_distribution = val_distribution\n",
    "        \n",
    "    def split(self, train_df, train_label):\n",
    "        val_size = len(train_df)/self.n_splits\n",
    "        val_nums = val_size*np.array(self.val_distribution)\n",
    "        \n",
    "        trn_idxs={}\n",
    "        val_idxs={}  \n",
    "        for i in range(self.n_splits):\n",
    "            trn_idxs[f'idx{i}_{_}'] = []\n",
    "            val_idxs[f'idx{i}_{_}'] = []\n",
    "        for label in range(4):\n",
    "            label_df = train_label[train_label==label+1]\n",
    "            label_size = val_nums[label]/len(label_df)\n",
    "\n",
    "            k = ShuffleSplit(n_splits=self.n_splits, train_size=label_size, random_state=self.random_state)\n",
    "            for i,(trn, val) in enumerate( k.split(label_df, label_df) ):\n",
    "                trn_idx = label_df.iloc[trn].index.tolist()\n",
    "                val_idx = label_df.iloc[val].index.tolist()\n",
    "                trn_idxs[f'idx{i}_{_}']+=trn_idx\n",
    "                val_idxs[f'idx{i}_{_}']+=val_idx\n",
    "                \n",
    "        return zip(np.array(list((trn_idxs.values()))), np.array(list((val_idxs.values()))))\n",
    "    \n",
    "    \n",
    "    \n",
    "class CustomSplit:\n",
    "    def __init__(self, n_splits, random_state, val_distribution):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.val_distribution = val_distribution\n",
    "        \n",
    "    def split(self, train_df, train_label):\n",
    "        val_size = len(train_df)/self.n_splits\n",
    "        val_nums = val_size*np.array(self.val_distribution)\n",
    "        \n",
    "        trn_idxs={}\n",
    "        val_idxs={}  \n",
    "        for i in range(self.n_splits):\n",
    "            trn_idxs[f'idx{i}_{_}'] = []\n",
    "            val_idxs[f'idx{i}_{_}'] = []\n",
    "        for label in range(4):\n",
    "            label_df = train_label[train_label==label+1]\n",
    "            label_size = val_nums[label]/len(label_df)\n",
    "\n",
    "            k = ShuffleSplit(n_splits=self.n_splits, train_size=label_size, random_state=self.random_state)\n",
    "            for i,(trn, val) in enumerate( k.split(label_df, label_df) ):\n",
    "                trn_idx = label_df.iloc[trn].index.tolist()\n",
    "                val_idx = label_df.iloc[val].index.tolist()\n",
    "                trn_idxs[f'idx{i}_{_}']+=trn_idx\n",
    "                val_idxs[f'idx{i}_{_}']+=val_idx\n",
    "                \n",
    "        return zip(np.array(list((trn_idxs.values()))), np.array(list((val_idxs.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tfidf_df(o_df, col='description'):\n",
    "    df = o_df.copy()\n",
    "    id_cols = ['jobflag','text_id']\n",
    "    preprocessing = Preprocessing()\n",
    "    for lan_col in ['description','translate_de', 'translate_es', 'translate_fr', 'translate_ja']:\n",
    "        df[lan_col] = df[lan_col].apply(lambda x: preprocessing.change_text(x))\n",
    "        \n",
    "    X = preprocessing.vectorize_tfidf(df[col])\n",
    "    X = pd.concat([df[id_cols], X], axis=1)\n",
    "    train_df = X[X.jobflag.notnull()].reset_index(drop=True)\n",
    "    test_df = X[X.jobflag.isnull()].drop(columns=['jobflag']).reset_index(drop=True)\n",
    "    mlp_feature = train_df.drop(columns=id_cols).columns.tolist()\n",
    "    return train_df, test_df, mlp_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class All_flow:\n",
    "    def __init__(self, path='../', step1_k=None, step2_k=None, step1_epoch_num=40, step2_epoch_num=60):\n",
    "        train_df = pd.read_csv(path+'train_translated.csv')\n",
    "        test_df = pd.read_csv(path+'test_translated.csv')\n",
    "        self.df = pd.concat([train_df, test_df],axis=0,ignore_index=True)\n",
    "        self.df['text_id'] = self.df['id']\n",
    "        self.df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "        self.step1_k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True) if step1_k==None else step1_k\n",
    "        self.step2_k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True) if step2_k==None else step2_k\n",
    "            \n",
    "        self.step1_epoch_num = step1_epoch_num\n",
    "        self.step2_epoch_num = step2_epoch_num\n",
    "\n",
    "    def main(self):\n",
    "        result_dic={}\n",
    "        off_df_dic={}\n",
    "        test_df_dic={}\n",
    "        \n",
    "        languages = ['description']\n",
    "        \n",
    "        for language in languages:\n",
    "            train_df, test_df, mlp_feature = make_tfidf_df(self.df, language)\n",
    "\n",
    "            train_predict = Train_Predict(train_df, test_df, mlp_feature, hidden_layers=[300, 100], lr=0.0006, weight_decay=0.0005)\n",
    "\n",
    "            off_df, trn_cv_loss, val_cv_loss, trn_score, val_score = train_predict.make_off_df(epoch_num=self.step1_epoch_num, k=self.step1_k)\n",
    "            test_df_mlp, trn_loss_list = train_predict.predict_test_df(epoch_num=self.step1_epoch_num)\n",
    "\n",
    "            \n",
    "            data = pd.DataFrame()  \n",
    "            for i in range(len(trn_cv_loss)):\n",
    "                data[f'train_trn_loss_k{i+1}'] = trn_cv_loss[i]\n",
    "                data[f'train_val_loss_k{i+1}'] = val_cv_loss[i]\n",
    "                data[f'train_trn_score_k{i+1}'] = trn_score[i]\n",
    "                data[f'train_val_score_k{i+1}'] = val_score[i]\n",
    "\n",
    "            result_dic[language]=data\n",
    "            off_df_dic[language]=off_df\n",
    "            test_df_dic[language]=test_df_mlp\n",
    "\n",
    "        return off_df_dic, test_df_dic, result_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[df.jobflag.notna()].jobflag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    1376\n",
       "1.0     624\n",
       "4.0     583\n",
       "2.0     348\n",
       "Name: jobflag, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 348)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sort_values().index[0], a.sort_values().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458.6666666666667"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1376/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Predict:\n",
    "    \n",
    "    def __init__(self, train_df, test_df, feature, hidden_layers, lr=6e-4, weight_decay=0.0005, loop_num=3):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.feature = feature\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lr=lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loop_num=loop_num\n",
    "        self.torch_random_state = 2020\n",
    "        torch.cuda.manual_seed_all(self.torch_random_state)\n",
    "    \n",
    "    def init_model(self):\n",
    "        self.model_trainer = Model_Trainer(output_size=4, feature=self.feature, hidden_layers=self.hidden_layers,\n",
    "                                           lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "    def make_off_df(self, epoch_num, k):\n",
    "        if k==None:\n",
    "            k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "        \n",
    "        trn_cv_loss = []\n",
    "        val_cv_loss = []\n",
    "        trn_score = []\n",
    "        val_score = []\n",
    "        \n",
    "        off_df=[]\n",
    "\n",
    "        for trn, val in tqdm( k.split(self.train_df, self.train_df.jobflag), total=k.n_splits ):\n",
    "            \n",
    "            all_trn_df = self.train_df.iloc[\n",
    "                trn,:\n",
    "            ]\n",
    "            val_df = self.train_df.iloc[\n",
    "                val,:\n",
    "            ]\n",
    "            \n",
    "            val_X, val_y = val_df[self.feature].values.tolist(),val_df[['jobflag']].values.tolist() \n",
    "            val_data_set = CreateDataset(val_X, val_y)\n",
    "            val_dataloader = DataLoader(val_data_set, shuffle=False, batch_size=len(val_data_set))\n",
    "            \n",
    "            all_trn_df_1 = all_trn_df[all_trn_df.jobflag==1]\n",
    "            all_trn_df_2 = all_trn_df[all_trn_df.jobflag==2]\n",
    "            all_trn_df_3 = all_trn_df[all_trn_df.jobflag==3]\n",
    "            all_trn_df_4 = all_trn_df[all_trn_df.jobflag==4]\n",
    "\n",
    "            k_1 = ShuffleSplit(n_splits=5, random_state=2020, train_size=len(all_trn_df_2))\n",
    "            k_3 = ShuffleSplit(n_splits=8, random_state=2020, train_size=len(all_trn_df_3))\n",
    "            k_4 = ShuffleSplit(n_splits=5, random_state=2020, train_size=len(all_trn_df_4))\n",
    "            \n",
    "            \n",
    "            # make columns\n",
    "            for e in range(epoch_num):\n",
    "                for mm in range(4):\n",
    "                    val_df[f'p_{mm+1}_{e}'] = 0  \n",
    "            \n",
    "            all_trn_loss=[]\n",
    "            all_val_loss=[]\n",
    "            all_trn_score=[]\n",
    "            all_val_score=[]\n",
    "            \n",
    "            for trn_1, val_1 in k_1.split(all_trn_df_1):\n",
    "                for trn_3, val_3 in k_1.split(all_trn_df_3):\n",
    "                    for trn_4, val_4 in k_1.split(all_trn_df_4):\n",
    "                        trn_df_1 = all_trn_df.iloc[trn_1]\n",
    "                        trn_df_3 = all_trn_df.iloc[trn_3]\n",
    "                        trn_df_4 = all_trn_df.iloc[trn_4]\n",
    "                        \n",
    "                        trn_df = pd.concat([\n",
    "                            trn_df_1,all_trn_df_3,trn_df_3,trn_df_4\n",
    "                        ], axis=0)\n",
    "                        \n",
    "                        trn_X, trn_y = trn_df[self.feature].values.tolist(),trn_df[['jobflag']].values.tolist() \n",
    "                        trn_data_set = CreateDataset(trn_X, trn_y)\n",
    "                        trn_dataloader = DataLoader(trn_data_set, shuffle=False, batch_size=512, sampler=ImbalancedDatasetSampler(trn_data_set))\n",
    "\n",
    "                        \n",
    "\n",
    "            \n",
    "                    \n",
    "                        for loop in range(self.loop_num):\n",
    "                            self.torch_random_state+=1\n",
    "                            torch.cuda.manual_seed_all(self.torch_random_state)\n",
    "\n",
    "                            trn_loss_list=[]\n",
    "                            val_loss_list=[]\n",
    "                            trn_score_list=[]\n",
    "                            self.init_model()\n",
    "\n",
    "                            for e in range(epoch_num):\n",
    "                                trn_loss_avg, trn_preds, trn_labels = self.model_trainer.train(trn_dataloader)\n",
    "                                val_loss_avg, p = self.model_trainer.eval(val_dataloader)\n",
    "\n",
    "                                trn_preds = np.argmax(trn_preds, axis=1) + 1\n",
    "                                trn_score_list.append(metrics.f1_score(trn_labels, trn_preds, average='macro'))\n",
    "\n",
    "                                trn_loss_list.append(trn_loss_avg)\n",
    "                                val_loss_list.append(val_loss_avg)\n",
    "\n",
    "                                for mm in range(4):\n",
    "                                    val_df[f'p_{mm+1}_{e}'] += p[:,mm]\n",
    "\n",
    "                            all_trn_loss.append(trn_loss_list)\n",
    "                            all_val_loss.append(val_loss_list)\n",
    "                            all_trn_score.append(trn_score_list)\n",
    "                \n",
    "            for e in range(epoch_num):\n",
    "                p = val_df[[f'p_{mm+1}_{e}' for mm in range(4)]].values\n",
    "                val_df[f'p_{e}'] = np.argmax(p, axis=1)+1\n",
    "                all_val_score.append( metrics.f1_score(val_df['jobflag'], val_df[f'p_{e}'], average='macro'))\n",
    "            \n",
    "            trn_cv_loss.append(np.mean(all_trn_loss, axis=0))\n",
    "            val_cv_loss.append(np.mean(all_val_loss, axis=0))\n",
    "            trn_score.append(np.mean(all_trn_score, axis=0))\n",
    "            val_score.append(all_val_score)\n",
    "            off_df.append(val_df)\n",
    "\n",
    "        off_df = pd.concat(off_df, axis=0)\n",
    "        off_df.sort_values('text_id', inplace=True)\n",
    "        off_df.reset_index(drop=True, inplace=True)\n",
    "        return off_df, trn_cv_loss, val_cv_loss, trn_score, val_score\n",
    "    \n",
    "    def predict_test_df(self, epoch_num):\n",
    "\n",
    "        \n",
    "\n",
    "        val_X, val_y = self.test_df[self.feature].values.tolist(), None\n",
    "        val_data_set = CreateDataset(val_X, val_y)\n",
    "        val_dataloader = DataLoader(val_data_set, shuffle=False, batch_size=len(val_data_set))\n",
    "        \n",
    "        all_trn_loss = []\n",
    "        test_df = self.test_df.copy()\n",
    "        \n",
    "        \n",
    "        X, y = self.train_df[self.feature].values.tolist(), self.train_df[['jobflag']].values.tolist()\n",
    "        trn_data_set = CreateDataset(X, y)\n",
    "        trn_dataloader = DataLoader(trn_data_set, shuffle=False, batch_size=512, sampler=ImbalancedDatasetSampler(trn_data_set))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #make columns\n",
    "        for e in range(epoch_num):\n",
    "            for mm in range(4):\n",
    "                test_df[f'p_{mm+1}_{e}'] = 0\n",
    "        \n",
    "        for loop in tqdm(range(self.loop_num)):\n",
    "            self.torch_random_state+=1\n",
    "            torch.cuda.manual_seed_all(self.torch_random_state)\n",
    "            \n",
    "            self.init_model()\n",
    "            trn_loss_list=[]\n",
    "            \n",
    "            for e in range(epoch_num):\n",
    "                trn_loss_avg = self.model_trainer.train(trn_dataloader)\n",
    "                p = self.model_trainer.predict(val_dataloader)\n",
    "                for mm in range(4):\n",
    "                    test_df[f'p_{mm+1}_{e}'] += p[:,mm]/self.loop_num\n",
    "                trn_loss_list.append(trn_loss_avg)\n",
    "                \n",
    "            all_trn_loss.append(trn_loss_list)\n",
    "            \n",
    "        trn_loss_list = np.mean(all_trn_loss, axis=0)\n",
    "\n",
    "        return test_df, trn_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShuffleSplit(n_splits=5, random_state=2020, train_size=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
