{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "import nltk, string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "import optuna\n",
    "\n",
    "random.seed(2020)\n",
    "np.random.seed(2020)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "        self.stop_words = get_stop_words('en')\n",
    "        self.stop_words.append(' ')\n",
    "        self.stop_words.append('')\n",
    "    \n",
    "    def pipeline(self, df):\n",
    "        for lang in ['description']:\n",
    "            #, 'translate_es', 'translate_fr', 'translate_de', 'translate_ja']:\n",
    "            df[lang] = df[lang].apply(lambda x: self.change_text(x))\n",
    "        return df\n",
    "\n",
    "    def change_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace('ml', 'machine learning')\n",
    "        text = text.replace('machine learning', 'machinelearning')\n",
    "        text = \"\".join([char if char not in string.punctuation else ' ' for char in text])\n",
    "        text = \" \".join([self.porter.stem(char) for char in text.split(' ') if char not in self.stop_words])\n",
    "        return text\n",
    "    \n",
    "    def vectorize_tfidf(self, df):\n",
    "        vec_tfidf = TfidfVectorizer()\n",
    "        X = vec_tfidf.fit_transform(df.description.values)\n",
    "        X = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())\n",
    "        return X\n",
    "    \n",
    "    def vectorize_cnt(self, df):\n",
    "        vec_cnt = CountVectorizer()\n",
    "        X = vec_cnt.fit_transform(df.description.values)\n",
    "        X = pd.DataFrame(X.toarray(), columns=vec_cnt.get_feature_names())\n",
    "        return X\n",
    "\n",
    "\n",
    "class Optimize_by_Optuna:\n",
    "    def __init__(self, data, features, target_colname, target_name_2=None, _objective=None):\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.target = target_colname\n",
    "        if not target_colname:\n",
    "            self.target_2 = target_colname\n",
    "        else:\n",
    "            self.target_2 = target_name_2\n",
    "        self._objective = _objective\n",
    "        \n",
    "    \n",
    "    def make_score(self, y, preds):\n",
    "        s_1=1 - metrics.accuracy_score(y, preds)\n",
    "        s_2=list(self.model.best_score['valid_1'].values())[0]\n",
    "\n",
    "        return (s_1+s_2)/2\n",
    "\n",
    "    def objective(self, trial):\n",
    "                        \n",
    "        PARAMS = {#'boosting_type': 'gbdt', 'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            \n",
    "            #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "            \n",
    "            'objective': 'tweedie','metric': 'tweedie',\n",
    "            \n",
    "            'n_estimators': 1400,\n",
    "            'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "        \n",
    "\n",
    "            'tweedie_variance_power': trial.suggest_uniform('tweedie_variance_power', 1.01, 1.8),\n",
    "\n",
    "\n",
    "            'max_bin': trial.suggest_int('max_bin', 50, 300),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.4, 0.9),\n",
    "            'subsample_freq': trial.suggest_uniform('subsample_freq', 0.4, 0.9),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.03, 0.5),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 4, 2*5),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.0001, 10.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.0001, 10.0),\n",
    "        }\n",
    "        \n",
    "        score = 0\n",
    "        k = StratifiedKFold(n_splits=5)\n",
    "        for trn, val in k.split(self.data, self.data[self.target_2]):\n",
    "            train_df = self.data.iloc[trn,:]\n",
    "            val_df = self.data.iloc[val,:]\n",
    "            train_set= lgb.Dataset(train_df[self.features],  train_df[self.target])\n",
    "            val_set = lgb.Dataset(val_df[self.features],  val_df[self.target])   \n",
    "            \n",
    "            self.model = lgb.train(\n",
    "                train_set=train_set, valid_sets=[train_set, val_set], params=PARAMS, num_boost_round=3000, \n",
    "                early_stopping_rounds=200, verbose_eval=500\n",
    "                )\n",
    "                \n",
    "            preds = self.model.predict(val_df[self.features])\n",
    "            preds = np.round(preds)\n",
    "            y = val_df[self.target]\n",
    "            s = self.make_score(y, preds)\n",
    "            score+=s/5\n",
    "            \n",
    "        return score\n",
    "\n",
    "\n",
    "class Null_Importance:\n",
    "    def __init__(self, train_X, train_y, PARAMS, y_2=None):\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.y_2= y_2\n",
    "        self.PARAMS = PARAMS\n",
    "\n",
    "    def make_null_importance_df(self):\n",
    "        null_importance=pd.DataFrame()\n",
    "        null_importance['col'] = self.train_X.columns.tolist()\n",
    "        try:\n",
    "            for i in range(50):\n",
    "                tmp_null_importance=[]\n",
    "                \n",
    "                _train_y = self.train_y.apply(lambda x: random.choice([0,1]))\n",
    "                _train_y_2 = self.y_2.sample(frac=1).values\n",
    "                \n",
    "                print(f\"\"\"\n",
    "                \n",
    "                Train Null Importance   {i+1}\n",
    "                \n",
    "                \"\"\" )\n",
    "                k = StratifiedKFold(n_splits=5)\n",
    "                for trn, val in k.split(self.train_X, _train_y_2):\n",
    "                    trn_X, val_X = self.train_X.iloc[trn,:], self.train_X.iloc[val,:]\n",
    "                    trn_y, val_y = _train_y.iloc[trn].astype(int), _train_y.iloc[val].astype(int)\n",
    "                    train_set = lgb.Dataset(trn_X, trn_y)\n",
    "                    val_set = lgb.Dataset(val_X, val_y)\n",
    "\n",
    "                    model = lgb.train(params=self.PARAMS,\n",
    "                                      train_set=train_set, \n",
    "                                      valid_sets=[train_set, val_set],\n",
    "                                    num_boost_round=3000, early_stopping_rounds=200, verbose_eval=500)\n",
    "                    \n",
    "                    preds = model.predict(val_X)\n",
    "                    tmp_null_importance.append(model.feature_importance('gain'))\n",
    "                null_importance[f'null_importance_{i+1}'] = np.mean(tmp_null_importance, axis=0)\n",
    "            return null_importance\n",
    "        except:\n",
    "            return null_importance\n",
    "\n",
    "    def calu_importance(self, importance_df, null_importance_df):\n",
    "        importance_df = pd.merge(\n",
    "            importance_df, null_importance_df, on='col'\n",
    "            )\n",
    "        null_importance_col = [col for col in importance_df.columns if 'null' in col]\n",
    "        null_importance=pd.DataFrame()\n",
    "        for idx, row in importance_df.iterrows():\n",
    "            acc_v = 1e-10+row['true_importance']\n",
    "            null_v = 1+np.percentile(row[null_importance_col], 75)\n",
    "            null_importance[row['col']] = [np.log(acc_v/null_v)]\n",
    "        null_importance = null_importance.T\n",
    "        return null_importance\n",
    "\n",
    "    def all_flow(self):\n",
    "        k = StratifiedKFold(n_splits=5)\n",
    "        score=[]\n",
    "        importance=[]\n",
    "\n",
    "        importance_df=pd.DataFrame()\n",
    "        importance_df['col'] = self.train_X.columns\n",
    "        print(\"\"\"\n",
    "        \n",
    "        Train True Importance\n",
    "        \n",
    "        \"\"\" )\n",
    "        for trn, val in k.split(self.train_X, self.y_2):\n",
    "            trn_X, val_X = self.train_X.iloc[trn,:], self.train_X.iloc[val,:]\n",
    "            trn_y, val_y = self.train_y.iloc[trn].astype(int), self.train_y.iloc[val].astype(int)\n",
    "            train_set = lgb.Dataset(trn_X, trn_y)\n",
    "            val_set = lgb.Dataset(val_X, val_y)\n",
    "            \n",
    "            PARAMS['random_state']+=1\n",
    "            model = lgb.train(params=self.PARAMS, train_set=train_set, valid_sets=[train_set, val_set],\n",
    "                            num_boost_round=3000, early_stopping_rounds=200, verbose_eval=500)\n",
    "            preds = model.predict(val_X)\n",
    "            importance.append(model.feature_importance('gain'))\n",
    "        importance_df['true_importance'] = np.mean(importance, axis=0)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \n",
    "        Train Null Importance\n",
    "        \n",
    "        \"\"\" )\n",
    "        try:\n",
    "            null_importance_df = self.make_null_importance_df()\n",
    "        except:\n",
    "            pass\n",
    "        print(\"\"\"\n",
    "        \n",
    "        Calulate null_null_importance\n",
    "        \n",
    "        \"\"\" )\n",
    "        null_importance = self.calu_importance(importance_df, null_importance_df)\n",
    "        null_importance = null_importance.reset_index()\n",
    "        null_importance.columns = ['col', 'score']\n",
    "        null_importance = null_importance.sort_values('score', ascending=False)\n",
    "        return null_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Users/kanoumotoharu/Documents/signate_std_2020/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../train.csv')\n",
    "test_df = pd.read_csv('../test.csv')\n",
    "for i in range(4):    \n",
    "    train_df = pd.merge(train_df, pd.read_csv(f'../train_df_off{i+1}.csv'), on='id')\n",
    "    test_df = pd.merge(test_df, pd.read_csv(f'../test_df_off{i+1}.csv').drop(columns=['description', 'jobflag']), on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, test_df],axis=0,ignore_index=True)\n",
    "preprocessing = Preprocessing()\n",
    "df.description = df.description.apply(lambda x: preprocessing.change_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['jobflag','id','bert_pred_1_1', 'bert_pred_2_1', 'bert_pred_3_1',\n",
    "       'bert_pred_4_1', 'bert_pred_5_1', 'bert_pred_1_2', 'bert_pred_2_2',\n",
    "       'bert_pred_3_2', 'bert_pred_4_2', 'bert_pred_5_2', 'bert_pred_1_3',\n",
    "       'bert_pred_2_3', 'bert_pred_3_3', 'bert_pred_4_3', 'bert_pred_5_3',\n",
    "       'bert_pred_1_4', 'bert_pred_2_4', 'bert_pred_3_4', 'bert_pred_4_4',\n",
    "       'bert_pred_5_4']\n",
    "X = preprocessing.vectorize_tfidf(df)\n",
    "X = pd.concat([df[cols], X], axis=1)\n",
    "train_df = X[X.jobflag.notnull()].reset_index(drop=True)\n",
    "test_df = X[X.jobflag.isnull()].drop(columns=['jobflag']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['abil', 'abl', 'accept', 'access', 'accord', 'account', 'accur', 'accuraci', 'achiev', 'acquisit', 'across', 'act', 'action', \n",
    "           'activ', 'ad', 'addit', 'address', 'adher', 'administr', 'advanc', 'advis', 'advisor', 'agil', 'agre', 'ai', 'algorithm', \n",
    "           'align', 'analys', 'analysi', 'analyst', 'analyt', 'analyz', 'api', 'appli', 'applic', 'approach', 'appropri', 'approv', \n",
    "           'architect', 'architectur', 'area', 'assembl', 'assess', 'assign', 'assist', 'audienc', 'autom', 'avail', 'aw', 'back',\n",
    "           'backend', 'base', 'basic', 'behavior', 'benefit', 'best', 'board', 'bug', 'build', 'busi', 'call', 'can', 'candid', 'capabl',\n",
    "           'capac', 'case', 'caus', 'challeng', 'chang', 'clearli', 'client', 'clinic', 'close', 'cloud', 'cluster', 'coach', 'code', \n",
    "           'collabor', 'collect', 'commerci', 'commiss', 'commun', 'compani', 'complet', 'complex', 'complianc', 'compon', 'comput', \n",
    "           'concept', 'conduct', 'confer', 'configur', 'connect', 'consist', 'construct', 'consult', 'content', 'continu', 'contract',\n",
    "           'contribut', 'control', 'coordin', 'core', 'corpor', 'correct', 'cost', 'creat', 'creation', 'creativ', 'critic', 'cross', \n",
    "           'cultur', 'current', 'custom', 'cycl', 'daili', 'dashboard', 'data', 'databas', 'dataset', 'date', 'deadlin', 'debug', 'decis',\n",
    "           'deep', 'defect', 'defin', 'definit', 'deliv', 'deliver', 'deliveri', 'demand', 'demonstr', 'depart', 'depend', 'deploy', 'depth',\n",
    "           'deriv', 'design', 'desir', 'detail', 'detect', 'determin', 'develop', 'devic', 'devop', 'differ', 'digit', 'direct', 'disciplin',\n",
    "           'discoveri', 'discuss', 'distribut', 'divers', 'document', 'domain', 'draw', 'drive', 'duti', 'dynam', 'edg', 'educ', 'effect',\n",
    "           'effici', 'effort', 'electron', 'email', 'embed', 'employe', 'enabl', 'end', 'engag', 'engin', 'enhanc', 'ensur', 'enterpris',\n",
    "           'environ', 'equip', 'erp', 'escal', 'establish', 'estim', 'etc', 'evalu', 'event', 'excel', 'execut', 'exist', 'expand', 'experi',\n",
    "           'expert', 'expertis', 'explain', 'explor', 'exploratori', 'extern', 'extract', 'face', 'facilit', 'failur', 'featur', 'feder', \n",
    "           'field', 'find', 'fix', 'flow', 'focu', 'follow', 'form', 'formul', 'framework', 'front', 'full', 'function', 'futur', 'gain',\n",
    "           'gap', 'gather', 'gener', 'global', 'go', 'goal', 'good', 'govern', 'group', 'grow', 'growth', 'guid', 'guidanc', 'hand', \n",
    "           'hardwar', 'healthcar', 'help', 'high', 'highli', 'hoc', 'idea', 'identifi', 'impact', 'implement', 'improv', 'incid', 'includ',\n",
    "           'increas', 'independ', 'individu', 'industri', 'influenc', 'inform', 'infrastructur', 'initi', 'innov', 'input', 'insight',\n",
    "           'inspect', 'instal', 'integr', 'intellig', 'interact', 'interfac', 'intern', 'interpret', 'investig', 'issu', 'iter', 'java',\n",
    "           'job', 'junior', 'keep', 'key', 'knowledg', 'languag', 'larg', 'latest', 'lead', 'leader', 'leadership', 'learn', 'level', \n",
    "           'leverag', 'librari', 'life', 'like', 'limit', 'linux', 'log', 'logic', 'machin', 'machinelearn', 'maintain', 'mainten', \n",
    "           'make', 'manag', 'manner', 'manufactur', 'map', 'market', 'materi', 'matter', 'may', 'measur', 'mechan', 'medic', 'meet',\n",
    "           'member', 'mentor', 'met', 'method', 'methodolog', 'metric', 'microsoft', 'migrat', 'mission', 'mobil', 'model', 'moder',\n",
    "           'modifi', 'modul', 'monitor', 'multi', 'multipl', 'must', 'necessari', 'need', 'net', 'network', 'new', 'next', 'non', 'novel',\n",
    "           'object', 'obtain', 'ongo', 'open', 'oper', 'opportun', 'optim', 'order', 'organ', 'organiz', 'orient', 'outcom', 'outsid',\n",
    "           'overal', 'overse', 'part', 'parti', 'particip', 'partner', 'partnership', 'pattern', 'payrol', 'peer', 'perform', 'person',\n",
    "           'personnel', 'pipelin', 'plan', 'platform', 'point', 'polici', 'posit', 'post', 'potenti', 'practic', 'pre', 'predict', 'prepar', \n",
    "           'present', 'price', 'principl', 'prior', 'priorit', 'proactiv', 'problem', 'procedur', 'process', 'produc', 'product', \n",
    "           'profession', 'program', 'progress', 'project', 'promot', 'proof', 'propos', 'prospect', 'protocol', 'prototyp','provid', \n",
    "           'purpos', 'python', 'qa', 'qualifi', 'qualiti', 'queri', 'question', 'quickli', 'real', 'recommend', 'referr', 'refin', 'regard',\n",
    "           'region', 'regul', 'regular', 'regulatori', 'relat', 'relationship', 'releas', 'relev', 'reliabl', 'report','repres', 'request',\n",
    "           'requir', 'research', 'resid', 'resolut', 'resolv', 'resourc', 'respons', 'result', 'retail', 'review', 'rigor', 'risk', 'roadmap',\n",
    "           'role', 'root', 'rule', 'run', 'safeti', 'sale', 'scalabl', 'scale', 'schedul', 'scienc', 'scientist', 'scope', 'script', 'scrum',\n",
    "           'secur', 'segment', 'select', 'self', 'sell', 'senior', 'serv', 'server', 'servic', 'set', 'share', 'show', 'simul', 'site',\n",
    "           'skill', 'small', 'softwar', 'solut', 'solv', 'sourc', 'specif', 'sql', 'stack', 'staff', 'stakehold', 'standard', 'state',\n",
    "           'statist', 'statu', 'stay', 'store', 'stori', 'strateg', 'strategi', 'stream', 'strong', 'structur', 'studi', 'subject', \n",
    "           'success', 'suggest', 'supplier', 'support', 'system', 'take', 'target', 'task', 'team', 'technic', 'techniqu', 'technolog', \n",
    "           'term','test', 'think', 'thought', 'throughout', 'time', 'timelin', 'tool', 'top', 'track', 'train', 'transform', 'translat',\n",
    "           'travel', 'trend', 'troubleshoot', 'tune', 'understand', 'unit', 'updat', 'upgrad', 'use', 'user', 'util', 'valid',\n",
    "           'valu', 'variou', 'vehicl', 'vendor', 'verif', 'verifi', 'version', 'via', 'vision', 'visual', 'way',\n",
    "           'web', 'well', 'wide', 'will', 'window', 'within', 'work', 'workflow','write']\n",
    "\n",
    "PARAMS_1={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    'objective': 'tweedie','metric': 'tweedie',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "   'tweedie_variance_power': 1.349969119190657, 'max_bin': 212, 'subsample': 0.5774043241504451, 'subsample_freq': 0.7045972939301558, \n",
    "    'learning_rate': 0.16528226095247364, 'num_leaves': 4, 'feature_fraction': 0.9964784224971625,\n",
    "    'bagging_freq': 6, 'min_child_samples': 23, 'lambda_l1': 0.016924825494747078, 'lambda_l2': 0.0008031532180312293\n",
    "}\n",
    "\n",
    "\n",
    "PARAMS_2={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    'objective': 'tweedie','metric': 'tweedie',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "    'tweedie_variance_power': 1.3014991003823067, 'max_bin': 134, 'subsample': 0.8990859498726816, 'subsample_freq': 0.5274951186330312,\n",
    "    'learning_rate': 0.3937162652059595, 'num_leaves': 5, 'feature_fraction': 0.8861294810479933, 'bagging_freq': 5,\n",
    "    'min_child_samples': 28, 'lambda_l1': 6.037171725930821, 'lambda_l2': 0.0025254105473444784\n",
    "}\n",
    "\n",
    "PARAMS_3={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    #'objective': 'tweedie','metric': 'tweedie',\n",
    "     \n",
    "    'objective': 'xentropy','metric': 'xentropy',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "    'max_bin': 50, 'subsample': 0.8509082362331666, 'subsample_freq': 0.6958806976511948, 'learning_rate': 0.09406169926162017,\n",
    "    'num_leaves': 7, 'feature_fraction': 0.7562554580497556, 'bagging_freq': 4, 'min_child_samples': 5, 'lambda_l1': 0.00021420978217365439,\n",
    "    'lambda_l2': 0.011867471326820044\n",
    "}\n",
    "\n",
    "PARAMS_4={\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    #'objective': 'multiclass','metric': 'multiclass', 'num_class':4,\n",
    "    \n",
    "    'objective': 'tweedie','metric': 'tweedie',\n",
    "    \n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,'verbose': -1,'random_state':2020,\n",
    "    \n",
    "    'tweedie_variance_power': 1.3572492826220748, 'max_bin': 169, 'subsample': 0.6874225607452877, 'subsample_freq': 0.5369168449326642,\n",
    "    'learning_rate': 0.0353671206084155, 'num_leaves': 8, 'feature_fraction': 0.9508830019260512, \n",
    "    'bagging_freq': 2, 'min_child_samples': 63, 'lambda_l1': 8.281467382972142, 'lambda_l2': 0.1428656656583413\n",
    "}\n",
    "\n",
    "param_list = [PARAMS_1, PARAMS_2, PARAMS_3, PARAMS_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobflag</th>\n",
       "      <th>id</th>\n",
       "      <th>bert_pred_1_1</th>\n",
       "      <th>bert_pred_2_1</th>\n",
       "      <th>bert_pred_3_1</th>\n",
       "      <th>bert_pred_4_1</th>\n",
       "      <th>bert_pred_5_1</th>\n",
       "      <th>bert_pred_1_2</th>\n",
       "      <th>bert_pred_2_2</th>\n",
       "      <th>bert_pred_3_2</th>\n",
       "      <th>...</th>\n",
       "      <th>xrd</th>\n",
       "      <th>yarn</th>\n",
       "      <th>year</th>\n",
       "      <th>yearli</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yet</th>\n",
       "      <th>yield</th>\n",
       "      <th>younger</th>\n",
       "      <th>zeiss</th>\n",
       "      <th>zookeep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524498</td>\n",
       "      <td>0.257973</td>\n",
       "      <td>0.144118</td>\n",
       "      <td>0.133779</td>\n",
       "      <td>0.076740</td>\n",
       "      <td>0.484906</td>\n",
       "      <td>0.294941</td>\n",
       "      <td>0.220875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.488020</td>\n",
       "      <td>0.272449</td>\n",
       "      <td>0.144516</td>\n",
       "      <td>0.144751</td>\n",
       "      <td>0.080113</td>\n",
       "      <td>0.499350</td>\n",
       "      <td>0.503633</td>\n",
       "      <td>0.270341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.467154</td>\n",
       "      <td>0.275015</td>\n",
       "      <td>0.178735</td>\n",
       "      <td>0.249429</td>\n",
       "      <td>0.163889</td>\n",
       "      <td>0.461666</td>\n",
       "      <td>0.266647</td>\n",
       "      <td>0.198963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.512362</td>\n",
       "      <td>0.530206</td>\n",
       "      <td>0.703093</td>\n",
       "      <td>0.771459</td>\n",
       "      <td>0.770495</td>\n",
       "      <td>0.344096</td>\n",
       "      <td>0.296803</td>\n",
       "      <td>0.208545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.454002</td>\n",
       "      <td>0.380196</td>\n",
       "      <td>0.209382</td>\n",
       "      <td>0.296949</td>\n",
       "      <td>0.224078</td>\n",
       "      <td>0.474927</td>\n",
       "      <td>0.471786</td>\n",
       "      <td>0.417289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3405 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   jobflag  id  bert_pred_1_1  bert_pred_2_1  bert_pred_3_1  bert_pred_4_1  \\\n",
       "0      2.0   0       0.524498       0.257973       0.144118       0.133779   \n",
       "1      3.0   1       0.488020       0.272449       0.144516       0.144751   \n",
       "2      4.0   2       0.467154       0.275015       0.178735       0.249429   \n",
       "3      1.0   3       0.512362       0.530206       0.703093       0.771459   \n",
       "4      4.0   4       0.454002       0.380196       0.209382       0.296949   \n",
       "\n",
       "   bert_pred_5_1  bert_pred_1_2  bert_pred_2_2  bert_pred_3_2   ...     xrd  \\\n",
       "0       0.076740       0.484906       0.294941       0.220875   ...     0.0   \n",
       "1       0.080113       0.499350       0.503633       0.270341   ...     0.0   \n",
       "2       0.163889       0.461666       0.266647       0.198963   ...     0.0   \n",
       "3       0.770495       0.344096       0.296803       0.208545   ...     0.0   \n",
       "4       0.224078       0.474927       0.471786       0.417289   ...     0.0   \n",
       "\n",
       "   yarn  year  yearli  yellow  yet  yield  younger  zeiss  zookeep  \n",
       "0   0.0   0.0     0.0     0.0  0.0    0.0      0.0    0.0      0.0  \n",
       "1   0.0   0.0     0.0     0.0  0.0    0.0      0.0    0.0      0.0  \n",
       "2   0.0   0.0     0.0     0.0  0.0    0.0      0.0    0.0      0.0  \n",
       "3   0.0   0.0     0.0     0.0  0.0    0.0      0.0    0.0      0.0  \n",
       "4   0.0   0.0     0.0     0.0  0.0    0.0      0.0    0.0      0.0  \n",
       "\n",
       "[5 rows x 3405 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bert_pred_1_1</th>\n",
       "      <th>bert_pred_2_1</th>\n",
       "      <th>bert_pred_3_1</th>\n",
       "      <th>bert_pred_4_1</th>\n",
       "      <th>bert_pred_5_1</th>\n",
       "      <th>bert_pred_1_2</th>\n",
       "      <th>bert_pred_2_2</th>\n",
       "      <th>bert_pred_3_2</th>\n",
       "      <th>bert_pred_4_2</th>\n",
       "      <th>...</th>\n",
       "      <th>xrd</th>\n",
       "      <th>yarn</th>\n",
       "      <th>year</th>\n",
       "      <th>yearli</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yet</th>\n",
       "      <th>yield</th>\n",
       "      <th>younger</th>\n",
       "      <th>zeiss</th>\n",
       "      <th>zookeep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2931</td>\n",
       "      <td>0.304999</td>\n",
       "      <td>0.261890</td>\n",
       "      <td>0.256754</td>\n",
       "      <td>0.299635</td>\n",
       "      <td>0.292701</td>\n",
       "      <td>0.487088</td>\n",
       "      <td>0.482487</td>\n",
       "      <td>0.499640</td>\n",
       "      <td>0.490313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2932</td>\n",
       "      <td>0.157685</td>\n",
       "      <td>0.186549</td>\n",
       "      <td>0.179390</td>\n",
       "      <td>0.162669</td>\n",
       "      <td>0.161642</td>\n",
       "      <td>0.204091</td>\n",
       "      <td>0.194912</td>\n",
       "      <td>0.183070</td>\n",
       "      <td>0.193481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2933</td>\n",
       "      <td>0.109045</td>\n",
       "      <td>0.131893</td>\n",
       "      <td>0.112129</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>0.127725</td>\n",
       "      <td>0.169313</td>\n",
       "      <td>0.180290</td>\n",
       "      <td>0.184457</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2934</td>\n",
       "      <td>0.914908</td>\n",
       "      <td>0.937084</td>\n",
       "      <td>0.923681</td>\n",
       "      <td>0.940199</td>\n",
       "      <td>0.922476</td>\n",
       "      <td>0.194414</td>\n",
       "      <td>0.175685</td>\n",
       "      <td>0.217253</td>\n",
       "      <td>0.189669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2935</td>\n",
       "      <td>0.129471</td>\n",
       "      <td>0.158606</td>\n",
       "      <td>0.149187</td>\n",
       "      <td>0.163271</td>\n",
       "      <td>0.166768</td>\n",
       "      <td>0.138063</td>\n",
       "      <td>0.127007</td>\n",
       "      <td>0.146903</td>\n",
       "      <td>0.132465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3404 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  bert_pred_1_1  bert_pred_2_1  bert_pred_3_1  bert_pred_4_1  \\\n",
       "0  2931       0.304999       0.261890       0.256754       0.299635   \n",
       "1  2932       0.157685       0.186549       0.179390       0.162669   \n",
       "2  2933       0.109045       0.131893       0.112129       0.123854   \n",
       "3  2934       0.914908       0.937084       0.923681       0.940199   \n",
       "4  2935       0.129471       0.158606       0.149187       0.163271   \n",
       "\n",
       "   bert_pred_5_1  bert_pred_1_2  bert_pred_2_2  bert_pred_3_2  bert_pred_4_2  \\\n",
       "0       0.292701       0.487088       0.482487       0.499640       0.490313   \n",
       "1       0.161642       0.204091       0.194912       0.183070       0.193481   \n",
       "2       0.127725       0.169313       0.180290       0.184457       0.186450   \n",
       "3       0.922476       0.194414       0.175685       0.217253       0.189669   \n",
       "4       0.166768       0.138063       0.127007       0.146903       0.132465   \n",
       "\n",
       "    ...     xrd  yarn  year  yearli  yellow  yet  yield  younger  zeiss  \\\n",
       "0   ...     0.0   0.0   0.0     0.0     0.0  0.0    0.0      0.0    0.0   \n",
       "1   ...     0.0   0.0   0.0     0.0     0.0  0.0    0.0      0.0    0.0   \n",
       "2   ...     0.0   0.0   0.0     0.0     0.0  0.0    0.0      0.0    0.0   \n",
       "3   ...     0.0   0.0   0.0     0.0     0.0  0.0    0.0      0.0    0.0   \n",
       "4   ...     0.0   0.0   0.0     0.0     0.0  0.0    0.0      0.0    0.0   \n",
       "\n",
       "   zookeep  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "\n",
       "[5 rows x 3404 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_offdf(train_df, test_df, feature, params_list):\n",
    "    k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "    \n",
    "    y_1 = train_df.jobflag.apply(lambda x: 1 if x==1 else 0)\n",
    "    y_2 = train_df.jobflag.apply(lambda x: 1 if x==2 else 0)\n",
    "    y_3 = train_df.jobflag.apply(lambda x: 1 if x==3 else 0)\n",
    "    y_4 = train_df.jobflag.apply(lambda x: 1 if x==4 else 0)\n",
    "    \n",
    "    off_df = []\n",
    "    for i in range(4):\n",
    "        test_df[f'lgb_preds_{i+1}']=0\n",
    "    \n",
    "    for trn, val in k.split(train_df, train_df.jobflag):\n",
    "        train_X, val_X = train_df.iloc[trn,:][feature], train_df.iloc[val,:][feature]\n",
    "        tmp_off_df = train_df.iloc[val,:]\n",
    "        c=1\n",
    "        for y, param in zip([y_1, y_2, y_3, y_4], params_list):\n",
    "            tmp_off_df[f'lgb_preds_{c}']=0\n",
    "            for _ in range(5):\n",
    "                train_y, val_y = y.iloc[trn], y.iloc[val]\n",
    "                train_set= lgb.Dataset(train_X,  train_y)\n",
    "                val_set = lgb.Dataset(val_X,  val_y)   \n",
    "\n",
    "                model = lgb.train(\n",
    "                    train_set=train_set, valid_sets=[train_set, val_set], params=param, num_boost_round=3000, \n",
    "                    early_stopping_rounds=200, verbose_eval=500\n",
    "                )\n",
    "                tmp_off_df[f'lgb_preds_{c}'] += model.predict(val_X)/5\n",
    "                param['random_state']+=1\n",
    "                \n",
    "                test_df[f'lgb_preds_{c}'] += model.predict(test_df[feature])/5\n",
    "                \n",
    "            c+=1\n",
    "        \n",
    "        off_df.append(tmp_off_df)\n",
    "    \n",
    "    for i in range(4):\n",
    "        test_df[f'lgb_preds_{i+1}']/=5\n",
    "    \n",
    "    off_df = pd.concat(off_df, axis=0)\n",
    "    return off_df.reset_index(drop=True), test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[84]\ttraining's tweedie: 1.3915\tvalid_1's tweedie: 1.44308\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[101]\ttraining's tweedie: 1.37837\tvalid_1's tweedie: 1.44034\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttraining's tweedie: 1.4117\tvalid_1's tweedie: 1.42641\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's tweedie: 1.41501\tvalid_1's tweedie: 1.43065\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\ttraining's tweedie: 1.40503\tvalid_1's tweedie: 1.41303\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's tweedie: 0.954497\tvalid_1's tweedie: 0.989485\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's tweedie: 0.906892\tvalid_1's tweedie: 0.989645\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's tweedie: 0.926546\tvalid_1's tweedie: 0.992464\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttraining's tweedie: 0.946567\tvalid_1's tweedie: 0.993885\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttraining's tweedie: 0.93126\tvalid_1's tweedie: 0.991291\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[153]\ttraining's cross_entropy: 0.375734\tvalid_1's cross_entropy: 0.489106\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[216]\ttraining's cross_entropy: 0.335766\tvalid_1's cross_entropy: 0.493043\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[151]\ttraining's cross_entropy: 0.379124\tvalid_1's cross_entropy: 0.497446\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[202]\ttraining's cross_entropy: 0.345359\tvalid_1's cross_entropy: 0.497692\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[218]\ttraining's cross_entropy: 0.337247\tvalid_1's cross_entropy: 0.489137\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36563\tvalid_1's tweedie: 1.40858\n",
      "Early stopping, best iteration is:\n",
      "[510]\ttraining's tweedie: 1.36489\tvalid_1's tweedie: 1.40847\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36529\tvalid_1's tweedie: 1.41115\n",
      "Early stopping, best iteration is:\n",
      "[398]\ttraining's tweedie: 1.3717\tvalid_1's tweedie: 1.41024\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36662\tvalid_1's tweedie: 1.40916\n",
      "Early stopping, best iteration is:\n",
      "[510]\ttraining's tweedie: 1.36616\tvalid_1's tweedie: 1.40873\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[278]\ttraining's tweedie: 1.38327\tvalid_1's tweedie: 1.40778\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36499\tvalid_1's tweedie: 1.40684\n",
      "Early stopping, best iteration is:\n",
      "[476]\ttraining's tweedie: 1.36637\tvalid_1's tweedie: 1.40634\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's tweedie: 1.44816\tvalid_1's tweedie: 1.51669\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttraining's tweedie: 1.42802\tvalid_1's tweedie: 1.5026\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's tweedie: 1.43188\tvalid_1's tweedie: 1.51174\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's tweedie: 1.44755\tvalid_1's tweedie: 1.50916\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's tweedie: 1.43256\tvalid_1's tweedie: 1.50084\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[50]\ttraining's tweedie: 0.907671\tvalid_1's tweedie: 0.969714\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttraining's tweedie: 0.920394\tvalid_1's tweedie: 0.979447\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's tweedie: 0.92517\tvalid_1's tweedie: 0.966811\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's tweedie: 0.912285\tvalid_1's tweedie: 0.966173\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[104]\ttraining's tweedie: 0.886666\tvalid_1's tweedie: 0.976487\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's cross_entropy: 0.355856\tvalid_1's cross_entropy: 0.515919\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[236]\ttraining's cross_entropy: 0.320221\tvalid_1's cross_entropy: 0.51277\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[184]\ttraining's cross_entropy: 0.349184\tvalid_1's cross_entropy: 0.517938\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[214]\ttraining's cross_entropy: 0.332458\tvalid_1's cross_entropy: 0.509773\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[243]\ttraining's cross_entropy: 0.317679\tvalid_1's cross_entropy: 0.511572\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36613\tvalid_1's tweedie: 1.42977\n",
      "[1000]\ttraining's tweedie: 1.34632\tvalid_1's tweedie: 1.4268\n",
      "Early stopping, best iteration is:\n",
      "[984]\ttraining's tweedie: 1.34652\tvalid_1's tweedie: 1.42662\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36731\tvalid_1's tweedie: 1.43001\n",
      "[1000]\ttraining's tweedie: 1.34641\tvalid_1's tweedie: 1.42573\n",
      "Early stopping, best iteration is:\n",
      "[1087]\ttraining's tweedie: 1.34373\tvalid_1's tweedie: 1.42522\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36705\tvalid_1's tweedie: 1.42836\n",
      "Early stopping, best iteration is:\n",
      "[458]\ttraining's tweedie: 1.36933\tvalid_1's tweedie: 1.42682\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36609\tvalid_1's tweedie: 1.43246\n",
      "[1000]\ttraining's tweedie: 1.34556\tvalid_1's tweedie: 1.43124\n",
      "Early stopping, best iteration is:\n",
      "[810]\ttraining's tweedie: 1.3525\tvalid_1's tweedie: 1.4299\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36759\tvalid_1's tweedie: 1.4318\n",
      "Early stopping, best iteration is:\n",
      "[372]\ttraining's tweedie: 1.375\tvalid_1's tweedie: 1.43008\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's tweedie: 1.40214\tvalid_1's tweedie: 1.45838\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's tweedie: 1.42068\tvalid_1's tweedie: 1.46447\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's tweedie: 1.44903\tvalid_1's tweedie: 1.48555\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's tweedie: 1.41802\tvalid_1's tweedie: 1.45432\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttraining's tweedie: 1.41786\tvalid_1's tweedie: 1.47044\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\ttraining's tweedie: 0.90353\tvalid_1's tweedie: 0.95445\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39]\ttraining's tweedie: 0.920228\tvalid_1's tweedie: 0.954898\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's tweedie: 0.90777\tvalid_1's tweedie: 0.960163\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttraining's tweedie: 0.905738\tvalid_1's tweedie: 0.965471\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's tweedie: 0.906539\tvalid_1's tweedie: 0.958776\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[141]\ttraining's cross_entropy: 0.385534\tvalid_1's cross_entropy: 0.482424\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[165]\ttraining's cross_entropy: 0.367881\tvalid_1's cross_entropy: 0.484035\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[219]\ttraining's cross_entropy: 0.334143\tvalid_1's cross_entropy: 0.483715\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's cross_entropy: 0.315417\tvalid_1's cross_entropy: 0.479259\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[175]\ttraining's cross_entropy: 0.362348\tvalid_1's cross_entropy: 0.486145\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[238]\ttraining's tweedie: 1.38523\tvalid_1's tweedie: 1.42935\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36188\tvalid_1's tweedie: 1.42906\n",
      "Early stopping, best iteration is:\n",
      "[460]\ttraining's tweedie: 1.36443\tvalid_1's tweedie: 1.42773\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36054\tvalid_1's tweedie: 1.43066\n",
      "Early stopping, best iteration is:\n",
      "[369]\ttraining's tweedie: 1.36859\tvalid_1's tweedie: 1.42754\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[280]\ttraining's tweedie: 1.37903\tvalid_1's tweedie: 1.42911\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36136\tvalid_1's tweedie: 1.43085\n",
      "Early stopping, best iteration is:\n",
      "[398]\ttraining's tweedie: 1.36878\tvalid_1's tweedie: 1.42996\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttraining's tweedie: 1.43171\tvalid_1's tweedie: 1.44912\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttraining's tweedie: 1.43428\tvalid_1's tweedie: 1.45545\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttraining's tweedie: 1.44694\tvalid_1's tweedie: 1.45794\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's tweedie: 1.42103\tvalid_1's tweedie: 1.44836\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's tweedie: 1.43901\tvalid_1's tweedie: 1.44958\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's tweedie: 0.936249\tvalid_1's tweedie: 0.998924\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's tweedie: 0.945759\tvalid_1's tweedie: 1.00044\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's tweedie: 0.930239\tvalid_1's tweedie: 0.992331\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's tweedie: 0.971333\tvalid_1's tweedie: 1.0072\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's tweedie: 0.932964\tvalid_1's tweedie: 0.995046\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[146]\ttraining's cross_entropy: 0.377687\tvalid_1's cross_entropy: 0.519178\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[149]\ttraining's cross_entropy: 0.372087\tvalid_1's cross_entropy: 0.523578\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[174]\ttraining's cross_entropy: 0.354579\tvalid_1's cross_entropy: 0.515332\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[148]\ttraining's cross_entropy: 0.373829\tvalid_1's cross_entropy: 0.524151\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[128]\ttraining's cross_entropy: 0.387693\tvalid_1's cross_entropy: 0.5219\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36163\tvalid_1's tweedie: 1.42927\n",
      "[1000]\ttraining's tweedie: 1.34101\tvalid_1's tweedie: 1.42862\n",
      "Early stopping, best iteration is:\n",
      "[913]\ttraining's tweedie: 1.34402\tvalid_1's tweedie: 1.42636\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36146\tvalid_1's tweedie: 1.43443\n",
      "[1000]\ttraining's tweedie: 1.34149\tvalid_1's tweedie: 1.43089\n",
      "Early stopping, best iteration is:\n",
      "[1176]\ttraining's tweedie: 1.33636\tvalid_1's tweedie: 1.42914\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36236\tvalid_1's tweedie: 1.4348\n",
      "Early stopping, best iteration is:\n",
      "[628]\ttraining's tweedie: 1.35628\tvalid_1's tweedie: 1.43209\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36237\tvalid_1's tweedie: 1.43271\n",
      "Early stopping, best iteration is:\n",
      "[712]\ttraining's tweedie: 1.35174\tvalid_1's tweedie: 1.43052\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36199\tvalid_1's tweedie: 1.4305\n",
      "[1000]\ttraining's tweedie: 1.34056\tvalid_1's tweedie: 1.42924\n",
      "Early stopping, best iteration is:\n",
      "[846]\ttraining's tweedie: 1.34532\tvalid_1's tweedie: 1.42808\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttraining's tweedie: 1.43586\tvalid_1's tweedie: 1.45181\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttraining's tweedie: 1.4012\tvalid_1's tweedie: 1.4419\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\ttraining's tweedie: 1.4004\tvalid_1's tweedie: 1.45311\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's tweedie: 1.38521\tvalid_1's tweedie: 1.44446\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's tweedie: 1.41299\tvalid_1's tweedie: 1.44044\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's tweedie: 0.957284\tvalid_1's tweedie: 0.987873\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttraining's tweedie: 0.96919\tvalid_1's tweedie: 0.993254\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's tweedie: 0.917844\tvalid_1's tweedie: 0.993949\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's tweedie: 0.954265\tvalid_1's tweedie: 0.993386\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's tweedie: 0.954831\tvalid_1's tweedie: 0.999032\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's cross_entropy: 0.363696\tvalid_1's cross_entropy: 0.490517\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's cross_entropy: 0.387103\tvalid_1's cross_entropy: 0.498477\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\ttraining's cross_entropy: 0.341806\tvalid_1's cross_entropy: 0.491521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\ttraining's cross_entropy: 0.398134\tvalid_1's cross_entropy: 0.495323\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[194]\ttraining's cross_entropy: 0.34687\tvalid_1's cross_entropy: 0.495964\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36791\tvalid_1's tweedie: 1.40598\n",
      "[1000]\ttraining's tweedie: 1.34605\tvalid_1's tweedie: 1.40194\n",
      "Early stopping, best iteration is:\n",
      "[1078]\ttraining's tweedie: 1.34356\tvalid_1's tweedie: 1.40109\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.369\tvalid_1's tweedie: 1.40418\n",
      "[1000]\ttraining's tweedie: 1.34769\tvalid_1's tweedie: 1.39929\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's tweedie: 1.3348\tvalid_1's tweedie: 1.39814\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36804\tvalid_1's tweedie: 1.40382\n",
      "[1000]\ttraining's tweedie: 1.34711\tvalid_1's tweedie: 1.39996\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1372]\ttraining's tweedie: 1.3359\tvalid_1's tweedie: 1.39836\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.3671\tvalid_1's tweedie: 1.40342\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttraining's tweedie: 1.35963\tvalid_1's tweedie: 1.40178\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's tweedie: 1.36766\tvalid_1's tweedie: 1.40181\n",
      "Early stopping, best iteration is:\n",
      "[664]\ttraining's tweedie: 1.35938\tvalid_1's tweedie: 1.39872\n"
     ]
    }
   ],
   "source": [
    "off_df, test_df2 = make_offdf(train_df, test_df, feature, param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb_preds_1</th>\n",
       "      <th>lgb_preds_2</th>\n",
       "      <th>lgb_preds_3</th>\n",
       "      <th>lgb_preds_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.525360</td>\n",
       "      <td>0.176473</td>\n",
       "      <td>-0.443558</td>\n",
       "      <td>-0.160037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.125056</td>\n",
       "      <td>0.366627</td>\n",
       "      <td>-0.207003</td>\n",
       "      <td>-0.169210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.404214</td>\n",
       "      <td>-0.241977</td>\n",
       "      <td>0.586404</td>\n",
       "      <td>-0.096219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.135513</td>\n",
       "      <td>-0.173602</td>\n",
       "      <td>-0.109832</td>\n",
       "      <td>0.419984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lgb_preds_1  lgb_preds_2  lgb_preds_3  lgb_preds_4\n",
       "1     0.525360     0.176473    -0.443558    -0.160037\n",
       "2     0.125056     0.366627    -0.207003    -0.169210\n",
       "3    -0.404214    -0.241977     0.586404    -0.096219\n",
       "4    -0.135513    -0.173602    -0.109832     0.419984"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "    pd.get_dummies(off_df.jobflag)[[1,2,3,4]],\n",
    "    off_df[[ 'lgb_preds_1', 'lgb_preds_2', 'lgb_preds_3', 'lgb_preds_4']]\n",
    "], axis=1).corr().loc[[1,2,3,4], [ 'lgb_preds_1', 'lgb_preds_2', 'lgb_preds_3', 'lgb_preds_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobflag</th>\n",
       "      <th>id</th>\n",
       "      <th>bert_pred_1_1</th>\n",
       "      <th>bert_pred_2_1</th>\n",
       "      <th>bert_pred_3_1</th>\n",
       "      <th>bert_pred_4_1</th>\n",
       "      <th>bert_pred_5_1</th>\n",
       "      <th>bert_pred_1_2</th>\n",
       "      <th>bert_pred_2_2</th>\n",
       "      <th>bert_pred_3_2</th>\n",
       "      <th>...</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yet</th>\n",
       "      <th>yield</th>\n",
       "      <th>younger</th>\n",
       "      <th>zeiss</th>\n",
       "      <th>zookeep</th>\n",
       "      <th>lgb_preds_1</th>\n",
       "      <th>lgb_preds_2</th>\n",
       "      <th>lgb_preds_3</th>\n",
       "      <th>lgb_preds_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524498</td>\n",
       "      <td>0.257973</td>\n",
       "      <td>0.144118</td>\n",
       "      <td>0.133779</td>\n",
       "      <td>0.076740</td>\n",
       "      <td>0.484906</td>\n",
       "      <td>0.294941</td>\n",
       "      <td>0.220875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124375</td>\n",
       "      <td>0.046654</td>\n",
       "      <td>0.776604</td>\n",
       "      <td>0.049806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.578211</td>\n",
       "      <td>0.409709</td>\n",
       "      <td>0.237590</td>\n",
       "      <td>0.349191</td>\n",
       "      <td>0.117446</td>\n",
       "      <td>0.445859</td>\n",
       "      <td>0.238172</td>\n",
       "      <td>0.217901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119715</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>0.906843</td>\n",
       "      <td>0.025833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.394083</td>\n",
       "      <td>0.252006</td>\n",
       "      <td>0.214029</td>\n",
       "      <td>0.169860</td>\n",
       "      <td>0.175528</td>\n",
       "      <td>0.664439</td>\n",
       "      <td>0.778905</td>\n",
       "      <td>0.830853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118658</td>\n",
       "      <td>0.069503</td>\n",
       "      <td>0.623205</td>\n",
       "      <td>0.242803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.450969</td>\n",
       "      <td>0.387301</td>\n",
       "      <td>0.247143</td>\n",
       "      <td>0.448752</td>\n",
       "      <td>0.165567</td>\n",
       "      <td>0.498336</td>\n",
       "      <td>0.543906</td>\n",
       "      <td>0.383343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145846</td>\n",
       "      <td>0.090250</td>\n",
       "      <td>0.390933</td>\n",
       "      <td>0.200563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.426481</td>\n",
       "      <td>0.272545</td>\n",
       "      <td>0.192479</td>\n",
       "      <td>0.184106</td>\n",
       "      <td>0.098343</td>\n",
       "      <td>0.473353</td>\n",
       "      <td>0.459011</td>\n",
       "      <td>0.220175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098323</td>\n",
       "      <td>0.045169</td>\n",
       "      <td>0.565293</td>\n",
       "      <td>0.142971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3409 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   jobflag  id  bert_pred_1_1  bert_pred_2_1  bert_pred_3_1  bert_pred_4_1  \\\n",
       "0      2.0   0       0.524498       0.257973       0.144118       0.133779   \n",
       "1      3.0   9       0.578211       0.409709       0.237590       0.349191   \n",
       "2      3.0  17       0.394083       0.252006       0.214029       0.169860   \n",
       "3      3.0  18       0.450969       0.387301       0.247143       0.448752   \n",
       "4      3.0  20       0.426481       0.272545       0.192479       0.184106   \n",
       "\n",
       "   bert_pred_5_1  bert_pred_1_2  bert_pred_2_2  bert_pred_3_2     ...       \\\n",
       "0       0.076740       0.484906       0.294941       0.220875     ...        \n",
       "1       0.117446       0.445859       0.238172       0.217901     ...        \n",
       "2       0.175528       0.664439       0.778905       0.830853     ...        \n",
       "3       0.165567       0.498336       0.543906       0.383343     ...        \n",
       "4       0.098343       0.473353       0.459011       0.220175     ...        \n",
       "\n",
       "   yellow  yet  yield  younger  zeiss  zookeep  lgb_preds_1  lgb_preds_2  \\\n",
       "0     0.0  0.0    0.0      0.0    0.0      0.0     0.124375     0.046654   \n",
       "1     0.0  0.0    0.0      0.0    0.0      0.0     0.119715     0.038722   \n",
       "2     0.0  0.0    0.0      0.0    0.0      0.0     0.118658     0.069503   \n",
       "3     0.0  0.0    0.0      0.0    0.0      0.0     0.145846     0.090250   \n",
       "4     0.0  0.0    0.0      0.0    0.0      0.0     0.098323     0.045169   \n",
       "\n",
       "   lgb_preds_3  lgb_preds_4  \n",
       "0     0.776604     0.049806  \n",
       "1     0.906843     0.025833  \n",
       "2     0.623205     0.242803  \n",
       "3     0.390933     0.200563  \n",
       "4     0.565293     0.142971  \n",
       "\n",
       "[5 rows x 3409 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bert_pred_1_1</th>\n",
       "      <th>bert_pred_2_1</th>\n",
       "      <th>bert_pred_3_1</th>\n",
       "      <th>bert_pred_4_1</th>\n",
       "      <th>bert_pred_5_1</th>\n",
       "      <th>bert_pred_1_2</th>\n",
       "      <th>bert_pred_2_2</th>\n",
       "      <th>bert_pred_3_2</th>\n",
       "      <th>bert_pred_4_2</th>\n",
       "      <th>...</th>\n",
       "      <th>zeiss</th>\n",
       "      <th>zookeep</th>\n",
       "      <th>preds_1</th>\n",
       "      <th>preds_2</th>\n",
       "      <th>preds_3</th>\n",
       "      <th>preds_4</th>\n",
       "      <th>lgb_preds_1</th>\n",
       "      <th>lgb_preds_2</th>\n",
       "      <th>lgb_preds_3</th>\n",
       "      <th>lgb_preds_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2931</td>\n",
       "      <td>0.304999</td>\n",
       "      <td>0.261890</td>\n",
       "      <td>0.256754</td>\n",
       "      <td>0.299635</td>\n",
       "      <td>0.292701</td>\n",
       "      <td>0.487088</td>\n",
       "      <td>0.482487</td>\n",
       "      <td>0.499640</td>\n",
       "      <td>0.490313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286953</td>\n",
       "      <td>0.300346</td>\n",
       "      <td>1.255970</td>\n",
       "      <td>0.246627</td>\n",
       "      <td>0.146160</td>\n",
       "      <td>0.173406</td>\n",
       "      <td>0.528802</td>\n",
       "      <td>0.161975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2932</td>\n",
       "      <td>0.157685</td>\n",
       "      <td>0.186549</td>\n",
       "      <td>0.179390</td>\n",
       "      <td>0.162669</td>\n",
       "      <td>0.161642</td>\n",
       "      <td>0.204091</td>\n",
       "      <td>0.194912</td>\n",
       "      <td>0.183070</td>\n",
       "      <td>0.193481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160164</td>\n",
       "      <td>0.229296</td>\n",
       "      <td>1.148036</td>\n",
       "      <td>0.251646</td>\n",
       "      <td>0.063599</td>\n",
       "      <td>0.112505</td>\n",
       "      <td>0.616123</td>\n",
       "      <td>0.151675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2933</td>\n",
       "      <td>0.109045</td>\n",
       "      <td>0.131893</td>\n",
       "      <td>0.112129</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>0.127725</td>\n",
       "      <td>0.169313</td>\n",
       "      <td>0.180290</td>\n",
       "      <td>0.184457</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.301584</td>\n",
       "      <td>0.134988</td>\n",
       "      <td>0.992523</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.152356</td>\n",
       "      <td>0.077784</td>\n",
       "      <td>0.506150</td>\n",
       "      <td>0.182886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2934</td>\n",
       "      <td>0.914908</td>\n",
       "      <td>0.937084</td>\n",
       "      <td>0.923681</td>\n",
       "      <td>0.940199</td>\n",
       "      <td>0.922476</td>\n",
       "      <td>0.194414</td>\n",
       "      <td>0.175685</td>\n",
       "      <td>0.217253</td>\n",
       "      <td>0.189669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.596967</td>\n",
       "      <td>0.167668</td>\n",
       "      <td>0.189846</td>\n",
       "      <td>0.133128</td>\n",
       "      <td>0.766366</td>\n",
       "      <td>0.098118</td>\n",
       "      <td>0.095914</td>\n",
       "      <td>0.079734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2935</td>\n",
       "      <td>0.129471</td>\n",
       "      <td>0.158606</td>\n",
       "      <td>0.149187</td>\n",
       "      <td>0.163271</td>\n",
       "      <td>0.166768</td>\n",
       "      <td>0.138063</td>\n",
       "      <td>0.127007</td>\n",
       "      <td>0.146903</td>\n",
       "      <td>0.132465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219979</td>\n",
       "      <td>0.060010</td>\n",
       "      <td>1.269596</td>\n",
       "      <td>0.909977</td>\n",
       "      <td>0.094777</td>\n",
       "      <td>0.034124</td>\n",
       "      <td>0.636871</td>\n",
       "      <td>0.592658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3412 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  bert_pred_1_1  bert_pred_2_1  bert_pred_3_1  bert_pred_4_1  \\\n",
       "0  2931       0.304999       0.261890       0.256754       0.299635   \n",
       "1  2932       0.157685       0.186549       0.179390       0.162669   \n",
       "2  2933       0.109045       0.131893       0.112129       0.123854   \n",
       "3  2934       0.914908       0.937084       0.923681       0.940199   \n",
       "4  2935       0.129471       0.158606       0.149187       0.163271   \n",
       "\n",
       "   bert_pred_5_1  bert_pred_1_2  bert_pred_2_2  bert_pred_3_2  bert_pred_4_2  \\\n",
       "0       0.292701       0.487088       0.482487       0.499640       0.490313   \n",
       "1       0.161642       0.204091       0.194912       0.183070       0.193481   \n",
       "2       0.127725       0.169313       0.180290       0.184457       0.186450   \n",
       "3       0.922476       0.194414       0.175685       0.217253       0.189669   \n",
       "4       0.166768       0.138063       0.127007       0.146903       0.132465   \n",
       "\n",
       "      ...       zeiss  zookeep   preds_1   preds_2   preds_3   preds_4  \\\n",
       "0     ...         0.0      0.0  0.286953  0.300346  1.255970  0.246627   \n",
       "1     ...         0.0      0.0  0.160164  0.229296  1.148036  0.251646   \n",
       "2     ...         0.0      0.0  0.301584  0.134988  0.992523  0.277500   \n",
       "3     ...         0.0      0.0  1.596967  0.167668  0.189846  0.133128   \n",
       "4     ...         0.0      0.0  0.219979  0.060010  1.269596  0.909977   \n",
       "\n",
       "   lgb_preds_1  lgb_preds_2  lgb_preds_3  lgb_preds_4  \n",
       "0     0.146160     0.173406     0.528802     0.161975  \n",
       "1     0.063599     0.112505     0.616123     0.151675  \n",
       "2     0.152356     0.077784     0.506150     0.182886  \n",
       "3     0.766366     0.098118     0.095914     0.079734  \n",
       "4     0.094777     0.034124     0.636871     0.592658  \n",
       "\n",
       "[5 rows x 3412 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4d70c12b764ffd8a44ff30b8d792d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d06df27c575479c9bd0face8255a79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7af8fcf04ed4265b38d99f59b912f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ddb50024924fd8a38c9944e5d5335f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d500d735fe44442ab083c36b60db649a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n",
    "off_df_2=[]\n",
    "test_preds = np.zeros(shape=(len(test_df2),4))\n",
    "bert_pred_cols =['bert_pred_1_1', 'bert_pred_2_1', 'bert_pred_3_1', 'bert_pred_4_1', 'bert_pred_5_1', 'bert_pred_1_2', 'bert_pred_2_2',\n",
    "       'bert_pred_3_2', 'bert_pred_4_2', 'bert_pred_5_2', 'bert_pred_1_3', 'bert_pred_2_3', 'bert_pred_3_3', 'bert_pred_4_3', \n",
    "       'bert_pred_5_3', 'bert_pred_1_4', 'bert_pred_2_4', 'bert_pred_3_4', 'bert_pred_4_4', 'bert_pred_5_4']\n",
    "for trn, val in k.split(train_df, train_df.jobflag):\n",
    "    trn_df = off_df.iloc[trn,:]\n",
    "    val_df  =  off_df.iloc[val,:]\n",
    "    \n",
    "    min_value = trn_df.jobflag.value_counts().min()\n",
    "    \n",
    "    preds = np.zeros(shape=(len(val_df),4))\n",
    "    \n",
    "    for i in tqdm(range(80)):\n",
    "        tmp_trn_df = pd.concat(\n",
    "        [trn_df[trn_df.jobflag==1].sample(n=min_value, random_state=i),\n",
    "         trn_df[trn_df.jobflag==2].sample(n=min_value, random_state=i),\n",
    "         trn_df[trn_df.jobflag==3].sample(n=min_value, random_state=i),\n",
    "         trn_df[trn_df.jobflag==4].sample(n=min_value, random_state=i)], axis=0).reset_index(drop=True)\n",
    "        tmp_trn_X = tmp_trn_df[bert_pred_cols+[ 'lgb_preds_1', 'lgb_preds_2', 'lgb_preds_3', 'lgb_preds_4']]\n",
    "        tmp_trn_y = tmp_trn_df['jobflag']\n",
    "        \n",
    "        \n",
    "        for penalty  in [ 'l2']:\n",
    "            for m in range(5):\n",
    "                logit = LogisticRegression(penalty=penalty, random_state=m)\n",
    "                logit.fit(tmp_trn_X, tmp_trn_y)\n",
    "\n",
    "                    #ridge_cls = RidgeClassifier()\n",
    "                    #ridge_cls.fit(tmp_trn_X, tmp_trn_y)\n",
    "\n",
    "                    #kncls = KNeighborsClassifier(n_neighbors=4)\n",
    "                    #kncls.fit(tmp_trn_X, tmp_trn_y)\n",
    "                preds += logit.predict_proba(val_df[bert_pred_cols+[ 'lgb_preds_1', 'lgb_preds_2', 'lgb_preds_3', 'lgb_preds_4']])\n",
    "                test_preds += logit.predict_proba(test_df2[bert_pred_cols+[ 'lgb_preds_1', 'lgb_preds_2', 'lgb_preds_3', 'lgb_preds_4']])\n",
    "                \n",
    "    val_df[f'preds'] = np.argmax(preds, axis=1)+1\n",
    "    off_df_2.append(val_df)\n",
    "\n",
    "test_df2[f'preds'] = np.argmax(test_preds, axis=1)+1\n",
    "off_df_2 = pd.concat(off_df_2, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.556777990628666\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>353</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>143</td>\n",
       "      <td>72</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91</td>\n",
       "      <td>119</td>\n",
       "      <td>873</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>27</td>\n",
       "      <td>129</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3\n",
       "0  353  128   64   75\n",
       "1   90  143   72   32\n",
       "2   91  119  873  268\n",
       "3   51   27  129  371"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(metrics.f1_score(off_df_2.jobflag, off_df_2.preds, average='macro'))\n",
    "plt.figure(figsize=(10,10))\n",
    "cnfn_matrix = pd.DataFrame(metrics.confusion_matrix(off_df_2.jobflag, off_df_2.preds))\n",
    "#cnfn_matrix.index = \n",
    "sns.heatmap(cnfn_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "cnfn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    586\n",
       "3    584\n",
       "1    378\n",
       "2    195\n",
       "Name: preds, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2.preds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../submit_sample.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[1] = test_df2.preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2931</td>\n",
       "      <td>4</td>\n",
       "      <td>2931</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2932</td>\n",
       "      <td>3</td>\n",
       "      <td>2932</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2933</td>\n",
       "      <td>3</td>\n",
       "      <td>2933</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2934</td>\n",
       "      <td>1</td>\n",
       "      <td>2934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2935</td>\n",
       "      <td>3</td>\n",
       "      <td>2935</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2936</td>\n",
       "      <td>3</td>\n",
       "      <td>2936</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2937</td>\n",
       "      <td>3</td>\n",
       "      <td>2937</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2938</td>\n",
       "      <td>3</td>\n",
       "      <td>2938</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2939</td>\n",
       "      <td>3</td>\n",
       "      <td>2939</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2940</td>\n",
       "      <td>4</td>\n",
       "      <td>2940</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2941</td>\n",
       "      <td>1</td>\n",
       "      <td>2941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2942</td>\n",
       "      <td>3</td>\n",
       "      <td>2942</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2943</td>\n",
       "      <td>3</td>\n",
       "      <td>2943</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2944</td>\n",
       "      <td>4</td>\n",
       "      <td>2944</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2945</td>\n",
       "      <td>1</td>\n",
       "      <td>2945</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2946</td>\n",
       "      <td>4</td>\n",
       "      <td>2946</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2947</td>\n",
       "      <td>4</td>\n",
       "      <td>2947</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2948</td>\n",
       "      <td>3</td>\n",
       "      <td>2948</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2949</td>\n",
       "      <td>3</td>\n",
       "      <td>2949</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2950</td>\n",
       "      <td>3</td>\n",
       "      <td>2950</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2951</td>\n",
       "      <td>3</td>\n",
       "      <td>2951</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2952</td>\n",
       "      <td>3</td>\n",
       "      <td>2952</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2953</td>\n",
       "      <td>3</td>\n",
       "      <td>2953</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2954</td>\n",
       "      <td>2</td>\n",
       "      <td>2954</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2955</td>\n",
       "      <td>2</td>\n",
       "      <td>2955</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2956</td>\n",
       "      <td>1</td>\n",
       "      <td>2956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2957</td>\n",
       "      <td>3</td>\n",
       "      <td>2957</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2958</td>\n",
       "      <td>3</td>\n",
       "      <td>2958</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2959</td>\n",
       "      <td>3</td>\n",
       "      <td>2959</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2960</td>\n",
       "      <td>3</td>\n",
       "      <td>2960</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>4644</td>\n",
       "      <td>1</td>\n",
       "      <td>4644</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>4645</td>\n",
       "      <td>4</td>\n",
       "      <td>4645</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>4646</td>\n",
       "      <td>4</td>\n",
       "      <td>4646</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>4647</td>\n",
       "      <td>2</td>\n",
       "      <td>4647</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>4648</td>\n",
       "      <td>1</td>\n",
       "      <td>4648</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>4649</td>\n",
       "      <td>1</td>\n",
       "      <td>4649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>4650</td>\n",
       "      <td>2</td>\n",
       "      <td>4650</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>4651</td>\n",
       "      <td>3</td>\n",
       "      <td>4651</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>4652</td>\n",
       "      <td>1</td>\n",
       "      <td>4652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>4653</td>\n",
       "      <td>3</td>\n",
       "      <td>4653</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>4654</td>\n",
       "      <td>1</td>\n",
       "      <td>4654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>4655</td>\n",
       "      <td>4</td>\n",
       "      <td>4655</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>4656</td>\n",
       "      <td>4</td>\n",
       "      <td>4656</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>4657</td>\n",
       "      <td>3</td>\n",
       "      <td>4657</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>4658</td>\n",
       "      <td>1</td>\n",
       "      <td>4658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>4659</td>\n",
       "      <td>3</td>\n",
       "      <td>4659</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>4660</td>\n",
       "      <td>1</td>\n",
       "      <td>4660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>4661</td>\n",
       "      <td>4</td>\n",
       "      <td>4661</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>4662</td>\n",
       "      <td>3</td>\n",
       "      <td>4662</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>4663</td>\n",
       "      <td>1</td>\n",
       "      <td>4663</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>4664</td>\n",
       "      <td>3</td>\n",
       "      <td>4664</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>4665</td>\n",
       "      <td>1</td>\n",
       "      <td>4665</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>4666</td>\n",
       "      <td>3</td>\n",
       "      <td>4666</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>4667</td>\n",
       "      <td>1</td>\n",
       "      <td>4667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>4668</td>\n",
       "      <td>1</td>\n",
       "      <td>4668</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>4669</td>\n",
       "      <td>1</td>\n",
       "      <td>4669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>4670</td>\n",
       "      <td>4</td>\n",
       "      <td>4670</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>4671</td>\n",
       "      <td>1</td>\n",
       "      <td>4671</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>4672</td>\n",
       "      <td>3</td>\n",
       "      <td>4672</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>4673</td>\n",
       "      <td>3</td>\n",
       "      <td>4673</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1743 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1     0  1\n",
       "0     2931  4  2931  2\n",
       "1     2932  3  2932  3\n",
       "2     2933  3  2933  3\n",
       "3     2934  1  2934  1\n",
       "4     2935  3  2935  4\n",
       "5     2936  3  2936  4\n",
       "6     2937  3  2937  3\n",
       "7     2938  3  2938  3\n",
       "8     2939  3  2939  3\n",
       "9     2940  4  2940  3\n",
       "10    2941  1  2941  1\n",
       "11    2942  3  2942  3\n",
       "12    2943  3  2943  3\n",
       "13    2944  4  2944  3\n",
       "14    2945  1  2945  1\n",
       "15    2946  4  2946  4\n",
       "16    2947  4  2947  3\n",
       "17    2948  3  2948  3\n",
       "18    2949  3  2949  3\n",
       "19    2950  3  2950  4\n",
       "20    2951  3  2951  3\n",
       "21    2952  3  2952  3\n",
       "22    2953  3  2953  4\n",
       "23    2954  2  2954  2\n",
       "24    2955  2  2955  2\n",
       "25    2956  1  2956  1\n",
       "26    2957  3  2957  3\n",
       "27    2958  3  2958  3\n",
       "28    2959  3  2959  3\n",
       "29    2960  3  2960  3\n",
       "...    ... ..   ... ..\n",
       "1713  4644  1  4644  2\n",
       "1714  4645  4  4645  4\n",
       "1715  4646  4  4646  1\n",
       "1716  4647  2  4647  2\n",
       "1717  4648  1  4648  1\n",
       "1718  4649  1  4649  1\n",
       "1719  4650  2  4650  3\n",
       "1720  4651  3  4651  4\n",
       "1721  4652  1  4652  1\n",
       "1722  4653  3  4653  2\n",
       "1723  4654  1  4654  1\n",
       "1724  4655  4  4655  3\n",
       "1725  4656  4  4656  4\n",
       "1726  4657  3  4657  3\n",
       "1727  4658  1  4658  1\n",
       "1728  4659  3  4659  3\n",
       "1729  4660  1  4660  1\n",
       "1730  4661  4  4661  4\n",
       "1731  4662  3  4662  3\n",
       "1732  4663  1  4663  1\n",
       "1733  4664  3  4664  3\n",
       "1734  4665  1  4665  4\n",
       "1735  4666  3  4666  3\n",
       "1736  4667  1  4667  1\n",
       "1737  4668  1  4668  1\n",
       "1738  4669  1  4669  1\n",
       "1739  4670  4  4670  4\n",
       "1740  4671  1  4671  1\n",
       "1741  4672  3  4672  3\n",
       "1742  4673  3  4673  3\n",
       "\n",
       "[1743 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.read_csv('/Users/kanoumotoharu/Downloads/sub_7.csv', header=None),sub], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('/Users/kanoumotoharu/Downloads/sub_13.csv',  index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10127369, 0.40086803, 0.27877044, 0.21908784],\n",
       "       [0.08530234, 0.28073404, 0.39763328, 0.23633033],\n",
       "       [0.17745063, 0.25154499, 0.33936384, 0.23164054],\n",
       "       ...,\n",
       "       [0.25766106, 0.2301475 , 0.22386374, 0.2883277 ],\n",
       "       [0.06947772, 0.0708203 , 0.41543539, 0.44426659],\n",
       "       [0.07184275, 0.12031271, 0.59142699, 0.21641754]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds/(80*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
